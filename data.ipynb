{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7472a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2252f149",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "#bs4.SoupStrainer is a utility in BeautifulSoup that allows you to parse only part of an HTML document \n",
    "#instead of parsing the entire document. This is very useful for speeding up parsing when you only care about certain tags or parts of the HTML.\n",
    "\n",
    "bs4_strainer = bs4.SoupStrainer(class_=(\"framer-tef8j0\",\"framer-rdjk9z\",  \"framer-styles-preset-1o1ea90\"),\n",
    "                                )\n",
    "\n",
    "loader = WebBaseLoader(\n",
    "    web_path=(\"https://research.perplexity.ai/articles/weight-transfer-for-rl-post-training-in-under-2-seconds\",\n",
    "              \"https://research.perplexity.ai/articles/gpt-oss-on-day-0\",\n",
    "              \"https://research.perplexity.ai/articles/disaggregated-prefill-and-decode\",\n",
    "              \"https://research.perplexity.ai/articles/high-performance-gpu-memory-transfer-on-aws-sagemaker-hyperpod\",\n",
    "              \"https://research.perplexity.ai/articles/accelerating-sonar-through-speculation\",\n",
    "              \"https://research.perplexity.ai/articles/rl-training-for-math-reasoning\",\n",
    "              \"https://research.perplexity.ai/articles/lower-latency-and-higher-throughput-with-multi-node-deepseek-deployment\",\n",
    "              \"https://research.perplexity.ai/articles/efficient-and-portable-mixture-of-experts-communication\",\n",
    "              \"https://www.perplexity.ai/hub/blog/announcing-comet-plus-launch-partners\",\n",
    "              \"https://www.perplexity.ai/hub/blog/introducing-the-perplexity-search-api\",\n",
    "              \"https://www.perplexity.ai/hub/blog/a-personal-assistant-for-your-inbox\",\n",
    "              \"https://www.perplexity.ai/hub/blog/power-your-organization-s-full-potential\",\n",
    "              \"https://www.perplexity.ai/hub/blog/secure-credentials-on-comet-with-1password\",\n",
    "              \"https://www.perplexity.ai/hub/blog/introducing-perplexity-for-government\",\n",
    "              \"https://www.perplexity.ai/hub/blog/introducing-comet-plus\",\n",
    "              \"https://www.perplexity.ai/hub/blog/perplexity-x-theo-von-s-this-past-weekend\",\n",
    "              \"https://www.perplexity.ai/hub/blog/the-intelligent-business-introducing-comet-for-enterprise-pro\",\n",
    "              \"https://www.perplexity.ai/hub/blog/gpt-oss-on-day-0\",\n",
    "              \"https://www.perplexity.ai/hub/blog/agents-or-bots-making-sense-of-ai-on-the-open-web\",\n",
    "              \"https://www.perplexity.ai/hub/blog/book-a-table-with-perplexity-and-opentable\",\n",
    "              \"https://www.perplexity.ai/hub/blog/welcoming-gannett-to-the-perplexity-publisher-program\",\n",
    "              \"https://www.perplexity.ai/hub/blog/introducing-comet\",\n",
    "              \"https://www.perplexity.ai/hub/blog/introducing-perplexity-max\",\n",
    "              \"https://www.perplexity.ai/hub/blog/bringing-european-ai-models-to-global-audiences\",\n",
    "              \"https://www.perplexity.ai/hub/blog/accelerating-sonar-through-speculation\",\n",
    "              \"https://www.perplexity.ai/hub/blog/answers-for-every-investor\",\n",
    "              \"https://www.perplexity.ai/hub/blog/introducing-perplexity-labs\",\n",
    "              \"https://www.perplexity.ai/hub/blog/perplexity-x-lewis-hamilton\",\n",
    "              \"https://www.perplexity.ai/hub/blog/more-value-in-every-answer-new-benefits-for-every-level-of-perplexity-user\",\n",
    "              \"https://www.perplexity.ai/hub/blog/perplexity-and-sap-turbocharging-joule-with-real-time-answers-for-every-enterprise\",\n",
    "              \"https://www.perplexity.ai/hub/blog/perplexity-partners-with-wiley-to-power-educational-ai-search\",\n",
    "              \"https://www.perplexity.ai/hub/blog/announcing-our-global-partnership-with-motorola\",\n",
    "              \"https://www.perplexity.ai/hub/blog/choice-is-the-remedy\",\n",
    "              \"https://www.perplexity.ai/hub/blog/how-perplexity-enterprise-pro-keeps-your-data-secure\",\n",
    "              \"https://www.perplexity.ai/hub/blog/perplexity-sonar-dominates-new-search-arena-evolution\",\n",
    "              \"https://www.perplexity.ai/hub/blog/efficient-and-portable-mixture-of-experts-communication\",\n",
    "              \"https://www.perplexity.ai/hub/blog/rebuilding-tiktok-in-america\",\n",
    "              \"https://www.perplexity.ai/hub/blog/new-sonar-search-modes-outperform-openai-in-cost-and-performance\",\n",
    "              \"https://www.perplexity.ai/hub/blog/perplexity-expands-partnership-with-softbank-to-launch-enterprise-pro-japan\",\n",
    "              \"https://www.perplexity.ai/hub/blog/open-sourcing-r1-1776\",\n",
    "              \"https://www.perplexity.ai/hub/blog/introducing-perplexity-deep-research\",\n",
    "              \"https://www.perplexity.ai/hub/blog/meet-new-sonar\",\n",
    "              \"https://www.perplexity.ai/hub/blog/introducing-the-sonar-pro-api\",\n",
    "              \"https://www.perplexity.ai/hub/blog/welcoming-carbon-to-the-perplexity-team\",\n",
    "              \"https://www.perplexity.ai/hub/blog/perplexity-s-2024-campus-strategist-program\",\n",
    "              \"https://www.perplexity.ai/hub/blog/perplexity-expands-publisher-program-with-15-new-media-partners\",\n",
    "              \"https://www.perplexity.ai/hub/blog/research-nonprofits-with-charity-navigator-on-perplexity\",\n",
    "              \"https://www.perplexity.ai/hub/blog/shop-like-a-pro\",\n",
    "              \"https://www.perplexity.ai/hub/blog/why-we-re-experimenting-with-advertising\",\n",
    "              \"https://www.perplexity.ai/hub/blog/introducing-the-election-information-hub\",\n",
    "              \"https://www.perplexity.ai/hub/blog/about-the-dow-jones-lawsuit\",\n",
    "              \"https://www.perplexity.ai/hub/blog/a-student-s-guide-to-using-perplexity-spaces\",\n",
    "              \"https://www.perplexity.ai/hub/blog/meet-our-first-channel-partners-data-integrators\",\n",
    "              \"https://www.perplexity.ai/hub/blog/introducing-internal-knowledge-search-and-spaces\",\n",
    "              \"https://www.perplexity.ai/hub/blog/getting-started-with-perplexity\",\n",
    "              \"https://www.perplexity.ai/hub/blog/redeem-a-free-year-of-perplexity-pro-through-xfinity-rewards\",\n",
    "              \"https://www.perplexity.ai/hub/blog/perplexity-makes-gift-to-northwestern-medill-to-research-ai-and-journalism\",\n",
    "              \"https://www.perplexity.ai/hub/blog/eligible-uber-one-members-can-now-unlock-a-complimentary-full-year-of-perplexity-pro\",\n",
    "              \"https://www.perplexity.ai/hub/blog/introducing-the-perplexity-publishers-program\",\n",
    "              \"https://www.perplexity.ai/hub/blog/perplexity-collaborates-with-amazon-web-services-to-launch-enterprise-pro\",\n",
    "              \"https://www.perplexity.ai/hub/blog/pro-search-upgraded-for-more-advanced-problem-solving\",\n",
    "              \"https://www.perplexity.ai/hub/blog/bringing-perplexity-to-education-and-not-for-profits\",\n",
    "              \"https://www.perplexity.ai/hub/blog/perplexity-pages\",\n",
    "              \"https://www.perplexity.ai/hub/blog/perplexity-launches-enterprise-pro\",\n",
    "              \"https://www.perplexity.ai/hub/blog/perplexity-pro-is-coming-to-all-sk-telecom-users\",\n",
    "              \"https://www.perplexity.ai/hub/blog/perplexity-partners-with-elevenlabs-to-launch-discover-daily-podcast\",\n",
    "              \"https://www.perplexity.ai/hub/blog/arc-x-perplexity\",\n",
    "              \"https://www.perplexity.ai/hub/blog/perplexity-raises-series-b-funding-round\",\n",
    "              \"https://www.perplexity.ai/hub/blog/introducing-pplx-online-llms\",\n",
    "              \"https://www.perplexity.ai/hub/blog/turbocharging-llama-2-70b-with-nvidia-h100\",\n",
    "              \"https://www.perplexity.ai/hub/blog/introducing-pplx-api\",\n",
    "              \"https://www.perplexity.ai/hub/blog/announcing-our-series-a-funding-round-and-mobile-app-launch\",\n",
    " ),\n",
    "    bs_kwargs={\"parse_only\": bs4_strainer},\n",
    ")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0f4d62f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://research.perplexity.ai/articles/weight-transfer-for-rl-post-training-in-under-2-seconds'}, page_content='We recently achieved 1.3-second cross-machine parameter updates for Kimi-K2 (1T parameters), transferring weights from 256 training GPUs (BF16) to 128 inference GPUs (FP8).In asynchronous reinforcement learning fine-tuning, training and inference run on separate GPUs. After each training step, new weights must be pushed to inference nodes. Many existing frameworks take several seconds—or even minutes—for trillion-parameter models.By leveraging RDMA point-to-point communication, we are able to make the weight transfer blazing fast, without changing inference engine, and make the code easier to write and maintain.RDMA WRITE: one-sided transfersOur solution is built on RDMA WRITE, a one-sided primitive where the source directly writes into the destination’s GPU memory.def rdma_write(src_ptr, dst_ptr, size, src_mr, dst_mr):\\n    # Write from local [src_ptr, src_ptr+size) to remote [dst_ptr, dst_ptr+size).\\n    # src_mr and dst_mr contains Memory Region metadata of both sides.\\n    ...The destination side won’t even get notified for the transfer. This gives us low-latency, high-throughput, zero-copy transfers driven by the training nodes without any control logic on the inference nodes.High-level workflowMetadata collection – Controller gathers parameter metadata from all training and inference GPUs.Schedule computation – Controller computes a static weight transfer schedule, mapping which training GPU sends which parameter to which inference GPU, and in what order.Schedule distribution – Controller sends the schedule to all training GPUs.Execution – After each training step, the controller signals training GPUs to start transfers.Weight transfer executionWith the high-level workflow defined, the key challenge is how to execute weight transfers efficiently at trillion-parameter scale. Here we describe the details of the execution path.DeviceMesh and Mesh GroupsParameters in training are distributed according to FSDP placements. Using full_tensor(), all GPUs in a DeviceMesh can reconstruct the full parameter, hence all can serve as a source for weight transfer.Multiple disjoint DeviceMeshes form a mesh group. Because DeviceMeshes in the same group are disjoint, their transfers don’t interfere and can run fully in parallel. Between mesh groups, we insert a global barrier to enforce ordering.Task pipelineWe treat the transfer of each parameter tensor as a task. The weight transfer process utilizes multiple types of hardware sources, hence we split a weight transfer task into different pipeline stages which overlap in time:Host to device memcpy — If FSDP offloads weight to CPUParameter preparation — Reconstruct full weight with full_tensor(), apply projection fusion, quantize if needed.RDMA transfer — Zero-copy write to remote inference GPU memoryGlobal barrier — After all full_tensor() calls are done, synchronize across mesh groups using GLOO via Ethernet.In implementation, we maintain a FIFO queue of tasks for each pipeline stage. Whenever the head of queue task completes the stage, it is moved to the tail of the next stage queue.GPU memory usage controlfull_tensor() and other GPU operations introduces extra GPU memory usage. To avoid out of memory error, we start the execution of a task only if the current on-the-fly tasks occupies less temporary GPU memory than a configurable watermark.Why it’s fast and simpleSeveral design choices make our system significantly faster to run and easier to maintain than common open-source solutions.Point-to-point communicationA common pattern is to funnel all parameters through rank-0 GPUs: gather on training rank-0, send to inference rank-0, then scatter again. This quickly becomes a choke point, limited by a single GPU’s PCIe bandwidth and NIC (e.g., 400 Gbps ≈ 50 GB/s).In contrast, our point-to-point setup allows every training GPU to send directly to every inference GPU, saturating the full network fabric rather than a single link.One-sided data transferSome systems rely on calling into the inference engine’s update_weight() method for each tensor. That means intrusive changes to the inference code, plus overhead from RPCs, serialization, and control-plane coordination.With RDMA WRITE primitive, we update weights silently on inference GPU memory, without extra copies. No control plane message and no CPU control logic is involved. No modification to inference engine is required.PipeliningThe weight transfer process can leverage four types of hardware resources: (1) Host-device data movement (2) GPU computation for projection fusion and quantization (3) RDMA network for data plane (4) Ethernet for control plane.Our design split weight transfer tasks into pipeline stages, allowing easy overlapping across different hardware resources.Static ScheduleSome implementations recompute a transfer schedule at every training step, repeatedly collecting metadata and distributing instructions. This adds unnecessary control-plane latency.Our schedule is computed once at initialization. Each training iteration simply replays the plan: the controller issues a “go” signal, and GPUs follow their pre-assigned routes. Execution is predictable and lightweight.Clean separationIt’s tempting to entangle the whole weight update process in one monolithic function: collect metadata, name matching, intra-node gathering, projection fusion, quantization, subslicing communication world, inter-node network transfer. It’s hard to program correctly, and even harder to optimize.In our engineering, we separate these steps as individual components. Each components can be unit tested, reasoned about, and optimized in isolation.ConclusionFast, reliable weight transfer is a critical building block for large-scale RL fine-tuning. By combining the RDMA WRITE primitive, a static transfer schedule, and pipelined execution, we reduced trillion-parameter updates to just 1.3 seconds on Kimi-K2. The approach is simple to reason about, easy to maintain, and avoids the bottlenecks of traditional designs.'),\n",
       " Document(metadata={'source': 'https://research.perplexity.ai/articles/gpt-oss-on-day-0'}, page_content='GPT-OSS on Day 0Perplexity is committed to making the best AI models available to curious people who demand accurate and trustworthy answers and agentic actions. OpenAI recently announced their open-weight models (gpt-oss-20b and gpt-oss-120b). As with many leading models, Perplexity is among the first organizations to evaluate gpt-oss-20b and gpt-oss-120b.In this post, we share the infrastructure decisions of our in-house inference stack that made Day-0 support possible. We focus on serving these models on NVIDIA H200 GPUs, detailing the kernel changes, deployment choices, and speed-cost trade-offs.GPT-OSS on HopperThe open-weight models are shipped using MXFP4 quantization, which helps them fit in the memory on consumer-grade hardware or achieve peak throughput on NVIDIA Blackwell. However, for initial evaluation, we wanted to run them on existing H200 Hopper clusters with minimal inference-engine changes. Hopper does not have dedicated FP4 tensor cores, which were introduced by Blackwell. Consequently, we decided to serve the models with FP8 precision instead to minimize the need for custom kernels and make best use of the available hardware.At a high level, transformer LLMs are structurally simple:Input embeddingA sequence of transformer layers. Each contains an attention block, and a dense MLP or sparse MoE block.Output logit projectionWhen bringing up a new transformer model, we are primarily interested in learning to what extent they match existing models to identify the changes required. OpenAI kindly shared an overview of their models ahead of time, allowing us to tweak our inference setup accordingly. We found that the model did require quite a number of small changes.Attention: GQA with Attention SinksEach attention head has a pre-trained sink parameter which is factored into SoftmaxYaRN positional encoding, similar to DeepSeek-V3QKV projections have biases, similar to Qwen 2Output projection has biasMLP: Sparse Mixture-of-Expert (MoE)SwiGLU activation function, different from most open-weight modelsSoftmax after Top-K for expert weightsExperts have biasesExpecting the number of parameters to be around 100B-200B, we decided to:Extend FlashInfer to support attention sinksRe-use the MoE implementation already built for the DeepSeek-V3 family of modelsChoose FP8 128x128 block scaling as the quantization scheme, extending the DeepGEMM kernels to support a bias term.In-house Inference EngineOur inference efforts are backed by our in-house inference engine, ROSE. We are building ROSE out to be a flexible framework that allows new models to be quickly brought up. Additionally, once we are committed to serving a model at scale, it provides a platform to optimize and boost performance to reach the state of the art.ROSE exposes an LLM engine, which can load a model and generate decoded tokens for it through multiple decoders, including speculative and MTP decoders. The models themselves are composed of a custom set of layers, which support various configuration knows for quantization and parallelism. Underneath the layers, it bundles both Triton and CUDA kernels that efficiently implement and fuse the computations performed by individual layers.ROSE is primarily built in Python and relies of PyTorch for model definitions. While Python allows us to be flexible and adapt to new models, we are migrating most performance-sensitive components, such as serving and batch scheduling, to Rust.To bring up a new model, we firstly define the model hierarchy and build a weight converter for it. After all weights are accounted for, we define a forward pass for the simplest TP=1 use case and ensure it works end-to-end. Finally, we implement and test support for various forms of parallelism before we release a new container and deploy the model on a cluster for evaluation. We then iterate on performance, relying on feedback from both dashboards and finer-grained benchmarks.ROSE operates on numeric tokens, with little knowledge of chat formats outside of integration tests. In parallel, we had to look into adjusting surrounding infrastructure to support the Harmony tokenizer and correctly map requests to input token sequences.With the GPT OSS model, we found that no changes were required to our decoders, CUDA graph harnesses or surrounding infrastructure, allowing us to focus our efforts on tweaking the kernels backing the model.Kernels for GPT-OSSSink AttentionSink attention introduces an extra bias factor prior to softmax, accumulated with the product of Q and K:attn = softmax(q @ k^T * sm_scale + sink) * VWhile a naive implementation is fairly trivial, efficient attention implementations, such as FlashInfer, parallelize the computation of attention across both independent heads and across the KV sequence length. Additionally, for numerical stability, Softmax is computed online while also subtracting the maximal element for numerical stability:softmax(x) = e^x / sum(e^x) = e ^ (x - max(x)) / sum(e ^ (x - max(x))The online implementation tracked a running maximum m initialized to -inf and a scale d to 1. By initializing the maximum to the sink value of the current head, we would ensure that it would get accumulated into the Softmax correctly. We had to take care to ensure that this maximum would only be included in the first chunk if attention was split across multiple blocks along the sequence length.We have adjusted the FlashInfer FA2 kernel template, used for both prefill and decode, to include the sink parameters.MoEThe main difference between the MoE layer of different models is usually the routing scheme. Expecting substantial variability, ROSE implements this kernel in Triton, allowing us to easily adapt it to new models. In the case of GPT-OSS, we had to adjust weight scoring to perform softmax only across the Top-K select experts, unlike some DeepSeek-style models which compute softmax across all experts. We found it is critical for accuracy to compute the expert weights in bfloat16 precision.The challenge with the MoE layer was due to our decision to re-use the existing DeepGEMM kernels, which lacked support for bias terms. Fortunately, over time we integrated deeply with them, making changes to better connect them with our custom all-to-all dispatch kernels. We added bias support to DeepGEMM by initializing the WGMMA accumulator registers to the bias values loaded from memory. This is a one-off during the processing of an MN block, thus the cost is likely negligible and the L2 cache ensures efficient access to these values.Since these models do not readily benefit from Data Parallelism, we re-used TP-only dispatch and combine kernels. The inputs to dispatch are replicated across all rank, thus dispatch selects the tokens for the locally available experts. Combine performs the weighted accumulation across NVLink.Performance TweaksEven though our goal was to bring up the models quickly, we did want to ensure that performance was still reasonable. After ensuring correctness against a reference implementation provided by OpenAI, we profiled our implementation using the default PyTorch profiler, watching out for block and thread allocations across kernels. The functionality we re-used was built out for models which had a larger hidden dimension of 7168, substantially larger than the 2880 of the GPT-OSS models. We specialized kernel launchers to pick better grid dimensions, significantly boosting throughput.Tokenization and ChannelsHarmony’s chat format has several important concepts for structuring conversations, reasoning content, and function calls. In this format, each message consists of a header and content:<|start|>{header}<|message|>{content}<The header carries metadata — most importantly the role (system, developer, user, assistant, tool), the channel, and a recipient such as to=functions.<function-name> when invoking a tool call.Channels (such as analysis, commentary, and final answer) make the model’s outputs more transparent and segmented. Because the format enforces this separation, the model can reason transparently (and be inspected for debugging) without leaking chain-of-thought into production. Recipient tags (to= headers) tell the runtime exactly which actor should receive the next payload — whether that’s a function, the model itself, or the end user.Our inference engine ROSE is format-agnostic. It receives a batch of input tokens and streams back output tokens. In our inference stack, the task of formatting inputs into a tokenized prompt and parsing LLM results back into structured messages is handled by the dedicated JSON API frontend service. We integrated Harmony by adding a new pluggable formatter/parser to this frontend, allowing the backend to remain unchanged while the frontend cleanly manages chat-format rules, tool calls, and streaming. This separation of concerns keeps the architecture modular, enabling quick adoption of new formats, response features with minimal backend changes.Cost and PerformanceGiven the relatively small size of GPT-OSS models, we limit the deployment of a replica to a single node to avoid inter-node communication overhead. Our implementation supports flexible combinations of expert parallelism (EP), data parallelism (DP), and tensor parallelism (TP). To determine the optimal deployment setup, we benchmarked all combinations of sharding strategies across batch sizes from 1 to 128. Thanks to Prefill-Decode Disaggregation, we are able to discuss the performance-cost trade-off separately for prefill and decode.For cost estimates, we assume a nominal H200 market rate of $3.00/hour (note: this is not our actual GPU price).We discovered that setting batch_size=1 and DP=1 yields the best prefill performance. Different TP configurations then provide varying trade-offs between first-token latency and cost. The tables below present prefill latency and cost per million input tokens for various configurations and input lengths.GPT-OSS 120B:Input Length81923276865536128000EP1 DP1 TP10.364s, $0.0372.421s, $0.0627.375s, $0.09423.902s, $0.156EP2 DP1 TP20.190s, $0.0391.245s, $0.0633.786s, $0.09612.116s, $0.158EP4 DP1 TP40.165s, $0.0670.900s, $0.0922.449s, $0.1257.165s, $0.187EP8 DP1 TP80.179s, $0.1450.839s, $0.1712.003s, $0.2045.119s, $0.267GPT-OSS 20B:Input Length81923276865536128000EP1 DP1 TP10.239s, $0.0241.610s, $0.0414.895s, $0.06216.037s, $0.104EP2 DP1 TP20.127s, $0.0260.833s, $0.0422.522s, $0.0648.143s, $0.106EP4 DP1 TP40.108s, $0.0440.593s, $0.0601.616s, $0.0824.766s, $0.124EP8 DP1 TP80.116s, $0.0940.547s, $0.1111.310s, $0.1333.365s, $0.175For decode, we discovered that EP4 DP1 TP4 is the best configuration for most cases. The four plots below illustrate the trade-off between decode speed and cost per million output tokens for both the 120B and 20B models at sequence lengths of 8192 and 32768.We have also considered using the smaller 20B model as a speculative decoding draft model for the larger 120B model. However, because the number of activated parameters is similar (3B vs. 5B), the resulting speedups were marginal.ReferencesDisaggregated Prefill and DecodeAccelerating Sonar Through Speculation\\u200bLower Latency and Higher Throughput with Multi-node DeepSeek Deployment\\u200bEfficient and Portable Mixture-of-Experts Communication\\u200bpplx-kernelsDeepGEMMFlashInfer: Efficient and Customizable Attention Engine for LLM Inference ServingEfficient Streaming Language Models with Attention SinksDeepSeek-V3 Technical Report\\u200b\\u200bOpenAI Harmony Response Format | OpenAI Cookbook\\u200b'),\n",
       " Document(metadata={'source': 'https://research.perplexity.ai/articles/disaggregated-prefill-and-decode'}, page_content='In order to generate output tokens from an input prompt, LLM inference is split into two stages: prefill and decode. Prefill runs on the input tokens, populating KV caches, before entering the decode stage that generates tokens one-by-one. While a single decode step typically runs for tens of milliseconds, prefill takes substantially longer. If run on the same devices, mixing prefill with decode degrades decode performance. In this article we explore an established solution in the form of disaggregated prefill and decode, running them on separate devices to maximize both prefill throughput and decode latencies.Prefill vs Decode PerformanceIn a typical LLM serving engine, the batch scheduler selects requests to process in each execution step of a model. When running on a single device or node, both prefill and decode requests are batched together. The cost of attention, which aggregates along the sequence length, grows for both prefill and decode, proportionally to the length of entries in the KV cache (kv_len). Decode requests usually forward a single token (qo_len=1), at a minimal cost through other layers which operate independently on the tokens of a sequence. Prefill requests forward thousands or tens of thousands of tokens at a significant cost through dense layers (large qo_len).The latency of a forward pass is more strongly influenced by the number of independent tokens passed through dense layers (qo_len) than the number of tokens retrieved from the KV cache during attention (kv_len). Attention can parallelize both across the number of requests and the kv_len proportional to sequence lengths, achieving good utilization. Prefill is compute bound: qo_len being high, GEMM kernels can allocate sufficient blocks along the M dimension to fully utilize the compute capabilities of modern GPUs. Decode is memory bound: due to typically low batch sizes, the number of inputs along M is usually small, sufficient for only one block. While Split-K GEMM kernels can improve SM utilization for low token batch sizes, the caches and the matrix multiplication units typically remain under-utilized. When mixed together, batches containing requests for prefill incur higher latencies through the forward pass, negatively affecting the decode throughput of the entire instance. While mixing prefill requests with decode requests or employing chunked prefill can slightly improve decode performance, it is difficult to maintain sufficient prefill throughput to process enough requests on an instance to maximize decode throughput. In the case of large models, with typical output lengths, to maintain a large batch size for decode, prefill must be performed often enough that it significantly degrades average latency and causes stutter in the output.These issues can be addressed by using a separate set of nodes to perform prefill and decode. By associating a prefiller node with multiple decoder nodes, sufficient requests can be scheduled for prefill to maximize throughput and maintain a large enough number of concurrent requests on the decoder nodes to also maximize decode throughput. The prefiller nodes populate the KV caches, which are then transferred to the decoder nodes. Since the decoders no longer have to break for prefill, latencies become much more deterministic, as the overall impact of growing kv_len of active requests is much less pronounced. The cost is paid in an increase in Time to First Token (TTFT), as the transfer of KV caches over the network can take tens to hundreds of milliseconds.KV MessengerAt Perplexity, our implementation for disaggregated prefill and decode is built around a KV messenger which interacts with the LLM engine to orchestrate KV cache transfers from prefiller nodes to the decoder nodes through a network. On the prefiller side, the messenger accepts requests from decoder nodes, handing them over to the batch scheduler and keeping track of the forward pass execution to dispatch KV caches with as little latency as possible. On the decoder side, after un-evictable pages are allocated, the messenger blocks the request from being scheduled for decode until it is notified of the completion of the KV cache and decoder context transfers.Disaggregating prefill requires high throughput, low-latency connections, thus our implementation is tailored for RDMA, supporting both EFA and ConnectX Network Interface Controllers (NICs). The KV Messenger is built upon libfabric, using our fabric-lib wrappers to provide higher-level low-latency abstractions over the Remote Direct Memory Access (RDMA) primitives, implementing efficient page and metadata transfers, along with low-latency signaling. In the background, fabric-lib coordinates a GPU and its directly connected NICs to copy data from the prefiller node to the decoder node.Upon receipt, the prefiller node allocates a corresponding set of source KV pages and schedules the request for prefill using its local engine. To minimize latency, transfers do not wait for the forward pass: instead, KV page copies are initiated as soon as the model finishes appending KV cache entries to the KV cache for individual layers. Since prefill requests can be chunked, the batch scheduler notifies the KV messenger of the currently scheduled chunks before execution. To support CUDA graphs whilst being able to track layers, the messenger keeps a dedicated thread polling a counter incremented after the output projection of attention. The counter is maintained only on the lead node in a sharded environment: even though the KV cache entries are valid after append and before attention, the output projection is reduced across ranks, implicitly synchronizing them. Once a change in the counter is observed, the messenger is notified and it calls fabric-lib to initiate the transfer of a layer.After the transfer of the last chunk is complete, any additional metadata is also copied over: speculative decoding or MTP require logits and hidden states to be moved to the decoder. These copies are also performed through RDMA, to and from pre-allocated buffers.Upon the completion of all pending transfers of the last chunk, the prefiller node de-allocates the KV pages and completes the request. The decoder node is not explicitly notified: instead, it uses immediate counters to keep track of the number of operations completed. The number of RDMA operations on the prefiller side is proportional to the number of pages transferred. Upon the completion of the known number of page and context copies, fabric-lib calls the KV messenger to indicate that a request is ready for decoding. The messenger de-allocates any context and hands the request over to the LLM engine.Sharded KV Cache TransfersIf the prefiller and decoder rely on Tensor Parallelism (TP) and shard or replicate the KV caches identically, a single transfer engine coordinates multiple devices to send and receive the pages of all the replicas. In order to be able to use a single messenger and transfer engine despite the fact that the executor of the model is replicated across multiple devices and processes, cuMem and cuMemImportFromShareableHandle are used to allocate the device memory backing the KV caches and to map it into the main process. The transfer engine inspects the node’s topology to find the NICs and the CPUs in the closest NUMA node to use for the transfers of each of the KV cache slices.If the source and destination shard identically, transfers are trivial as there is a one-to-one mapping from the devices and pages of the source and destination. In this situation, sharding implicitly helps transfer latencies: by using more GPUs, more associated NICs can be employed, reaching closer to full bandwidth utilization. However, if there is a mismatch, the transfer engine must split or reconstruct pages depending on the ratio between source and destination slices.If the prefiller splits the KV cache across more devices, full pages are reconstructed on the decoder by sending the corresponding halves out from the prefiller devices. If the decoder has more shards, it receives pages from multiple sources. The decoder needs to know of the sharding scheme of the prefiller in order to be able to compute the number of RDMA writes it is expected to receive. If replication is involved, the prefiller groups the devices into replica sets that replicate the full KV cache within themselves. Destination replica sets are randomly assigned one of the source sets in order to use all the available devices to initiate RDMA writes.Sharded transfers require a slight tweak to KV caches. By default, FlashInfer relies on the NHD layout, which orders the tokens within a page within the heads. Since caches are most likely sharded along the number of attention heads, this creates discontinuity within the head. RDMA transfers do not implicitly support strided writes, requiring one operation per head to perform the transfer. Instead, in order to reduce the number of interactions with libfabric , we organize KV caches using the HND layout which places the head dimension before the number of tokens. This ensures continuity, allowing a page to be copied over with a single write.Speculative DecodingSpeculative decoding requires slight tweaks to disaggregated prefill-decode. In our implementation, prefiller nodes are not allowed to sample tokens. Since the Sonar models of Perplexity support structured output, we do not want to incur the complexity of synchronizing the schema processor implementations across prefillers and decoders. In the MTP and speculative decoding mechanisms, prefiling the draft model up to the last token involves sampling tokens from the target model.To work around these issues, prefill does not include the last token of the input sequence. Instead, hidden states or logits from prefill preceding the last token are transferred and it is treated as a decode token in the next step on the decoder. While this slightly increases latencies, as a full decode step must be performed after prefill to issue the first token, the complexity of the implementation is greatly reduced.Disaggregated DeploymentsWe have deployed or experimented with multiple disaggregated configurations with different models, to support either production traffic or in-house evaluation workloads. Based on the size and attention mechanism of models, we chose suitable sharding schemes for prefiller and decoder nodes to best utilize GPUs.DeepSeek-R1With DeepSeek, we considered both Tensor-Parallel (TP) and Data-Parallel (DP) deployments. As discussed in previous blog posts, TP deployments provide better latency and the cost of lower throughput, requiring more GPUs to serve heavy traffic. DP deployments scale much better with load, however their peak throughput is lower due to the cost of inter-device or inter-node communication.DeepSeek relies on Multi-Head Latent Attention, compressing KV caches. Since all the KV heads are compressed into a single latent vector, TP cannot shard the KV caches, as it must instead replicate the latent vectors on all ranks. Sharding happens after decompression, as each rank can extract different heads from the same latent representation. Consequently, all KV cache shards are identical across both prefiller and decoder shards.With an intranode TP setup, both prefillers and decoders are sharded identically. Transfers are dispatched from all ranks in order to fully utilize all available NICs. However, with a DP deployment, where the TP rank size is lower or each DP rank is assigned to a single GPU, any prefiller device that holds a replicated copy of the KV cache can dispatch it. To balance requests across all available NICs, we randomly select a GPU and a NIC to send the KV cache from the prefiller to the decoder.With mixed prefill-decode, our R1 deployment was struggling to consistently exceed 50 TPS due to frequent prefill interruptions in the order of hundreds of milliseconds. In contrast, by separating prefill, we incurred a penalty of about 100ms to TTFT for each request, but a single prefiller node could maintain consistent batch sizes on 3 decoder nodes, delivering a throughput in excess of 90 TPS while handling a load of about 1 QPS per decoder node. With data-parallel deployments, TPS was slightly lower at around 50, however the instances could handle a load of 1 QPS per rank, with 8 ranks to a single node.Qwen3-CoderThis 480B model uses Grouped-Query Attention (GQA), so attention can be easily sharded and can benefit from tensor parallelism without sacrificing memory for KV caches. Consequently, we could shard the model across 8 GPUs for both prefill and decode, pairing around 3 decoder nodes with a single prefiller node. Since attention is sharded, we rely on the HND KV cache layout to shard prefiller and decoder KV caches, pairing prefiller ranks with decoder ranks and fully utilizing all NICs to transfer slices in parallel.'),\n",
       " Document(metadata={'source': 'https://research.perplexity.ai/articles/high-performance-gpu-memory-transfer-on-aws-sagemaker-hyperpod'}, page_content='Modern deep learning infrastructure often requires transferring large amounts of data between GPUs across machines. At Perplexity, we encountered a unique technical challenge: efficiently transferring non-contiguous GPU memory regions between machines at maximum possible speed. Our target platform, AWS p5 instances, offers an impressive 3200 Gbps of network bandwidth through 32 network cards. This article shares our journey of building a custom high-performance networking solution that achieves 97.1% of this theoretical bandwidth.The Technical ChallengeOur use case presented several key technical requirements:High-bandwidth transfer between remote GPUs of non-contiguous memory chunksAbility to dynamically add or remove nodes from Kubernetes deployments without disrupting ongoing operationsSupport for peer-to-peer communication patternsWhile NVIDIA\\'s NCCL library is the de facto standard for distributed deep learning, it wasn\\'t ideal for our use case:NCCL excels at collective communication but requires establishing a static \"world\", which requires restarting the entire cluster when adjusting the participating nodes.NCCL\\'s synchronous communication model adds complexity for our asynchronous workloadWe wanted direct control over our memory transfer patterns for optimizationBuilding our own solution provided valuable learning opportunitiesModern High-Performance NetworksTo understand our solution, let\\'s first explore how modern high-performance networks differ from traditional networking.Most networks we use daily rely on TCP/IP protocols, where applications communicate with the network card through the operating system kernel using sockets. However, high-performance networks use RDMA (Remote Direct Memory Access) - a completely different hardware and software stack that enables direct memory access between machines without involving the CPU.AWS provides Elastic Fabric Adapter (EFA), a custom network interface that implements Amazon\\'s custom protocol called Scalable Reliable Datagram (SRD). Unlike traditional TCP/IP networking where data must be copied multiple times between user space, kernel space, and network buffers, EFA with RDMA allows direct data transfer between GPU memory and the network card, bypassing the CPU entirely.Philosophy of High-Performance Network DesignBuilding high-performance networking systems requires rethinking several fundamental assumptions:Buffer Ownership: Unlike traditional sockets where the kernel manages network buffers and requires copying between user space and kernel space, RDMA requires applications to manage their own buffers. When an application initiates a network operation, it transfers buffer ownership to the network card until the operation completes, eliminating the need for data copying.Memory Registration: Applications must register memory regions with the operating system kernel. The kernel sets up virtual address mappings that allow the CPU, GPUs, and network cards to all understand the same virtual addresses. This registration is a one-time operation that enables subsequent zero-copy data transfers.Control Plane vs Data Plane: High-performance networks separate operations into two categories:Control plane operations (like connection setup and memory registration) go through the kernel to ensure securityData plane operations (actual data transfer) bypass the kernel for maximum performanceReception Before Transmission: Without kernel-managed buffers, applications must pre-post receive operations, specifying where incoming data should be placed. This is a fundamental shift from the socket model where applications can receive data at any time.Poll-based Completion: Instead of waiting for kernel notifications through mechanisms like epoll, applications directly poll hardware completion queues. This eliminates system call overhead and allows immediate reaction to completed operations.Hardware Topology Awareness: Understanding and optimizing for hardware topology is crucial for achieving maximum performance.Understanding Hardware TopologyAWS p5 instances have a sophisticated internal architecture. As shown below, each instance contains two CPU sockets forming two NUMA nodes, with each NUMA node connecting to four PCIe switches:Under each PCIe switch, we find:Four 100 Gbps EFA network cardsOne NVIDIA H100 GPUOne NVMe SSDThe data paths for TCP/IP and RDMA transfers demonstrate fundamental differences in their approaches:With TCP/IP (left side), data must be copied multiple times:From GPU memory to application buffer in user spaceFrom user space buffer to kernel bufferFrom kernel buffer to network cardReverse process happens on the receiving sideEach copy operation consumes CPU cycles and memory bandwidth. The application must also context switch between user space and kernel space for each network operation.In contrast, RDMA (right side) provides true zero-copy data transfer:The network card reads directly from GPU memoryData travels directly to the remote network cardRemote network card writes directly to the destination GPU memoryThe application only needs to check a completion queue (CQ) in user space to know when the transfer is doneWith RDMA and proper hardware pairing, transferring data between two GPUs only requires traversing the local PCIe switch and the network:In contrast, TCP/IP transfers must copy data multiple times through main memory, causing significant PCIe bus congestion:Building with libfabricWe used libfabric - a framework that provides a generic interface for fabric services. Our implementation uses two types of RDMA operations:Two-sided RDMA (SEND/RECV) for control messages that carry metadata about memory regions (e.g., offsets and sizes).One-sided RDMA WRITE for actual data transfer, where each WRITE operation handles one contiguous memory chunk.Journey to Peak PerformanceOur development progressed through several stages:Implemented basic unidirectional message transfer using SEND/RECVExtended to bidirectional communicationAdded GPUDirect RDMA WRITE for direct GPU-GPU transferExpanded to handle multiple concurrent transfersIntroduced operation queuing for robustnessAchieved 97.4% bandwidth utilization on a single network cardWhen scaling to 32 network cards, we implemented several crucial optimizations:Operation Queuing: Rather than directly submitting operations to network cards, we maintain an application-level queue. This provides robustness against network congestion and simplifies the programming model.Network Warmup: Pre-establishing connections improved startup performance.Multi-threading: Dedicated threads for each GPU\\'s network operations.CPU Core Pinning: Binding threads to specific CPU cores to avoid NUMA effects and cache misses.State Sharding: Reducing contention between threads.Operation Batching: Submitting multiple operations together.Lazy Operation Posting: Operations are first queued in the application. After polling the completion queue, we attempt to submit pending operations to the network card, ensuring efficient use of network resources.NUMA-aware Resource Allocation: Allocate libfabric resources like completion queues on the correct NUMA node to minimize memory access latency.Through these optimizations, we achieved a final performance of 3,108 Gbps - 97.1% of the theoretical maximum bandwidth.The video below shows our command-line program in action. It transfers non-contiguous chunks of GPU memory to a remote node, achieving a transmission speed of 3108.283 Gbps - demonstrating near-theoretical bandwidth utilization of the network infrastructure:ConclusionBuilding a high-performance networking system requires understanding both hardware architecture and system design principles. While libraries like NCCL provide excellent solutions for common patterns, sometimes custom solutions are necessary for specific requirements.Our journey demonstrates that achieving near-theoretical network performance is possible with careful attention to system architecture, hardware topology, and various optimization techniques. The key is not just understanding individual components, but how they interact to form a complete system.The full technical deep-dive of this journey, including implementation details and code examples, is available in our open source repository.'),\n",
       " Document(metadata={'source': 'https://research.perplexity.ai/articles/accelerating-sonar-through-speculation'}, page_content='Speculative decoding speeds up the generation speed of Large Language Models (LLMs) by using a quick and small draft model to produce completion candidates that are verified by the larger target model. Under this scheme, instead of a run of the expensive target producing a single token, multiple are emitted in a single step. Here we present the implementation details of various kinds of speculative decoding, applied at Perplexity to reduce inter-token latency on Sonar models.Speculative DecodingSpeculative Decoding leverages the structure of natural languages and the auto-regressive nature of transformers to speed up token generation. Even though larger models, such as Llama-70B, carry more knowledge than smaller ones, such as Llama-1B, on some simpler tasks they perform similarly. This overlap does suggest that certain sequences are better generated by the less expensive models, leaving complex problems to larger ones. The challenge lies in determining which completions are better and whether the generation of the smaller model is of the same quality as that of the larger one.Fortunately, LLMs are auto-regressive transformers: when given a sequence of tokens, they output the probability distribution of the next token. Additionally, the logits derived from the intermediate features associated with the tokens in the input sequence also indicate how likely it is for the model to issue those exact tokens. This property enables speculation: if a sequence of tokens is generated by a smaller one starting from an input prefix, it can be run through the larger one to determine how well it lines up with the target model. Each prefix of the candidates is scored with a probability and the longest one above an acceptance threshold is picked. As a bonus, the target model also provides a subsequent token for free: if a draft model generates n tokens, up to n + 1 can be emitted in one step.At inference time, speculative sampling process can be split into roughly 4 stages:Prefill: both the target and the draft models must be run on the input sequence to populate the KV cache entries. While some schemes, such as Medusa, use simpler dense layers for prediction, in this post we focus on transformer-based drafts that need their own KV caches.Draft generation: the draft model iterates to produce a number of fixed tokens. The draft sequence can be linear or the model can explore a tree-like structure up to a given depth (EAGLE, Medusa). Here, we focus on linear sequences.Acceptance: the target model runs on the draft sequence, building logits corresponding to each draft token. The length of the longest acceptable sequence is determined.Target generation: since the target generated logits, at the mismatched position or the tail end of the sequence the logits correspond to a subsequent token. These logits can be sampled to provide a robust token from the target, capping off the sequence.Various methods exist to implement speculative decoding. In this post, we will focus on the schemes we used to accelerate Sonar models using an in-house 1B model, as well as the prediction mechanisms we are building out to speed up models at the scale of DeepSeek.Target-DraftSpeculative decoding can be achieved by coupling an existing small LLM as a draft model to a target model to generate candidate sequences. In production, we have accelerated Sonar using a Llama-1B model fine-tuned on the same dataset as the target. While this approach did not require training a draft from scratch, the small model still uses significant KV cache capacity and introduces a slight prefill overhead, increasing TTFT.Under this scheme, the decoder only speculates on decode-only batches, generating tokens through standard sampling during prefill or on mixed prefill-decode batches. In the prefill stage, the target logits are immediately sampled to also prefill the newly-generated token in the KV cache of the draft. The draft is not sampled yet, but the logits it produces are carried over to the decode stage.In decode, the draft model is advanced, sampling the top token at each stage. After the desired draft length is reached, the tokens are run through the target model to produce the logits based on which the sampler identifies the accepted sequence length. Acceptance is determined by comparing the full probability distributions from the draft and the target. Since the target always outputs one set of logits following the accepted draft sequence, that is sampled to produce an additional output. Since the draft model has not yet seen that accepted token, it is re-run to populate its corresponding KV cache entries in preparation for the next decode step, carrying the logits over again.EAGLEEAGLE is speculative decoding scheme which explores multiple draft sequences, generated through a tree-like traversal of probable draft tokens. A fixed (EAGLE) or dynamically-shaped (EAGLE-2) tree is explored using consecutive executions of the draft tokens, considering the Top-K candidates at each node instead of following the highest scoring token in a linear sequence. The sequences are then scored and the longest suitable one is selected to continue, also appending an additional token from the target.In order to achieve more accurate prediction, an EAGLE draft model predicts not only based on tokens, but also using the target features (last layer hidden states) of the target model. The disadvantage of EAGLE is the need to train custom, small draft models which are accurate enough to generate suitable candidates within a low latency budget. Typically, a draft model is a single transformer layer identical to a decoder layer of the original model, which is tightly coupled to the target by tying to its embeddings and lm_head projections. Since this requires less KV cache capacity, EAGLE has a lower memory footprint.To verify tree-like sequences in the target model, custom attention masks must be used. Unfortunately, using a custom attention mask for a whole sequence significantly slows down attention for realistic input lengths (by up to 50%), nullifying some of the speedup achievable through speculation. We have not yet deployed full tree exploration to production for this reason, focusing instead on the special case of single-token prediction via MTP-like schemes presented in the DeepSeek-V3 Technical Report.MTPThis scheme is similar to draft-target decoding, with the exception of hidden states being used alongside tokens for prediction. Slightly more work must be done in both the prefill and decode stages compared to regular draft-target speculation. The draft model uses both tokens and hidden states: token t_{i+1} is sampled from the logits L_i corresponding to token t_i, which in turn are derived from the hidden states H_i. Consequently, the input token buffers must be shifted one step to the left relative to the hidden state vectors output by the target. The figure below marks the correspondences used for training, as well as the shift during inference.The decoding flow is quite similar to draft-target decoding, with the exception of both hidden states and logits being carried over. Our implementation shares all the associated sampling and logit processing logits, specializing only the model forward invocations. When multiple tokens are predicted, the draft model uses draft hidden states for prediction, also populating KV cache entries based on its own features. In the long run, this can degrade accuracy. Subsequently, when running the draft model to populate the KV cache entry for the target prediction, we run it on the whole sequence taking the more accurate target hidden states as inputs. Since these draft models are small, the added cost of processing the additional tokens is negligible.Training MTP HeadsIn order to benefit from MTP, we built the infrastructure required to train MTP heads attached to our fine-tuned models on Perplexity’s datasets, running on one node with 8xH100 devices. In about one day, we can build heads for models ranging from Llama-1B to Llama-70B and DeepSeek V2-Lite. For larger models, we rely on MTP heads built during the fine-tuning process.The target of MTP training is to match up the draft hidden states and logits extrapolated from the target hidden states to the next token logits and hidden states of the target. Since inference for hidden states is expensive, we pre-compute them using our inference-optimized implementation of the target model, to be used during training. However, to validate the inference MTP implementation and ensure that numerical differences due to quantization or optimizations do not hinder results, for validation loss and accuracy estimation we fully re-use the inference implementation of both the target and the draft models.When scaling from the ShareGPT dataset used in the original paper to larger samples, we noticed that the MTP head architecture outlined and implemented in the EAGLE paper failed to train for 70B-sized models. Unlike ShareGPT which contained a larger number of shorter sequences, we train on a slightly smaller number of substantially longer prompts. Since the original EAGLE heads slightly diverged in structure from a typical transformer, we re-introduced some RMS Normalization layers that were stripped. We found that this not only allowed training to converge, but it also boosted the accuracy of the heads by a few percentage points.Not only do layer norms facilitate training, re-introducing the norms is also mathematically intuitive. MTP heads re-use the embeddings and the logit projections of the target model, as they can be substantial in size (about 2 GB for Llama 70B). During training, these are frozen and the expectation is that the MTP layer learns to embed predictions into the same vector space as what the projection layer of the original model learnt during training. By dropping the norms, a single MLP is expected to learn the same function as an MLP followed by a norm, which hinders the matchup between the hidden states of the draft and the target models.Inference with Speculative DecodingIn the inference engine, in order to generate tokens for input sequences, they need to be first grouped into reasonably-sized batches, then pages must be allocated in the KV cache for the next tokens. The input tokens and the KV page information is then packed into a buffer broadcast to all parallel ranks running the model. Finally, the metadata is copied into GPU memory and the model is executed to produce the logits from which the next token is sampled.Unlike certain implementations which loosely couple a draft and target inference server via a wrapper that orchestrates requests between them, our draft-target pairs are tightly coupled and step through generation in unison. Batch scheduling and KV page allocation is shared between the models for all forms of speculative decoding: this unifies the logic that bridges a model with the overarching inference server, as they all expose the same interface.The inference runtime at Perplexity is shaped around FlashInfer, which determines the metadata that needs to be built in order to configure and schedule the attention kernel. Given some input sequences forming a batch, for prefill, decode or verification, CPU-side work must be done to allocate intermediate buffers and populate certain constant buffers used in attention. This work is in addition to the cost of batch scheduling and KV page allocation, which also incur latencies that must be hidden in order to maximize GPU utilization.While we fully parallelized CPU-side and GPU-side work for inference without speculation, we found that the CPU-GPU balance for speculative decoding is more intricate. The main challenge arises from the fact that the number of accepted tokens determines the sequence length for a subsequent run, introducing a difficult-to-avoid GPU-to-CPU synchronization point. We experimented with different scheduling schemes in order to best hide the latency of CPU work.Draft-Target ScheduleDespite being smaller than a target model, when an entire LLM is used as the draft, it still introduces considerable latency on the GPU, providing some headroom to hide expensive CPU operations. Since smaller models do not benefit from tensor parallelism, there is a mismatch between the number of ranks a target and a draft are sharded across. In our implementation, the draft model runs only on the leader rank of a TP group.As indicated before, a decode step carries over logits into the next run. This allows us to overlap one execution of the draft model with the CPU-side batch scheduling work. After the batch is put together, repeated calls to the sampler and the draft produce the draft tokens. In parallel, the batch for verification is put together for the target model and synchronized with the parallel workers. The target logits are verified and sampled to determine the accepted sequence lengths. At this point, GPU-to-CPU synchronization is necessary in order to determine subsequent sequence lengths. Since the draft model is only run on the leader node, its batch is set up sequentially and its execution is kicked off to populate its KV cache entries with the additional token that the target produced. The logits produced by this draft run in the current run will be used to sample the first draft token in the subsequent run. Most importantly, while the draft is running, the next batch can be scheduled.MTP Schedule for a Single TokenWhile the runtime does not yet provide Eagle-style draft tree exploration, we implemented a special case of this scheme, considering a linear sequence of draft tokens produced by a model the size of a single transformer decoder layer. This scheme can be used for draft prediction using the open-source weights of DeepSeek R1. The sub-case of predicting a single token is interesting, as large MTP layers achieve sufficiently high acceptance rates to justify their overhead.MTP scheduling is somewhat more complex, as the draft model is much faster, hiding less CPU-side latency. Additionally, the draft is sharded alongside the target model, requiring shared memory transfers for batch information. A run starts by transferring batch info and sampling the first token from carry-over logits, similarly to the previous scheme. Next, the target is run to validate tokens, processing 2 * D tokens, where D is the decode batch size. This is ideal for micro-batching in Mixture-of-Experts (MoE) models over slower interconnects such as InfiniBand, as the batch splits evenly into two halves. The hidden states of the target carry over to the next draft run, while the logits are passed into the sampler for verification.By performing a limited amount of additional work on the GPU, we avoid CPU-to-GPU synchronization after draft sequence acceptance. After the input tokens of the targets are shifted, a kernel plugs in the next target tokens into their corresponding locations. The draft is then re-run with the same batch information as the target, populating KV cache entries and building the logits and hidden states for the next run, doing some redundant work on tokens which were not accepted. In these situations, the latency of the unused work is barely measurable due to the small size of the draft model. In parallel with the draft run, sequence lengths are determined on the CPU and the scheduling of the next batch is kicked off, without having to wait for GPU work to terminate.The overhead of additional work in the draft layer is not noticeable in attention, however MLP layers are more problematic. Since matrix multiplication instructions pad to a boundary of 64 along the dimension of number of tokens, if doubling doesn’t require significantly more blocks, the overhead is hidden. For longer draft sequences the overhead is more expensive and the scheme used for regular draft-target models works better.ReferencesFast Inference from Transformers via Speculative DecodingEAGLE: Speculative Sampling Requires Rethinking Feature UncertaintyEAGLE-2: Faster Inference of Language Models with Dynamic Draft TreesEAGLE-3: Scaling up Inference Acceleration of Large Language Models via Training-Time TestMedusa: Simple LLM Inference Acceleration Framework with Multiple Decoding HeadsLayer NormalizationFlashInfer: Efficient and Customizable Attention Engine for LLM Inference ServingDeepSeek-V3 Technical Report'),\n",
       " Document(metadata={'source': 'https://research.perplexity.ai/articles/rl-training-for-math-reasoning'}, page_content='Reinforcement Learning (RL) algorithms, especially proximal policy optimization (PPO) and Group Relative Policy Optimization (GRPO), have proven to be essential for improving model capabilities in reasoning related tasks. In this blog we’d like to share the learnings and decision reasonings we experienced when developing RL infra as well as training math-reasoning models with RL. For illustration purpose, the results we show below are based on smaller open source models, but most of them apply to larger models as well.The goal of the RL model training exploration is two-folds: 1) share our lessons and learnings on how to train models to hit the state-of-the-art math reasoning performance. This equips the team with the right knowledge on data manipulation, data mixing recipes, training best practices, RL algorithm nuances, and general performance optimization experience. 2) Apply these learnings to real production use cases to improve Perplexity products.A summary of the key findings:Infrastructure: we’ve developed GRPO algorithm on torchtune library as well as the Nemo suite, with VLLM-based rollout integrated. Nemo will be our short term go-to infra for RL training, while we develop torchtune GRPO support, which will be our preferred infra in the longer-run, for self-contained maintenance (no external dependency) as well as simpler framework architecture.Math dataset vested: gsm8k, math, NuminaMath, Open Reasoning Zero (ORZ), AIME series.Math reasoning model training:Data mixture of different difficulty levels mattersRL proves to be able to further improve large language models (LLMs) reasoning capability beyond supervised fine tuning (SFT)The capability of base model matters a lot. In particular, long-CoT capability of the base model is important for further scaling with RL.A good SFT starting checkpoint helps the above. Light SFT serves two purposes:enable the RL base model to be comfortable with generating long-CoT responses. Without this ability, when RL forces the model to scale it’s response length, self-repeating collapse would happen.teach some reasoning capability to the base model, enabling the RL process to start higher and learn faster from the start, compared to a model that only knows to generate long-CoT responses with weak reasoning capability.Training infrastructure explorationDespite several open source RL training frameworks being available, many of them do not fit our situation. Ideally we’d want the following properties:Scales well with the large model sizes of our production models.Good training speed optimizations.Easy to implement new algorithms and maintain without too much external dependencies.Simpler and extendable framework architecture design.Ideally unify with SFT training framework.Framework comparisonA comparison of the frameworks that we considered [the following table comparison was done in Feb 2025. Note that a lot of the missing algorithms were later implemented.]:We chose Nemo Aligner as a short term option, and ruled out the rest due to the following reasons:Nemo-Aligner: due to the most complete features already implemented, as well as partnership support from Nvidia, we chose this option as our short term focus. However, the complex setup with dependencies on multiple repos puts some overhead on maintenance.torchtune: this is the SFT framework we use at Perplexity. The framework is elegantly designed and easy to extend in general. However, due to being fairly new, the framework lacks a lot of features to be added. We aim to shift to torchtune for RL in the long-run. Once we get Nemo-aligner to a good state, we will invest in maintaining an in-house version of torchtune with our own implementation of desired algorithms.VeRL: although integrates both FSDP and more powerful Megatron-LM backend, the latter support is very limited due to the community’s demand being mostly on smaller models where FSDP is sufficient. FSDP generally has weaker support for tensor-parallelism, which is crucial for larger models, especially in RL training. However, VeRL quickly become a popular choice for the community, and has developed significantly in the recent months. Given its selling points on throughput optimizations, and multiple recent papers on reasoning/agentic model training based on this framework (e.g. [1], [2]), it’s worth revisiting this option in the near future.openRLHF: popular in academic community. However, the DeepSpeed backend makes it less scalable to large models. We’ve ruled out this option.Algorithm development and validationIn this section, we first provide a brief introduction to the GRPO algorithm and discuss the associated technical enhancements that contribute to its implementation complexity. Subsequently, we describe the infrastructure developed to address these challenges.Comparison of PPO vs GRPO. Reference: https://arxiv.org/abs/2402.03300PPO is a popular RL algorithm widely used for fine-tuning LLMs due to its simplicity and effectiveness, as depicted above. However, PPO’s reliance on a separate value model introduces significant computational and memory overhead.To address this limitation, GRPO modifies PPO by removing the separate value model and introducing group-based advantage calculation, as illustrated in figure above. Specifically, generates multiple candidate responses for each input question, computes their reward scores collectively, and determines advantages relative to these grouped outputs. While such innovation simplify certain aspect, they introduce new implementation complexities, such as efficiently generating multiple long-sequence rollouts.Implementation detailsDespite similar RL algorithms (PPO, Reinforce) already in-place in Nemo-Aligner, it’s surprisingly time consuming to make GRPO work properly for our case. A summary of improvements we did include:GRPO algorithm implementation;A more robust KL-divergence estimator (details);Incorporate a format reward and enhance rules for mathematical accuracy (covering both numerical answers and symbolic ground-truth expressions);Work with Nvidia support team to integrate VLLM-based rollout, which improved rollout efficiency by 30% and also fixed the buggy TensorRT-LLM infra that had log-probability mismatch (see next point)Log-probability alignment with HFNote: this effort was by-far the most time-consuming. In order to ensure code correctness, we adopt the following metric to verify the computed log-probabilities from Nemo-Aligner is correct:where L is the length of a rollout, the first 𝓁𝑜𝑔𝓅ᵢ is the log-probability on the i-th token from Nemo-Aligner, and second 𝓁𝑜𝑔𝓅\\'ᵢ is the reference log-probability we get by manually running model.forward on a huggingface model directly. This metric needs to stay very close to 1.0. We went through several iterations of nemo model converter, repo update, and image rebuilding, to reduce the metric from 1e5 down to a normal range within [1, 1.05].Multiple rounds of hyper-parameter searches to optimize for memory. Due to the fact that GRPO requires multiple samples per-prompt, as well as math reasoning rollouts are usually long, we often end up with cuda OOM issue. Hyper parameters, especially parallelism setup, needs to be carefully picked to ensure smooth training.Minor issues with dataloader consumption early stopping, and tensor shape issues in corner cases.ExperimentsIn this section, we present the experimental setup and results, building upon the previously described infrastructure.Experimental setupWe evaluate the models on the MATH-500 dataset using the pass@1 metric defined as follows, and report results specifically for pass@1. During evaluation, we set the sampling temperature of 0.7 and a top-𝑝 value of 0.95 to generate k rollouts.where 𝒫𝒾 denotes the correctness of the i-th response. The training and evaluation prompts are listed below, respectively. Specifically, we are not using system prompt, i.e. the following prompts are prepended into the user message.# training prompt\\nTRAINING_PROMPT = (\\n    \"A conversation between User and Assistant. The user asks a question, and the Assistant solves it. \"\\n    \"The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. \"\\n    \"The reasoning process and answer are enclosed within <think></think> and <answer></answer> tags, respectively, \"\\n    \"i.e., <think>reasoning process here</think> <answer>answer here</answer>. User: {question} Assistant: \"\\n)# Evaluation prompt\\nEVALUATION_PROMPT = (\\n    \"You will be given a problem.Please reason step by step, \"\\n    \"and put your final answer within \\\\\\\\boxed{}. \"\\n)Additional experimental configurations are described below.Model SpecificationsWe primarily experimented with dense Llama models, selecting our base models from the following two variants due to their suitable size and widespread adoption within the research community. These models serve as baseline references for reasoning performance in our subsequent experiments:Llama 3.1 8B InstructLlama 3.1 8B BaseDataset DetailsThe datasets collected and used in our experiments are as outlined below.SourcesSizeLabeling ModelsDifficultyUsageGsm8k~7kQwQ 32B preview & R1EasyTrainingMATH~7kQwQ 32B preview & R1MediumTrainingOrz (contains MATH)~57kQwQ 32B preview & R1HardTrainingnuminamath~250K (filtering)NoneHardTrainingMATH-500500NoneMediumEvaluationTraining ProcedureThis subsection describes the specific training methods, specifically, the hyper-parameter selection, employed during GRPO-based fine-tuning.Hyperparameters:We used the GSM8K dataset for the training in this sub-section.Learning Rate (LR) and KL coefficientWe present accuracy results for various combinations of learning rates (LR) and KL coefficients below. (Note that some experimental runs terminated prematurely due to infrastructure-related errors, resulting in slight discrepancies in total training steps.)Learning Rate/KL coefficient0.10.010.0018e-80.4260.4340.4221e-70.4320.4920.4143e-70.4700.4880.4705e-70.4440.480.486Analysis of these results indicates that a learning rate of 3e-7 consistently demonstrates stable performance across different KL settings, thus it was adopted for primary experiments. Moreover, experiments revealed improved outcomes with lower KL values, leading us to eliminate the KL term entirely in subsequent primary training runs. Higher learning rates such as 5e-7 and 8e-7 tended to accelerate early convergence, limiting the development of long-chain-of-thought (CoT) reasoning capabilities in late stage.TemperatureTemperatures tested: [0.7 (dark red), 0.8 (red), 1.0 (green), 1.1 (purple), 1.2 (blue), 1.3 (grey)]The figures below are the training accuracy (left) vs the validation accuracy (right).Temperature0.70.811.11.21.3Validation0.5020.480.4520.4420.1840.082Conclusions:Based on the empirical results presented above, we select a temperature of 1.0 as the default setting, balancing convergence speed and overall accuracy.Higher temperature settings (e.g., 1.2, 1.3) negatively impact convergence, resulting in instability and failure to achieve satisfactory performance.Large temperature like, 1.2, 1.3 will make the model fail to convergeWhile lower temperature values (e.g., 0.7, 0.8 )facilitate rapid initial convergence, these settings typically lead to premature saturation, causing limited performance improvement on the validation set during later stages of training.Reward modelsIn our preliminary experiments, we evaluated the following two rule-based reward models proposed in the DeepSeek paper:Accuracy reward: This reward function assesses the correctness of the model\\'s output by comparing the assistant\\'s provided answers with predefined ground-truth solutions.Format reward: This reward model encourages the language model to structure its responses in a predefined, consistent format. In our experiments, the desired format requires the LLMs to explicitly separate the reasoning process from the final answer by enclosing them within specific XML-like tags. More concretely, the expected format is shown in TRAINING_PROMPT listed above.ResultsHere are the training setup for our primary experiments:LR = 3e-7, KL coefficient = 0, Temperature = 1, only Accuracy reward.Main resultThe figure below presents the primary results from this study, showcasing validation accuracy comparisons across three experimental conditions:Validation on three RL setups: Left: train from SFT-warmed-up model; Mid: train from Llama 3.1 8B instruct; Right: train from Llama 3.1 8B BaseThe three lines shown in the figure corresponding to the following setting[Red] RL fine-tuning on the ORZ dataset initialized from the ORZ-supervised fine-tuned warm-up checkpoint (epoch 4). The SFT data is labeled by QwQ-32B-preview model, which is a CoT reasoning model.[Green] RL fine-tuning on the ORZ dataset initialized from the vanilla Llama 3.1 8B instruct checkpoint.[Blue] RL fine-tuning on the ORZ dataset initialized from the vanilla Llama 3.1 8B base checkpoint.The results highlight two key findings:RL significantly enhanced the models\\' mathematical reasoning capabilities, as evidenced by improvements in validation accuracy and increased generation lengths. (Notes: the accuracy of Llama 3.1 8B instruct and ORZ tuned model are about ~0.38 and ~0.60, respectively. The best RL result, Red line in the above graph, achieved 0.70 score on MATH500, which matches similar results in the literatures, e.g. [1],\\xa0[2])Prior exposure to long CoT reasoning data during supervised fine-tuning or the pre-training stage substantially accelerated the efficacy and efficiency of subsequent RL fine-tuning. From the curves above, it’s clear that the long-CoT exposure from SFT gets the Red line to start and plateau much higher than the other two. (Note: Contrary to the claims in some papers, our observations suggest that SFT enhances the efficiency of subsequent RL stages. While we hypothesize that SFT may also improve the upper-bound performance of the base model, we have not yet trained the model sufficiently to validate this hypothesis conclusively.)Analysis of Observed PhenomenaDuring our experiments, we observed multiple noteworthy phenomena. Here we discuss these phenomena in detail.Learning Rate MattersWe found that learning rate in GRPO generally controls 2 things: 1) how fast the learning picks up, and 2) how stable the convergence is. Smaller learning rate causes the learning to progress slower, which is expected. However, with a big learning rate, even though the learning curve climbs fast in early steps, it often leads to model collapse if continuously trained after convergence. In the graph below, the 3 lines corresponds to 3 different learning rates:Purple: 8e-7,Green: 3e-7,Blue: 1e-7.With lr=8e-7, the learning plateaus very early, but then causes model to collapse (and response length gets exploded) very fast. With lr=3e-7, learning reaches the same level near the end of 15k steps, and was stable, while with lr=1e-7, the learning is too slow that the training didn’t converge within 15k steps. (All three runs are using adam optimizer, learning rate schedule is constant).Validation reward (i.e. accuracy) of three runs with different learning rate: Purple: 8e-7, Green: 3e-7; Blue: 1e-7We will look further into why the reward curve collapses in the following Model Collapse section.Model CollapseWe define model collapse as the sudden deterioration in the generative capabilities of a model during RL training. It’s observed in two primary forms during our experiments: self-repetition collapse and end-of-sequence collapse.Self-repetition collapse: This form of collapse emerged predominantly in experiments that incorporated a format reward. Initially intended to guide the model toward producing well-structured and interpretable outputs, the format reward required the model to organize its responses using distinct <think></think> and <answer></answer> tags. However, while promoting structural clarity, we found that this reward signal inadvertently increased the likelihood of collapse via repetition.Initially, we employed a relatively loose format reward—requiring at least one <think> block and one <answer> block—to encourage adoption of the desired response structure. In the following example, although the final reward converges well, the rollouts started to have severe repetition issue when training steps go beyond 110.Validation format reward (left), math accuracy (mid) and rollout length (right) with loose format rewardA closer look at the rollouts reveals that after convergence, the model generates a normal thinking process (something without the <think> tags), then the correct answer, then a repetition of the format blocks with empty content (see example below).<|start_header_id|>assistant<|end_header_id|><think>\\nLet\\'s denote the positive integer as n. When n is expressed in base 8, it forms the number $AB_8$, which can be written as $8A + B$. When n is expressed in base 6, it forms the number $BA_6$, which can be written as $6B + A$. Since both expressions represent the same number n, we can set them equal to each other:\\n\\n  $8A + B = 6B + A$\\n  \\n  Subtracting A from both sides gives:\\n\\n  $7A = 5B$\\n\\n  Dividing both sides by 7 gives:\\n\\n  $A=\\\\frac{5}{7}B$\\n\\n  Since A and B are digits, they must be integers. The only way for A to be an integer is if B is a multiple of 7. The smallest possible value of B that satisfies this condition is B = 7.\\n\\n  Substituting B = 7 into the equation A = (5/7)B gives:\\n\\n  $A=\\\\frac{5}{7} \\\\times 7 = 5$\\n\\n  Now that we have the values of A and B, we can find the decimal value of n. We know that n in base 8 is represented as 57, so the decimal value of n is:\\n\\n  $8 \\\\times 5 + 7 = 40 + 7 = 47$\\n\\nTherefore, the positive integer in decimal is $\\\\boxed{47}$.\\n</think>\\n</answer>\\n</think>\\n</answer>\\n</think>\\n</answer>\\n</think>\\n</answer>\\n</think>To address this, we experimented with a stricter format reward requiring exactly one <think> block followed by one <answer> block. However, this adjustment exacerbated the problem: not only did repetition persist, but the accuracy of the answers also deteriorated [link].Validation format reward (left), math accuracy (mid) and rollout length (right) with strict format rewardOne illustrative example is attached below:<|start_header_id|>assistant<|end_header_id|><think>\\nTo find the matrix $\\\\mathbf{M}$, we can use the property of linear transformations that represents the transformation of vector $\\\\mathbf{v}$ as $-4\\\\mathbf{v}$. This means that the matrix $\\\\mathbf{M}$ must be a scalar multiple of the identity matrix, where the scalar is $-4$.\\n</think>\\n<answer>\\n\\\\boxed{\\\\begin{pmatrix} -4 & 0 & 0 \\\\\\\\ 0 & -4 & 0 \\\\\\\\ 0 & 0 & -4 \\\\end{pmatrix}}\\n</answer>\\n\\\\boxed{\\\\begin{pmatrix} -4 & 0 & 0 \\\\\\\\ 0 & -4 & 0 \\\\\\\\ 0 & 0 & -4 \\\\end{pmatrix}}\\n</think>\\n........\\n<answer>\\n\\\\boxed{\\\\begin{pmatrix} -4 & 0 & 0 \\\\\\\\ 0 & -4 & 0 \\\\\\\\ 0 & 0 & -4 \\\\end{pmatrix}}\\n</answer>\\n\\\\boxed{\\\\begin{pmatrix} -4 & 0 & 0 \\\\\\\\ 0 & -4 & 0 \\\\\\\\ 0 & 0 & -4 \\\\end{pmatrix}}\\n</think>\\n<answer>\\n\\\\boxed{\\\\begin{pmatrix} -4 & 0 & 0 \\\\\\\\ 0 & -4 & 0 \\\\\\\\ 0 & 0 & -4 \\\\end{pmatrix}}\\n</answer>\\n\\\\boxed{\\\\begin{pmatrix} -4 & 0 & 0 \\\\\\\\ 0 & -4 & 0 \\\\\\\\ 0 & 0 & -4 \\\\end{pmatrix}}</answer>Based on these observations, we initially hypothesized that the format reward itself was the primary cause of the collapse. To test this, we conducted follow-up experiments with the format reward entirely removed which leads us to the following end-of-sequence collapse.End-of-sequence collapse: this collapse occurs when the model reaches the maximum allowed token generation limit, subsequently causing a performance breakdown.This phenomenon likely results from ill-defined loss scenarios, where multiple outputs from a single input query uniformly receive identical rewards. This happens when all rollouts of a prompt are cut-off, causing the reward to all be 0, thus the advantage is also 0. Let’s take a further look at the reward curve collapsing example in the Learning Rate Matters section. In this case, KL coefficient is set to 0. For prompts whose rollouts are all cut-off, it’s impossible to get the policy away from this state, because policy gradient becomes 0 [see Eq 2 in Limitations section]. The math accuracy starts to decrease as more rollouts get cut-off, and then got stuck in this situation. In the following graphs, we can see a clear correlation between the cut-off rate and train rollout reward drop.End-of-sequence collapse. Training reward collapse aligns with rollout cut-offNote that the end-of-sequence collapse is also observed in Demystifying Long Chain-of-Thought Reasoning in LLMs, even with the KL-term added. Their paper proposed a reward shaping approach that tries to control the rollout sequence length. However, our experiments show that while reward shaping successfully suppressed the generation length (length collapse), it hindered model from learning the correct reasoning and answer, causing much lower math accuracy throughout training:Validation accuracy and response length with reward shapingIn the Limitations section, we will propose new ways to handle this situation. Combining the previous self-repetition example and main results above, it seems with a based model that’s not exposed to long CoT reasoning answers, RL would cause the model to brokenly scale up response length and eventually collapse with repetition.Model CollapseWe conducted an ablation study to investigate the impact of SFT on subsequent RL performance. Models were first fine-tuned using datasets of varying difficulty levels—ORZ (difficult), MATH (medium), and GSM8K (easy)—followed by RL fine-tuning exclusively on the ORZ dataset.Validation accuracy on different initial models[Blue]: SFT on ORZ (difficulty: difficult) data then RL on ORZ[Red]: SFT on MATH (difficulty: medium) data then RL on ORZ[Green]: SFT on GSM8K (difficulty: easy) data then RL on ORZThe results clearly indicated that employing a diverse mixture of difficulty levels in the SFT stage significantly enhanced RL training efficiency.The outcomes from our ablation study suggest valuable future research directions, particularly exploring curriculum learning strategies.One potential avenue is examining curriculum design within a single domain vs capability, gradually increasing the complexity or difficulty to progressively enhance model competence.Another intriguing direction involves investigating curriculum strategies that span multiple capabilities or domains—for instance, initially developing instruction-following capabilities before progressing to more complex reasoning tasks.Understanding the efficacy and interaction of these curriculum strategies could significantly optimize training efficiency and ultimately enhance overall model performance. (Such kind of strategies have been employed by QwQ 32B, as illustrated in their technical report)Rollout Length ScalingAs seen above as well as in many literatures (e.g. R1 paper), RL intrinsically encourage rollouts to become longer (unless specific length-control reward is used). However, we found that RL does not blindly increase rollout length - rather it tries to find an optimal length based on the given initial model. For example, the green graph below shows the length scaling using llama3.1 8B base model, while the purple graph uses the same llama3.1 8B model but warmed-up on ORZ data labeled by a CoT labeler (QwQ-32B-preview). The former model was not trained on CoT data at all, and tends to generate short responses from the start. In order to achieve higher math accuracy, RL encourages the model to output longer responses. On the other hand, the latter model, due to SFT effect, tends to generate very long responses from the beginning. RL actually suppressed generation length, while math accuracy also increases.Rollout length scaling with different initial model: Llama 3.1 8B base (left) and SFT-warmed-up (right)LimitationsThe GRPO loss function and corresponding gradient update equations are provided below, highlighting potential biases introduced during gradient updates.Advantage Estimation BiasesWe note that the way advantage Âᵢ,ₜ  is estimated can significantly affect model training. Specifically, normalizing by the standard deviation of group-level rewards introduces two key issues:Vanishing Advantage for Uniform Responses. When all answers for a given query yield identical rewards, the resulted gradient becomes zero, causing the model to fail to update itself. A proposed solution is adjusting the reward calculation as follows:Unequal Gradient Scaling Across Queries. Queries with low reward variance (typically easier questions) result in greater Âᵢ,ₜ , leading to disproportionately large advantage estimates and hence stronger gradient updates. In contrast, harder queries (with high variance) contribute less to learning. This introduces a bias in training favoring easy examples.We plan to try these additional algorithm level modifications as our next steps.ConclusionIn this work, we systematically explored reinforcement learning (RL) approaches for enhancing mathematical reasoning in large language models, focusing on the development and deployment of the GRPO algorithm within modern RL training infrastructures. Our experiments demonstrate that RL, particularly when initialized from models exposed to long chain-of-thought (CoT) reasoning via supervised fine-tuning, can substantially boost math reasoning accuracy and response quality. We identified that both the choice of base model and the diversity of supervised data critically impact RL efficiency and final performance. While our implementation addressed key technical challenges—such as rollout efficiency and log-probability alignment—issues like model collapse and advantage estimation bias remain open areas for further research. Overall, our findings provide a robust foundation for future work on curriculum strategies and algorithmic improvements, with the goal of advancing state-of-the-art math reasoning capabilities in language models.'),\n",
       " Document(metadata={'source': 'https://research.perplexity.ai/articles/lower-latency-and-higher-throughput-with-multi-node-deepseek-deployment'}, page_content='In most systems, latency and throughput are often conflicting goals that require trade-offs during design and deployment. For example, in dense large language models, increasing batch size can improve throughput but also increases latency; increasing tensor parallelism within a single machine can reduce latency but decreases the number of replicas, leading to lower throughput.Mixture of Experts (MoE) models like DeepSeek-V3/R1 have recently demonstrated excellent model capabilities and operational efficiency. For instance, the DeepSeek-V3/R1 model has 671B parameters in total, but each token only uses 37B parameters during inference. This model architecture presents both challenges and opportunities for inference systems.This article demonstrates that, contrary to conventional systems, MoE models like DeepSeek-V3/R1 can simultaneously achieve higher throughput and lower latency when utilizing more GPUs in multi-node deployments across most scenarios.Deployment ArchitecturesDue to the large number of small experts the model has, deployments must be spread across multiple devices. We considered both single-node deployments on a single node with 8xH200 GPUs and multi-node deployments on 8xH100 GPUs.Both deployment architectures leverage Data Parallelism, orchestrated through our in-house request scheduler. Data parallelism implementation involves launching multiple inference engine instances, each operating independently to serve and maintain requests. The request scheduler, which interacts with the engine through GRPC, is responsible for spreading out requests as evenly as possible, while also facilitating KV re-use, sending requests with partial matched prefix to the servers containing the cache. Engine instances do not span multiple nodes. They can optionally use tensor parallelism to shard attention across multiple devices. The instances are inter-connected via NVLink in the single-node case or InfiniBand for the multi-node case, dispatching and collecting experts.The single-node deployment configuration delivers superior latency with small batch sizes; however, performance degrades rapidly under increased load conditions.To deploy the serving engine, we launch one pod per node hosting multiple engine instances. PyTorch is responsible for setting up the distributed communication and negotiating the NVSHMEM initialization. For communication, we rely on custom CUDA kernels described in an earlier blog post. The implementation of the two deployments is virtually identical, with the model picking the correct kernels to use based on the fabric implementing expert parallelism.Parallelization TechniquesBefore diving into our performance comparisons, it\\'s essential to understand the key parallelization strategies that make deploying massive MoE models like DeepSeek-V3/R1 possible.Tensor ParallelismIn LLM inference, Tensor Parallelism (TP) is typically used to reduce memory usage and computation per GPU, thereby reducing latency. Usually, we can shard Linear Projections in Attention and MLP Layers along row or column dimensions, and shard Attention operations along the attention head dimension.With TP, Llama-3 architecture has no duplicated computation for Linear Projection and Attention operations across GPUs, which is an ideal sharding method. However, in DeepSeek-V3/R1 models, TP cannot achieve this.DeepSeek-V3/R1 models use Multi-Latent Attention (MLA). An MLA Layer first uses a Linear Projection kv_a_proj to compute the latent vector, then uses another Linear Projection kv_b_proj to transform it into the space of each attention head. Since all attention heads share the same latent vector, TP cannot shard the latent vector, so all TP Ranks need to replicate the parameters and computation of kv_a_proj and kv_b_proj. Similarly, since MLA stores the latent vector in the KV Cache, each TP Rank stores an identical copy of the KV Cache.Despite some duplication in MLA, Tensor Parallelism still provides partial reduction in computation demands, rendering it valuable for scenarios requiring high output speeds.Expert ParallelismDeepSeek-V3/R1 models replace MLP Layers with MoE Layers. An MoE Layer has 256 routed experts and one shared expert. Each token is dispatched to 8 different routed experts for computation, and the results are weighted summed. Each token also computes in the shared expert, and the result is added to the result from the routed experts.Expert Parallelism (EP) serves as the typical sharding approach for MoE Layers, with each GPU managing 256 / EP routed experts while maintaining a copy of the shared expert. Compared to TP, the advantage of EP is that it can distribute computation across more GPUs, reducing the computation and memory usage per GPU.Before performing expert computation, all GPUs need to perform an AllToAll communication to dispatch tokens to the GPUs where the corresponding experts are located; after expert computation, another AllToAll communication is needed to collect computation results from various GPUs and perform weighted summation. We implemented an optimized version of these two AllToAll communication Kernels, Dispatch and Combine, using NVSHMEM. In our previous blog post, we detailed the implementation, and our kernels have been open-sourced on GitHub.Data ParallelismWith EP, we can distribute MoE computation across 128 or even more GPUs. However, MLA computation cannot be partitioned with EP. At this point, we can introduce Data Parallelism (DP). Each DP Group has a complete copy of MLA Layer. Each DP Group accepts different inputs and performs MLA Layer computation independently.MLA layer\\'s DP and TP can be combined, with one DP Group being split into multiple TP Ranks. MoE layer\\'s EP can be combined with MLA Layer\\'s DP/TP. EP = DP * TP. For example, on 16 machines, EP128 DP32 TP4 means distributing routed experts across 128 GPUs, with every 4 GPUs forming a DP Group, for a total of 32 independent DP Groups.Single-Node vs Multi-NodeDeepSeek\\'s 671B parameters exceed the memory capacity of a single 8-GPU H100 machine (80 GB * 8), but a single 8-GPU H200 machine can fully accommodate the entire model (141 GB * 8). Using the EP8 DP8 TP1 configuration, the model uses about 100 GB of memory per GPU, leaving approximately 40 GB for KV Cache and other intermediate results. One token occupies 70,272 bytes of KV Cache. Assuming each request has 5,000 tokens, each GPU can accommodate roughly 100 requests.We wanted to understand the performance differences between single-node and multi-node deployments under different configurations. We used one H200 machine for single-node deployment and up to 16 H100 machines for multi-node deployments. For each deployment environment, we used combinations of TP 1, 2, 4, 8, and batch sizes per GPU of 1, 2, 4, 8, 16, 32, 64, 128. We assumed each request had a KV Cache length of 5,000 tokens. We also assumed Multi-Token Prediction (MTP) predicts 1 additional token (i.e., each request\\'s query length is 2), and conservatively assumed an acceptance rate of 60%. The figure below shows the throughput and output speed for different configurations.The horizontal axis represents output speed per request in tokens/s. The vertical axis uses a logarithmic scale to show throughput per machine in tokens/s. We marked the Pareto Frontier for each EP configuration with different colored lines.In scenarios with extremely high output speed requirements, using single-node EP8 DP1 TP8 with a batch size of 1 can achieve an output speed exceeding 100 tokens/s, but the throughput is extremely low, equivalent to the output speed. In this scenario, the entire batch has only 2 tokens, which can be dispatched to at most 2*8=16 experts, activating a total of at most 57B parameters.In the output speed range of 80-40 tokens/s, as throughput increases, output speed decreases significantly. In contrast, EP128 has about 5x higher throughput than single-node deployment at the same output speed.This phenomenon can be explained by examining how single-node deployments behave: increasing batch size directly correlates with an increase in activated experts. When the batch size is 1, the average number of activated experts per GPU is 2 * 8 / 8 = 2. When the batch is large enough, all experts are activated, meaning each GPU activates 256 / 8 = 32 experts. Activating more experts means the GPU needs to read more parameters from memory, significantly increasing memory bandwidth pressure. Since the decode phase of large language models is already bottlenecked by memory bandwidth rather than compute performance, increasing batch size in single-node deployment significantly reduces output speed.Comparison of the four multi-node deployment configurations (EP16, EP32, EP64, and EP128) reveals that higher EP values shift the Pareto Frontier toward simultaneous improvements in throughput and output speed.Using a higher EP number means each GPU is allocated fewer experts. For example, EP128 means that each GPU is responsible for 256 / 128 = 2 experts, so the memory bandwidth pressure is significantly reduced. In other words, by using a larger EP number, we effectively gain more memory bandwidth. When the per-GPU batch size is less than 64, increasing the batch size doesn\\'t significantly affect expert computation speed because increasing the number of inputs doesn\\'t significantly increase memory bandwidth pressure. Therefore, we observe that when using EP128, increasing batch size doesn\\'t affect output speed as significantly.Interestingly, on larger batch sizes (64 requests per GPU), we observed a new phenomenon: single-node deployment throughput is slightly higher than multi-node deployment. Part of the reason is that intra-node NVLink has higher bandwidth than inter-node InfiniBand. Another part is due to limitations in our implementation. We will analyze this phenomenon in more detail later.Due to memory capacity limitations, the EP8 DP8 TP1 configuration cannot reach a batch size of 128 per GPU, so multi-node deployment is still a better choice in scenarios pursuing higher throughput.Computation and Communication OverlappingAs briefly introduced above regarding Expert Parallelism, GPUs are idle during MoE Layer communication. To reduce waste and lower latency, we need to find data-independent computation tasks to fill this idle time.The upper part of the above figure shows the computation flow of one layer. MoE computation depends on Dispatch, and the next layer\\'s computation depends on the result of Combine.We place the shared expert on each GPU. This way, shared expert computation doesn\\'t require AllToAll communication. Therefore, we can perform shared expert computation immediately after Dispatch Send, then wait for Dispatch Recv to complete. We call this overlap scheme \"Dispatch Overlap\".Dispatch Overlap offers straightforward implementation and broad applicability. This technique hides shared expert computation time across all EP sizes and batch sizes.To further increase computation and communication overlap, we used micro batching mentioned in the DeepSeek technical report to break data dependency. As shown in the lower part of the figure, we divided the computation of one Transformer Layer into 5 stages:Stage 1: InputNorm, QKVProj, AppendKV, BMMStage 2: BMM, Attn, OProj, PostNorm, GateStage 3: Dispatch Send, Shared ExpertStage 4: Dispatch Recv, MoE, Combine SendStage 5: Combine RecvIn the first 3 Dense Transformer Layers, we use the whole batch. In the following 58 MoE Transformer Layers, we evenly divide the batch into two micro batches. The two micro batches execute alternately, offset by 3 stages. Since there is no data dependency between these two micro batches, we can switch to another micro batch\\'s computation after Dispatch Send and after Combine Send.Latency BreakdownNext, we compare the effects of overlapping through an experiment, as well as compare the performance differences between single-node deployment EP8 and multi-node deployment EP128. For ease of comparison, we used H100 GPUs for the following experiment. We used TP1, a batch size of 128 per GPU, a Query length of 2 per request, and a KV Cache length of 5000.The figure above shows the total time spent on one MoE Transformer Layer and the latency proportion of different types of kernels. Except for Dispatch, Combine, and GroupGEMM, the execution time of other kernels should be equal in the EP8, EP128 NoOverlap, and EP128 DispatchOverlap series because the batch size is the same.OverlappingLet\\'s first compare the effects of the three overlapping methods. NoOverlap took 2667µs in total, DispatchOverlap took 2651µs, saving 16µs or only 0.6%. MicroBatch showed a very significant improvement, taking 1896µs, a 29% speedup. Both Dispatch and Combine time were significantly reduced. Dispatch decreased from 593µs to 367µs, and Combine from 1012µs to 237µs.Note that for computation kernels, splitting a batch of size 128 into two batches of size 64 increases the total execution time. Therefore, although the time spent on communication reduced by 1001µs, the total time only reduced by 771µs. We will explain the reason using the Roofline model in the following section.For this reason, microbatching doesn\\'t always improve performance.The figure above shows the performance improvement of Microbatch compared to DispatchOverlap for batch sizes 4-128. When the batch size is less than 32, Microbatch decreases performance by 5%-40%. When the batch size is greater than or equal to 32, Microbatch can improve performance by 10%-35%.EP8 vs EP128Let\\'s return to the previous figure and compare EP8 and EP128 Microbatch. EP8 took 1802µs in total, slightly less than EP128\\'s 1896µs. Besides the increased kernel execution time brought by Microbatch mentioned above, the main differences are in GroupGEMM used for MoE computation, and the two communication kernels, Dispatch and Combine.EP8\\'s GroupGEMM took 555µs, while EP128\\'s GroupGEMM took 270µs, reducing by half. This is the core advantage of multi-node deployment.Unfortunately, the time spent on communication increased by 213µs, which greatly offset the advantage of GroupGEMM. In separate performance tests of our communication kernels, we found that they can only achieve half of the Infiniband bandwidth. We will continue to optimize our communication kernels.Another kernel that significantly lags is GEMM. Microbatch increased GEMM by 95µs. We will analyze GEMM in more depth in the Roofline section below. We believe that the current GEMM implementation has not yet achieved optimal performance.RooflineThe Roofline Model is a good tool for analyzing kernel performance. Its horizontal axis is Arithmetic Intensity, the ratio of FLOP to memory I/O bytes. The horizontal axis value can be calculated directly from the kernel\\'s semantics. The vertical axis represents achieved performance, calculated by dividing FLOP by benchmark latency.The theoretical upper bound of kernel performance is directly determined by the GPU\\'s specifications. The H100\\'s FP8 peak performance is 1979 TFLOP/s, represented as a horizontal line in the Roofline model. The H100\\'s memory bandwidth is 3.35 TB/s, represented as the slope of a line passing through the origin. The two lines give the performance limits for compute-bound and memory-bound kernels, respectively.Below, we discuss the performance of the GroupGEMM and GEMM kernels.GroupGEMMThe GroupGEMM kernel in MoE performs the following computation: There are g groups in total, the i-th group has m_i tokens, performing a matrix multiplication of [m_i, k] x [k, n] -> [m_i, n]. In performance testing, we assume that the number of tokens in each group is the same, denoted as m_i = m. Then the FLOP count for GroupGEMM is 2 * g * m * k * n, and the memory I/O bytes is g * (m * k + n * k + m * n).In the DeepSeek-V3/R1 model, there are 256 experts, and each token is dispatched to 8 experts for computation. Assuming a batch size of 128, query length of 2, using EP128 DP128 configuration, the average number of tokens received by each expert (i.e., m) is 128 * 2 * 8 * 128 / 256 = 1024. Similarly, we can calculate m for other configurations and batch sizes.We used DeepGEMM\\'s GroupGEMM implementation for performance testing. Test points covered combinations of EP8, EP16, EP32, EP64, EP128 configurations with TP1 and batch sizes 1-128.The figure above shows the Roofline model for GroupGEMM under different EP configurations. Different EP corresponds to different numbers of groups. The figure illustrates nearly overlapping performance lines, indicating that GroupGEMM performance is predominantly determined by the total token count (represented as g * m).The stars mark the data points corresponding to a batch size of 128 per GPU for each EP configuration. Comparing these starred data points, we can see that as EP increases (and DP increases synchronously), the number of tokens per expert m also increases. At EP8, m=128, while at EP128, m=2048.As m increases, Arithmetic Intensity also increases. In most configurations, GroupGEMM is limited by memory bandwidth, so increasing m improves performance.GEMMThe GEMM kernel corresponds to Linear Projections in the model, such as Q/K/V/O Projection. For a matrix multiplication of [m, k] x [k, n] -> [m, n], the FLOP count is 2 * m * k * n, and the memory I/O bytes is m * k + n * k + m * n. We can also test the latency for batch sizes 1-128.The figure above shows the Roofline model for GEMM under different EP configurations. We can see that GEMM performance is limited by memory bandwidth. As batch size increases, Arithmetic Intensity also increases, thus improving performance.MicrobatchWhen using microbatching, we divide the batch evenly into two parts. From the two figures above, we can see that when m becomes m/2, the efficiency of matrix multiplication decreases. Therefore, executing two matrix multiplications of size m/2 takes longer than executing one matrix multiplication of size m.Multi-Token PredictionThroughout this article, we have assumed the use of Multi-Token Prediction (MTP) for speculative decoding. MTP changes the query length per request from 1 to 2. For matrix multiplication, this is equivalent to changing m to m * 2, thereby increasing matrix multiplication efficiency. On the other hand, if we draw the Roofline model for MLA, we would find that increasing query length significantly increases MLA kernel efficiency.Therefore, the use of MTP plays an important role in model efficiency.Implementation & OptimizationsIn this section, we will introduce some implementation and optimization details for our DeepSeek-V3/R1 model.QuantizationDeepSeek-V3/R1 was natively trained on FP8 using a per-block quantization scheme, with weights quantized statically and activations quantized on-the-fly. Instead of computing a scaling factor per channel or per matrix statically, scaling factors are computed across 128-element vectors for activations and 128x128 element tiles for matrices, limiting accuracy degradation due to quantization.At Perplexity, we rely on a mix of CUDA and Triton kernels to support inference, with CUDA being used for the most performance-sensitive and infrequently modified kernels (such as attention and GEMM), with Triton implementing a wide range of activation, normalization and utility kernels. Triton allowed us to quickly adapt the kernels to the block quantization scheme.For linear and MoE layers, we mix the Deep GEMM kernels with our own Triton GEMM kernels, as we have noticed that for certain matrix dimensions and low batch sizes Split-K delivers lower latency. If the unquantized layer performs a (M, K) x (K, N) multiplication, it needs (M x ceil_div(K, 128)) x (ceil_div(K, 128), ceil_div(N, 128)) scaling factor for block quantization. For block quantization, the scaling factors for activations are computed on-the-fly, instead of being pre-calibrated. Since activation scaling factors are aggregated only along the K and not along the M dimension, kernels require only slight alterations to support the scheme.The SiLU activation function used by DeepSeek-V3/R1 required substantial changes to support CUDA graphs, block quantization and dynamically routed token counts. Block quantization can be problematic as it introduces horizontal reductions, however the kernel already chunked activations along their hidden dimension into blocks of 1024 elements. Within one block, the tensor to be quantized was further chunked in blocks of 128 to compute the largest absolute value, with Triton generating efficient cross-warp max reductions, adding minimal overhead.To support MoE routing under CUDA graphs, the kernels must be aware of the routing information indicating the number of tokens per expert, instead of scheduling work based on the size of the buffers which were allocated to hold the upper bound of the token counts. We cannot split the problem based on the input tensor dimensions, so we launch a fixed number of persistent kernels that read the routing information to determine how many tokens are populated and split the work of processing the activations among them dynamically.We have already upstreamed some of our kernels to the FlashInfer project and in the future we will be open-sourcing more of our code.MLA LayerWe use FlashInfer for MLA computation. FlashInfer supports flexible Page Table settings and extremely high performance.We fused q_a_proj and kv_a_proj into a single qkv_a_proj. Latency decreased from 15.4 µs + 14.8 µs = 30.2 µs to 16.7 µs.We decomposed kv_b_proj into two matrices, k_b_proj and v_b_proj. We wrote an FP8 Block Quantized BMM kernel for computations related to these two matrices.Cuda GraphCuda Graph can significantly reduce kernel launching overhead, which is crucial for performance. We create a Cuda Graph for each batch size.Before developing our AllToAll Kernel, we used torch.all_to_all_single() for AllToAll communication. This operation requires all GPUs to use the same batch size. However, different DP Groups may run different batch sizes.To ensure all_to_all_single() is compatible with different DP Groups using different batch sizes, we first used an allreduce() operation before each model run to get the maximum batch size among all DP Groups. Then we made all DP Groups use this batch size to run.Although this approach ensures we can use Cuda Graph, it has three disadvantages. First, it requires an additional allreduce() operation. Second, DP Groups with smaller batch sizes are forced to pad. Third, it makes our implementation code complex.After implementing our own AllToAll Kernel, we no longer require all GPUs to use the same batch size. Therefore, we no longer need to perform additional allreduce() operations or pad batch sizes.MoE RouterThe MoE router is implemented in Triton, relying on a modified sort derived from the standard library which also keeps track of the indices of the sorted elements. The implementation is shared across all MoE models, as Mixtral routing is a special case of the DeepSeek routes where the Top-K group is the same as the group of all experts. The sparse kernels consume the Top-K indices and scores directly, whereas dense dispatch/combine schemes relying on all-to-all require routing information to be aggregated per-expert instead of a per-token basis.Future WorkIn future work, we plan to further optimize the performance of the DeepSeek model.The most important next optimization is Prefill Disaggregation. The Prefill phase and Decode phase of the DeepSeek-V3/R1 model have very different computational characteristics. Both can use different optimization strategies and deployment schemes.For the MLA Layer, in the Decode phase, we use Matrix Absorption to reduce the FLOP count of MLA computation. In the Prefill phase, first projecting the latent vector into K/V space and then computing in Multi-Head Attention (MHA) form would perform better.If Prefill and Decode run on the same GPU, to reduce the impact of Prefill on Decode output speed, we typically use chunked prefill to divide the query into multiple chunks for Prefill. Because the KV Cache stores the latent vector, it becomes difficult to convert MLA into MHA form.For the MoE Layer, in the Decode phase, we use as large EP and DP as possible to increase the number of input tokens per expert, thereby improving GroupGEMM performance. In the Prefill phase, because the number of tokens is already large enough, GroupGEMM is already compute-bound. Therefore, for Prefill, we can use smaller EP and DP.If Prefill and Decode run on the same GPU, as long as any DP Group is performing Prefill, the latency of MoE Layers on all GPUs will increase, significantly affecting Decode output speed.Besides Prefill Disaggregation, we also plan to optimize the following aspects:AllToAll Performance: Our AllToAll kernel currently can only achieve 1/3 of the Infiniband bandwidth. We will continue to optimize this kernel.EAGLE-style speculative decoding: In the data above, we assumed using speculative decoding to predict 1 token. EAGLE can use a tree structure to predict multiple tokens, improving acceptance length, which can significantly increase output speed.GEMM Kernel: In the Roofline Model shown earlier, we can find that the efficiency of the GEMM kernel is still far from the theoretical limit. We will continue to optimize this kernel.GB200 NVL72: In NVIDIA\\'s latest GB200 NVL72 solution, 72 Blackwell GPUs are interconnected via high-speed NVLink. For MoE architecture models, this is a very big opportunity and challenge.ConclusionMulti-node deployment of DeepSeek MoE models achieves what\\'s typically impossible with dense LLMs: simultaneously improving both throughput and latency. By distributing experts across more GPUs, we reduce memory bandwidth pressure per device, enabling faster processing and higher system throughput. Our experiments show EP128 configurations achieving up to 5x higher throughput at equivalent output speeds compared to single-node deployments.Computation-communication overlapping techniques like micro-batching significantly reduce multi-node communication overhead, with our implementation showing up to 40% speedup. Our custom AllToAll communication kernels and optimized kernel implementations have enabled efficient deployment of the 671B parameter model.As MoE architectures gain popularity for their capability, these deployment strategies provide valuable insights for scaling such models efficiently.ReferencesDeepSeek-V3 Technical ReportDeepSeek-V3/R1 Inference System OverviewMegatron-LM: Training Multi-Billion Parameter Language Models Using Model ParallelismEAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty'),\n",
       " Document(metadata={'source': 'https://research.perplexity.ai/articles/efficient-and-portable-mixture-of-experts-communication'}, page_content='We present a high-performance, portable, open-source library for Mixture-of-Experts (MoE) communication that achieves 10x faster performance compared to standard All-to-All communication primitives.Our implementation features several key technical innovations that deliver superior MoE communication efficiency:GPU-initiated communication (IBGDA): Supports direct GPU-to-NIC communication, significantly reducing latency by bypassing CPU involvementCommunication and computation overlap: Split kernel architecture with separate send and receive stages enables computation to proceed concurrently with network transfersFastest single-node performance: 2.5x lower latency than the previously fastest implementation on single-node configurationsEfficient and portable multi-node performance: Our implementation achieves speeds up to 10x faster than standard all-to-all communication. Although approximately 2x slower than highly specialized implementations, our approach offers better portability across NVSHMEM versions and network environments (NVLink, CX-7, and EFA)The library is fully open-source and available at https://github.com/ppl-ai/pplx-kernels.In this article, we explore the challenges of expert parallelism in large-scale MoE models and describe our approach to efficient token dispatch and combination across distributed GPU environments.IntroductionMixture-of-Experts (MoE) models, such as DeepSeek R1 and Mixtral 8x7B, improve upon dense models by limiting the number of weights that are activated for each token. Instead of achieving high parameter counts by increasing the number of layers or increasing the size of the linear layers in the Multi-Layer Perceptron (MLP) of each decoder layer, MoE models replace the traditional MLP with multiple experts and a router. For example, out of the 671B parameters of DeepSeek R1, only 37B are multiplied with a given token during inference. This greatly reduces the amount of computation required to decode a token and improves latency compared to a dense model.MoE models present some additional challenges for inference, compared to dense models. While the experts themselves are small MLP layers, each decoder layer includes a router that decides which experts a token is dispatched to, with each token being dispatched to multiple experts. The router is typically a small linear layer producing a probability distribution. Usually the experts with the top-K scores are picked and the final activation is computed as a weighted average, summing the expert outputs multiplied by a weight derived from the probability distribution of the router.To minimize latency in distributed MoE systems, parallelism can be exploited across multiple devices, but this introduces communication challenges. Models such as Mixtral 8x7B or Llama-70B fit within 8 devices across a single node, benefiting from fast and low-latency NVLink interconnections (up to 900Gbps). However, larger models require sharding experts across multiple nodes using InfiniBand (peaking at 400Gbps), which introduces additional latency challenges.In this article, we explore the problem of expert parallelism, describing our implementation of GPU kernels that efficiently dispatch tokens to devices holding the assigned experts and collect them back by computing the weighted sum of expert outputs. While aggressively optimized implementations targeting specific network hardware exist, we present a highly portable implementation that relies on a minimal set of NVSHMEM primitives yet delivers performance 10x faster than standard all-to-all communication. Our implementation achieves state-of-the-art performance on single-node configurations while maintaining excellent portability across various network environments.Sharding and ParallelismFor efficient inference, the weights of a model must be held in device memory, while also leaving sufficient space for activations, KV caches and other buffers required by the forward pass through the model. The most capable models exceed the capacity of even the most capable GPUs, thus inference must be spread across multiple devices, which can collectively store the weights of the model. Based on the sharding schemes of weights, different communication and computation schemes must be used to offload computation and synchronize devices.Expert Parallelism (EP), illustrated in the first figure, only parallelizes the expert computation. Different experts are assigned to different devices, which hold their weights. After routing, tokens are sent to the corresponding device, with the results being gathered and accumulated afterwards. The complexity of routing depends on the degree of parallelism in the other parts of the model: replicating other layers could eliminate the need for routing altogether, as each rank can select the tokens from a locally replicated routing table. However, if only one of the rank run routing, a broadcast is required to dispatch tokens, indices and weights to their respective experts. Finally, an all-gather or an all-to-all broadcast synchronizes the output tokens with whichever rank continues the execution of the model. Such an implementation is relatively simple as torch already exposes the required primitives, albeit some benefits could be gained by fusing accumulation with the gather operation collecting the tokens from the experts.Expert-only parallelism does not scale ideally, as nodes in a cluster might be idle while the model is running non-expert layers, such as attention, norm and sampling. However, the computation of these layers, primarily attention, can also benefit from Tensor Parallelism (TP). Most models rely on multi-head attention, meaning that the attention heads and their corresponding Q, K and V projections can also be sharded across devices, replicating or gathering the slices between various layers. If attention is spread across all devices, an all-gather can synchronize the activations, allowing routing to be replicated, requiring synchronization primitives similar to the expert parallelism case for an efficient implementation. However, there are limits to parallelism at this level, as reducing the number of attention heads below a certain threshold will yield diminishing returns.To best utilize all devices and support a very high degree of expert parallelism, of up to 128 or 256 GPUs, Data Parallelism (DP) is required. Under this scheme, the devices a model is split across are grouped to handle requests concurrently, computing attention and maintaining KV caches sharded across their local group. Multiple instances of these parallel groups collaborate on expert evaluation, with each hosting a different subset of the expert weights. Based on the number of attention heads, a group may typically span up to the size of an entire node, as intra-node communication is faster. For example, in the figure above, one DP rank independently services two requests, A and B, handling attention, norm and any other bookkeeping for the requests. The other DP rank processes a distinct request C. However, the first node hosts half of the experts, while the other node the other half, thus after routing tokens from A and B might be sent to the second node and vice-versa. This leads to a sparse communication problem: each device might send a different number of tokens to any other destination rank. Existing primitives from torch, primarily all_to_all, are not particularly well suited, as they might require some form of padding or GPU-to-CPU synchronization and metadata broadcast. To implement communication effectively, custom kernels are required to process the routing information and initiate communication from the devices without CPU intervention. After routing, a dispatch kernel must send tokens to the ranks they were routed to, while on the combine side the activations belonging to the requests in the current DP group must be collected. Additionally, work must be balanced within a DP group which may have multiple devices, in order to minimize broadcast and correctly replicate the tokens on all ranks for the subsequent layers.NVSHMEMNVSHMEM is an NVIDIA-specific OpenSHMEM implementation, providing portable inter-device communication facilities that abstract away the complexity of the underlying hardware. The API can express device-to-device reads and writes, which are mapped to the primitives of individual transport layers. Both NVLink and RDMA are supported, granting a degree of portability and allowing NVSHMEM kernels to operate within a node or across a cluster of multiple nodes. In our kernels, we use remote memory writes and signaling operations in order to transfer data and synchronize the participating devices.NVSHMEM operations are built around the concept of symmetric memory: they operate on buffers which have been allocated on all the devices participating in inter-device communication. A local device can use an address derived within its own local buffer alongside the index of the target rank to specify the destination of an operation. The figure below illustrates this concept: both GPUs allocate symmetric buffers of the same size, retaining src pointers to them. The first rank wants to send 3 integers to the second one, placing them at the start of the buffer: nvshmem_int_put_nbi derives the start address from the local buffer, specifying the target device. The second rank derives an offset from its own buffer, sending one element to the first device, offsetting by one. While destination addresses must always be symmetric buffers allocated using nvshmem_alloc, source buffers can be arbitrary regions of device memory, provided they are pre-registered with NVSHMEM.While NVSHMEM provides a wide range of primitives, our kernels rely on only 3 functions, building all synchronization and fencing upon them.nvshmemx_putmem_signal_nbi_warp: Transfers a block of data from one device to another, while also setting a flag on the remote device. The operation either sets (NVSHMEM_SIGNAL_SET) or increments (NVSHMEM_SIGNAL_ADD) a 64-bit location. The flag is updated after the entire block of memory is transferred. If the remote device observed a change in the flag, it can safely access the buffer in its own memory. This function is useful for coalescing data transfer and synchronization.nvshmemx_signal_op operates on a single memory location, typically a 64-bit flag, atomically setting or incrementing it. It is useful in sending over metadata and synchronizing devices.nvshmem_uint64_wait_until is used on the receiving end of a signal, to poll a flag until the remote updates it.Ensuring any form of ordering between operations through nvshmem_quiet or nvshmem_fence is expensive, thus our implementation of the kernels avoids the use of any other NVSHMEM primitives and minimizes any dependencies other than individual send-receive pairs. When sending data, we always use the non-blocking version of functions, without waiting for the data to be even sent out of the local rank. Other, implicit synchronization across the group of kernels ensures that these buffers are not destructively overwritten while the NICs need access to them.NVSHMEM uses GPUDirect RDMA so that the NIC can directly read and write GPU memory for data exchange without involving CPUs. Furthermore, on ConnectX NICs, NVSHEMEM supports GPU-initiated communication (also known as Infiniband GPUDirect Async, or IBGDA), which allows GPU to post RDMA operations directly to NIC and to poll completion queue of NIC directly. On platforms that does not support GPU-initiated communication, NVSHMEM spawns a “proxy” thread on host CPU to initiate communication and poll completion on behalf of the GPU. NVSHMEM program is portable regardless of whether GPU-initiated communication is supported or not. However, GPU-initiated communication significantly cuts latency because it completely bypasses the detour to CPU.Portable KernelsWe implement MoE communication through a pair of dispatch and combine kernels. The dispatch kernels are responsible for reading tokens and routing information on each rank, dispatching them to the appropriate experts. The combine kernels collect the activations produced by the experts and send them back to their source ranks, while also computing the weighted average from the selected experts based on the weights computed by the router. The kernels are further split into a send and receive component, in order to allow data transfers to be overlapped with computation. The send kernels are all non-blocking and non-synchronizing: they simply dispatch all the writes to the remotes. On the other end, the combine kernels only read from memory, waiting until all required data has been transferred. After dispatching work to the NICs, while the data is transferred asynchronously over the wire, the GPUs can do other useful work locally, such as applying shared experts or computing attention.Each pair of kernels has its own symmetric memory buffers, which each rank allocating private storage to receive data from all other ranks. The maximal number of tokens that can be dispatched from a DP group is fixed across the ranks, which also sets the upper bound each rank can receive for each local expert from each DP rank. This allows sender ranks to derive a unique address on the destination rank to write to, without requiring any synchronization among them. After data is received, the kernels shuffle the tokens around to lay them out in memory in the optimal format required by the receiving kernels. While the buffers have a sizable dimension, they are re-used across all sequential layers of a model.The only form of global synchronization, as illustrated in the figure above, is implemented in the combine-receive kernel. Once data is dispatched from send, a rank returns and continues execution when it receives all the tokens it requires from the other ranks and enters the combine send kernels as soon as they are processed. The barrier in combine-receive ensures that no rank can run ahead and start dispatch-send while any other rank is still waiting to receive data, potentially causing destructive overlapping. Synchronization is done in the combine kernel for simplicity, as an optimization the same barrier could be implementing between the dispatch receive and the combine send kernels, further reducing latency by allowing the NICs more time to settle the transfers while the GPU is running other kernels.Both kernels are split across all available SMs of the devices: while the dispatch send and combine receive kernels must parallelize across a per-rank maximum token count (max_m), the dispatch receive and combine send kernels must handle that many tokens from each rank and local expert, for a maximum of max_m * (num_experts // EP) * (EP // TP).DispatchThe dispatch kernel is responsible for dispatching tokens and block scaling factors to peers which host the experts the tokens were routed to. The sender side relies on warp specialization to parallelize two tasks: aggregating routing information to determine how many tokens are sent from each rank and packing the tokens into messages to be dispatched to the remote. The receiver side first waits for all the token counts to be received, then it reads and unpacks the payloads from shared memory into the tensors that will be passed on to the downstream Grouped GEMM kernels. It also stores information into buffers shared with the combine kernels, to indicate where each token should be sent back. This information is required as the receive kernel shuffles tokens around in contiguous buffers in a non-deterministic order.In the sender part, a single warp of 32 threads on each block is responsible for reading the routing information and counting the number of tokens each destination expert is expected to receive from this rank, sending the count plus one using nvshmem_signal_op. The count is incremented by one, as the transition from zero to non-zero on the remote end signifies the receipt of the counts from a source rank. In parallel, the remaining warps co-operate to copy tokens into symmetric memory across all blocks in parallel, packing activations, scaling factors and their index on the local rank into a contiguous chunk. The index is required by the combine sender to determine the address where the token will be written to. Next, after ensuring all the data has been copied through a barrier across the warp groups, the warps yet again operate independently, each sending the same buffer to a different expert in paralle. The tokens are sent using nvshmemx_putmem_signal_nbi_warp, which also atomically increases the count of sent tokens from the local rank on the remote device. Within a DP group, since each rank ows a replica of the token to be sent, dispatch is balanced evenly, with each device sending out a subset of the tokens.On the receive end, all kernels first synchronize with the sender ranks by waiting for the total token counts to be sent over. Afterwards, they all wait for the atomically incremented sent token counts to settle to the total counts, indicating that all the payloads from the source ranks have also been sent over, thanks to the semantics of the putmem call. The kernels poll on the counts using nvshmem_uint64_wait_until, parallelizing the operation across all blocks and threads. Subsequently, a cross-block barrier ensures that no block reads the buffers unless all data has been correctly received. Spread across blocks and synchronized via an atomically incremented counter, the tokens are copied from symmetric memory into regular contiguous buffers which become the inputs to the downstream kernels implementing the experts. The source rank, expert index and token index are stored separately, exactly pinpointing the location where the combine kernel has to send the activations. Even though tokens from within a DP group are sent from different devices, they are all grouped together to be passed on to the corresponding expert.CombineThe combine kernels are yet again split into send and receiver halves: the senders copy the un-quantized 16-bit activations over to the remote devices, with the receive kernels waiting for the data to be sent over and computing the weighted average of all expert contributions locally. Additionally, they also act as a barrier, to synchronize the dispatch-combine sequence: each rank sets a flag on each peer on entry to the send kernels, with the receive kernels not being allowed to return unless they observe the flag being set. The latency of synchronization is minimal, as it overlaps with the actual communication and computation.On the sender side, the kernels traverse the list of tokens assigned to all local experts in parallel, writing them to a buffer on the destination rank. The target rank, expert index and token index are read from the per-token buffers populated by the scater kernels upon the receipt of the tokens. Each sender has its own private memory region per expert to write to, as indicated in the figure above, avoiding the need to synchronize. Similarly to dispatch, combine atomically increments per token counters on the destination rank to indicate the receipt of the data: when the counter matches the number of experts a token was dispatched to, the token contents can be accessed.In the receive kernel, the list of tokens is traversed in parallel across multiple blocks, waiting for their contents to arrive by polling the flag set by the signalling operation. Upon arrival, the payloads are read from the private buffers, with the routing table indicating which buffer to read from and what weight to assign to each expert. The results are then written to externally allocated tensors, with the kernel finishing execution once all devices passed the barrier. Across a DP group, all ranks receive a copy of each expert activation to compute their own replicas.BenchmarksWe evaluate our kernels on a cluster of 128 GPUs spread across 16 nodes connected via InfiniBand and CX-7 NICs. We try both GPUDirect Async (IBGDA) and Reliable Connection (RC) with a CPU proxy. We compare them to the highly optimized DeepSeek implementation, as well as the dense primitives provided by PyTorch (through NCCL) or NVSHMEM.Although on the dense NVSHMEM all-to-all operation, the performance of IBRC and IBGDA is similar (6378 µs vs 6180 µs), IBGDA is significantly faster with the sparse kernels. Adding up Dispatch and Combine, IBGDA uses 902 µs whereas IBRC takes 3223 µs - a 3.6x improvement in latency. While all-to-all is bandwidth bound, the sparse kernels broadcast orders of magnitudes less data, being bound by latency. By triggering network transfers directly from a GPU, without requiring a CPU proxy to coordinate the GPU and the NIC, end-to-end latency is significantly reduced.While our portable kernels are about 2x slower than the highly optimized DeepSeek kernels, they improve latency by 10x compared to the dense kernels (902 µs vs 9944 µs).Additionally, the split into sender-receiver components also allow some of the latency to be hidden away, unlike the library primitives.On single-node (EP8), NVSHMEM utilizes NVLINK for transportation, delivering lower latency and higher throughput than inter-node networking. Our portable kernels are about 2.5x faster than DeepEP on single-node (186µs vs 481 µs).Further ImprovementsThe kernels described here outperform the built-in primitives of ML frameworks and offer decent performance without over-specializing for particular inter-device transports, such as NVLink or InfiniBand. Besides the already-mentioned opportunities, further performance gains are attainable by replacing the communication primitives with more specialized versions. For example, across NVLink, the use of symmetric memory could be replaced with buffers shared across the devices, eliminating some copying and allowing for finer-grained synchronizations schemes, across individual tokens instead of token batches. Across InfiniBand, an implementation could access the underlying queue pairs directly, eliminating the need to transfer both the expected and sent token counts and flushing tokens from the queues into output buffers as they arrive. However, such implementations come at the cost of portability, whilst our implementations allows clusters with specific configurations to be evaluated before committing development effort to optimize for particular hardware.ConclusionWe have presented a high-performance, portable library for MoE communication that achieves 10x faster performance compared to standard all-to-all communication while maintaining compatibility across diverse hardware configurations. On single-node deployments with NVLink, our solution demonstrates 2.5x lower latency than previous implementations.Our approach balances performance with portability through key innovations including GPU-initiated communication support, a split kernel architecture enabling computation-communication overlap, and efficient token dispatch using minimal NVSHMEM primitives. While approximately 2x slower than highly specialized implementations on multi-node setups, our library offers superior flexibility across various network environments (NVLink, CX-7, and EFA).As MoE models continue to scale, efficient communication strategies like ours will become increasingly important for practical deployment. Our fully open-source implementation is available at https://github.com/ppl-ai/pplx-kernels.ReferencesDeepEPDeepSeek-V3 Technical ReportNVIDIA NVSHMEM'),\n",
       " Document(metadata={'source': 'https://www.perplexity.ai/hub/blog/announcing-comet-plus-launch-partners'}, page_content='Announcing Comet Plus Launch Partners Today, we’re proud to unveil the initial launch partners for Comet Plus. Comet Plus is the new business model designed by Perplexity to deliver premium journalism to a more curious, empowered internet and ensure publishers benefit from the new demands of the AI age.What is Comet Plus?Comet Plus is our answer to the critical question: What does a better internet look like in the age of AI? Comet Plus is a $5 standalone subscription (included at no additional cost with Perplexity Pro and Max) that gives users and their AI assistants direct access to high-quality journalism and answers supported by trusted and accurate reporting. Participating publishers are compensated according to each of the three ways their participating content is valuable in the age of AI.\\xa0 (Read all the details about Comet Plus and the new model we announced for publisher compensation.)Publishers are at the center of our vision for a better internet.The web was created for questions and curiosity, but somewhere along the way, users were funneled into clickbait, misinformation, and low-quality content optimized for an era of clicks and traffic.At Perplexity, we believe in a better internet. That’s why we built Comet. The internet is already better on Comet, but a truly better internet must also reward accurate, well-written news that fuels more accurate AI answers and builds the trust that readers place in their AI assistants.Our vision with Comet and Comet Plus is to create the business model that enables this: Publishers participating in Comet Plus are compensated according to human and AI-driven interactions with their participating content, shifting away from the \"pageview at any cost\" model that has undermined quality journalism for years.Meet our Comet Plus launch partners.We are honored to launch Comet Plus with some of the world’s best-known, most respected sources of news and culture:CNNConde Nast, publishers of The New Yorker, Wired, Architectural Digest, Allure, Ars Technica, Bon Appétit, Condé Nast Traveler, Epicurious, GQ, Glamour, Pitchfork, Self, Teen Vogue, them, Vanity Fair, and Vogue.FortuneLe FigaroLe MondeThe Los Angeles TimesThe Washington PostThe participation of these leading publishers means Comet Plus users get immediate, frictionless access to premium stories that matter, from global headlines to in-depth analysis and exceptional cultural writing all directly on the web as part of the Comet browsing experience.The agentic internet is for peopleA better internet starts with Comet. With Comet Plus, a better internet includes the expertise and perspective of top journalists and publishers.\\xa0Most importantly, Comet Plus makes the internet better by ensuring great publishers and journalists benefit from the new forms of usage people demand in the age of AI. At the same time, Comet Plus is specifically designed to allow publishers to own the direct relationship with their audience.We’re building an ecosystem that rewards trust, accuracy, and curiosity, so everyone can confidently ask bigger, better questions, and get the best answers that move them forward.We’re grateful to our launch partners for believing in this vision. Together, we are supporting the world’s curiosity.Announcing Comet Plus Launch Partners Today, we’re proud to unveil the initial launch partners for Comet Plus. Comet Plus is the new business model designed by Perplexity to deliver premium journalism to a more curious, empowered internet and ensure publishers benefit from the new demands of the AI age.What is Comet Plus?Comet Plus is our answer to the critical question: What does a better internet look like in the age of AI? Comet Plus is a $5 standalone subscription (included at no additional cost with Perplexity Pro and Max) that gives users and their AI assistants direct access to high-quality journalism and answers supported by trusted and accurate reporting. Participating publishers are compensated according to each of the three ways their participating content is valuable in the age of AI.\\xa0 (Read all the details about Comet Plus and the new model we announced for publisher compensation.)Publishers are at the center of our vision for a better internet.The web was created for questions and curiosity, but somewhere along the way, users were funneled into clickbait, misinformation, and low-quality content optimized for an era of clicks and traffic.At Perplexity, we believe in a better internet. That’s why we built Comet. The internet is already better on Comet, but a truly better internet must also reward accurate, well-written news that fuels more accurate AI answers and builds the trust that readers place in their AI assistants.Our vision with Comet and Comet Plus is to create the business model that enables this: Publishers participating in Comet Plus are compensated according to human and AI-driven interactions with their participating content, shifting away from the \"pageview at any cost\" model that has undermined quality journalism for years.Meet our Comet Plus launch partners.We are honored to launch Comet Plus with some of the world’s best-known, most respected sources of news and culture:CNNConde Nast, publishers of The New Yorker, Wired, Architectural Digest, Allure, Ars Technica, Bon Appétit, Condé Nast Traveler, Epicurious, GQ, Glamour, Pitchfork, Self, Teen Vogue, them, Vanity Fair, and Vogue.FortuneLe FigaroLe MondeThe Los Angeles TimesThe Washington PostThe participation of these leading publishers means Comet Plus users get immediate, frictionless access to premium stories that matter, from global headlines to in-depth analysis and exceptional cultural writing all directly on the web as part of the Comet browsing experience.The agentic internet is for peopleA better internet starts with Comet. With Comet Plus, a better internet includes the expertise and perspective of top journalists and publishers.\\xa0Most importantly, Comet Plus makes the internet better by ensuring great publishers and journalists benefit from the new forms of usage people demand in the age of AI. At the same time, Comet Plus is specifically designed to allow publishers to own the direct relationship with their audience.We’re building an ecosystem that rewards trust, accuracy, and curiosity, so everyone can confidently ask bigger, better questions, and get the best answers that move them forward.We’re grateful to our launch partners for believing in this vision. Together, we are supporting the world’s curiosity.Announcing Comet Plus Launch Partners Today, we’re proud to unveil the initial launch partners for Comet Plus. Comet Plus is the new business model designed by Perplexity to deliver premium journalism to a more curious, empowered internet and ensure publishers benefit from the new demands of the AI age.What is Comet Plus?Comet Plus is our answer to the critical question: What does a better internet look like in the age of AI? Comet Plus is a $5 standalone subscription (included at no additional cost with Perplexity Pro and Max) that gives users and their AI assistants direct access to high-quality journalism and answers supported by trusted and accurate reporting. Participating publishers are compensated according to each of the three ways their participating content is valuable in the age of AI.\\xa0 (Read all the details about Comet Plus and the new model we announced for publisher compensation.)Publishers are at the center of our vision for a better internet.The web was created for questions and curiosity, but somewhere along the way, users were funneled into clickbait, misinformation, and low-quality content optimized for an era of clicks and traffic.At Perplexity, we believe in a better internet. That’s why we built Comet. The internet is already better on Comet, but a truly better internet must also reward accurate, well-written news that fuels more accurate AI answers and builds the trust that readers place in their AI assistants.Our vision with Comet and Comet Plus is to create the business model that enables this: Publishers participating in Comet Plus are compensated according to human and AI-driven interactions with their participating content, shifting away from the \"pageview at any cost\" model that has undermined quality journalism for years.Meet our Comet Plus launch partners.We are honored to launch Comet Plus with some of the world’s best-known, most respected sources of news and culture:CNNConde Nast, publishers of The New Yorker, Wired, Architectural Digest, Allure, Ars Technica, Bon Appétit, Condé Nast Traveler, Epicurious, GQ, Glamour, Pitchfork, Self, Teen Vogue, them, Vanity Fair, and Vogue.FortuneLe FigaroLe MondeThe Los Angeles TimesThe Washington PostThe participation of these leading publishers means Comet Plus users get immediate, frictionless access to premium stories that matter, from global headlines to in-depth analysis and exceptional cultural writing all directly on the web as part of the Comet browsing experience.The agentic internet is for peopleA better internet starts with Comet. With Comet Plus, a better internet includes the expertise and perspective of top journalists and publishers.\\xa0Most importantly, Comet Plus makes the internet better by ensuring great publishers and journalists benefit from the new forms of usage people demand in the age of AI. At the same time, Comet Plus is specifically designed to allow publishers to own the direct relationship with their audience.We’re building an ecosystem that rewards trust, accuracy, and curiosity, so everyone can confidently ask bigger, better questions, and get the best answers that move them forward.We’re grateful to our launch partners for believing in this vision. Together, we are supporting the world’s curiosity.'),\n",
       " Document(metadata={'source': 'https://www.perplexity.ai/hub/blog/introducing-the-perplexity-search-api'}, page_content='Introducing the Perplexity Search APIProviding access to the same global-scale infrastructure that powers Perplexity’s public answer engineThe modern internet represents humanity’s collective knowledge and wisdom. Yet accessing this information programmatically and at scale has remained out of reach for too long. Legacy search incumbents maintain proprietary indices for their own exclusive use. Other providers have tried their best to fill the gaps, but are unable to provide the quality that today’s developers need to build new capabilities in an increasingly information-driven, AI-forward marketplace. We’ve long heard from our own developer community that a world-class search and retrieval offering is sorely needed within the AI ecosystem.Today, we’re answering that call by launching the Perplexity Search API. Our API provides access to the same global-scale infrastructure that powers Perplexity’s public answer engine. With an index covering hundreds of billions of webpages, developers can now tap information from across the internet with one simple yet powerful interface.We’re also releasing an SDK, open-source evaluation framework, and deep dive by Perplexity Research into how we designed, optimized, and evaluated our Search API.Built for modern AI workloadsPerplexity Search API is designed for the unique demands of AI applications. Unlike other API offerings that expose a restricted universe of information, our API provides rich structured responses that are ready for use in AI and traditional applications alike.When it comes to AI, context is king. It is insufficient to operate simply at the document level. Our indexing and retrieval infrastructure divides documents up into fine-grained units. These sub-document units are individually surfaced and scored against the original query parameters, allowing our API to return documents with the most relevant snippets already ranked. This means less preprocessing, faster integration, and more valuable downstream results.Bringing Perplexity’s investments in accuracy and trust to developersSince our founding, Perplexity has emphasized accuracy and trust across everything we do. We led the industry in corroborating AI answers with sources, and we’ve since doubled down on our R&D investments to ensure that Perplexity is the world’s most accurate and factual AI assistant.Our search infrastructure is designed with this north star in mind. Our own experience reveals that information staleness is one of the biggest failure modes for AI agents, and we’ve optimized our indexing workflows to make Perplexity a truly realtime assistant. Each second, our systems process tens of thousands of index update requests, ensuring that our index provides the freshest results available. These indexing operations leverage an AI-powered content understanding module that dynamically generates parsing logic to handle the messiness of the open web. The module optimizes itself via an iterative AI self-improvement process, powered by robust evaluations and realtime signals from the millions of user queries we service each hour.Thanks to these and other investments, our search infrastructure positions Perplexity as the market leader in accuracy and relevance. Now, developers have effortless access to that same infrastructure to innovate across countless industries and use cases.Market-leading performance, pricing, and ergonomicsPerplexity Search API sits at the Pareto frontier of relevance and speed. We’re releasing an open-source evaluation framework, search_evals, to enable researchers and developers to rigorously test any publicly available search API. On both single step search and deep research agentic workflows, Perplexity leads the competition across output quality and latency. Thanks to our infrastructure investments, we provide this superior performance at lower cost.Performance and cost are just the start. We’ve also carefully designed our API to prioritize ease of use for both human developers and AI agents. This includes our newly-launched API Platform (which now houses the developer console and documentation for both our Search and Sonar APIs) and our Search SDK. Our own engineers have been able to use the Search SDK alongside their favorite AI coding tools to develop impressive product prototypes in under an hour. We anticipate even more impressive feats from startups and solo developers, mature enterprises, and everyone in between.The Search API team will join our San Francisco API Day and London hackathon next month, and you can also reach them online through our developer community. Developers who choose our Search API can look forward to benefitting from the same research and engineering improvements that we deploy within Perplexity’s user-facing products, meaning even better performance and cost-effectiveness over time.Democratizing access to knowledge across humans and codeOur journey to democratize access to knowledge began with satisfying our users’ boundless curiosity. With the Perplexity Search API, that journey continues with empowering the world’s millions of developers. We can’t wait to see what they build with internet-scale search at their fingertips.[Get started on API Platform][Install our SDK][Read more at Perplexity Research]Introducing the Perplexity Search APIProviding access to the same global-scale infrastructure that powers Perplexity’s public answer engineThe modern internet represents humanity’s collective knowledge and wisdom. Yet accessing this information programmatically and at scale has remained out of reach for too long. Legacy search incumbents maintain proprietary indices for their own exclusive use. Other providers have tried their best to fill the gaps, but are unable to provide the quality that today’s developers need to build new capabilities in an increasingly information-driven, AI-forward marketplace. We’ve long heard from our own developer community that a world-class search and retrieval offering is sorely needed within the AI ecosystem.Today, we’re answering that call by launching the Perplexity Search API. Our API provides access to the same global-scale infrastructure that powers Perplexity’s public answer engine. With an index covering hundreds of billions of webpages, developers can now tap information from across the internet with one simple yet powerful interface.We’re also releasing an SDK, open-source evaluation framework, and deep dive by Perplexity Research into how we designed, optimized, and evaluated our Search API.Built for modern AI workloadsPerplexity Search API is designed for the unique demands of AI applications. Unlike other API offerings that expose a restricted universe of information, our API provides rich structured responses that are ready for use in AI and traditional applications alike.When it comes to AI, context is king. It is insufficient to operate simply at the document level. Our indexing and retrieval infrastructure divides documents up into fine-grained units. These sub-document units are individually surfaced and scored against the original query parameters, allowing our API to return documents with the most relevant snippets already ranked. This means less preprocessing, faster integration, and more valuable downstream results.Bringing Perplexity’s investments in accuracy and trust to developersSince our founding, Perplexity has emphasized accuracy and trust across everything we do. We led the industry in corroborating AI answers with sources, and we’ve since doubled down on our R&D investments to ensure that Perplexity is the world’s most accurate and factual AI assistant.Our search infrastructure is designed with this north star in mind. Our own experience reveals that information staleness is one of the biggest failure modes for AI agents, and we’ve optimized our indexing workflows to make Perplexity a truly realtime assistant. Each second, our systems process tens of thousands of index update requests, ensuring that our index provides the freshest results available. These indexing operations leverage an AI-powered content understanding module that dynamically generates parsing logic to handle the messiness of the open web. The module optimizes itself via an iterative AI self-improvement process, powered by robust evaluations and realtime signals from the millions of user queries we service each hour.Thanks to these and other investments, our search infrastructure positions Perplexity as the market leader in accuracy and relevance. Now, developers have effortless access to that same infrastructure to innovate across countless industries and use cases.Market-leading performance, pricing, and ergonomicsPerplexity Search API sits at the Pareto frontier of relevance and speed. We’re releasing an open-source evaluation framework, search_evals, to enable researchers and developers to rigorously test any publicly available search API. On both single step search and deep research agentic workflows, Perplexity leads the competition across output quality and latency. Thanks to our infrastructure investments, we provide this superior performance at lower cost.Performance and cost are just the start. We’ve also carefully designed our API to prioritize ease of use for both human developers and AI agents. This includes our newly-launched API Platform (which now houses the developer console and documentation for both our Search and Sonar APIs) and our Search SDK. Our own engineers have been able to use the Search SDK alongside their favorite AI coding tools to develop impressive product prototypes in under an hour. We anticipate even more impressive feats from startups and solo developers, mature enterprises, and everyone in between.The Search API team will join our San Francisco API Day and London hackathon next month, and you can also reach them online through our developer community. Developers who choose our Search API can look forward to benefitting from the same research and engineering improvements that we deploy within Perplexity’s user-facing products, meaning even better performance and cost-effectiveness over time.Democratizing access to knowledge across humans and codeOur journey to democratize access to knowledge began with satisfying our users’ boundless curiosity. With the Perplexity Search API, that journey continues with empowering the world’s millions of developers. We can’t wait to see what they build with internet-scale search at their fingertips.[Get started on API Platform][Install our SDK][Read more at Perplexity Research]Introducing the Perplexity Search APIProviding access to the same global-scale infrastructure that powers Perplexity’s public answer engineThe modern internet represents humanity’s collective knowledge and wisdom. Yet accessing this information programmatically and at scale has remained out of reach for too long. Legacy search incumbents maintain proprietary indices for their own exclusive use. Other providers have tried their best to fill the gaps, but are unable to provide the quality that today’s developers need to build new capabilities in an increasingly information-driven, AI-forward marketplace. We’ve long heard from our own developer community that a world-class search and retrieval offering is sorely needed within the AI ecosystem.Today, we’re answering that call by launching the Perplexity Search API. Our API provides access to the same global-scale infrastructure that powers Perplexity’s public answer engine. With an index covering hundreds of billions of webpages, developers can now tap information from across the internet with one simple yet powerful interface.We’re also releasing an SDK, open-source evaluation framework, and deep dive by Perplexity Research into how we designed, optimized, and evaluated our Search API.Built for modern AI workloadsPerplexity Search API is designed for the unique demands of AI applications. Unlike other API offerings that expose a restricted universe of information, our API provides rich structured responses that are ready for use in AI and traditional applications alike.When it comes to AI, context is king. It is insufficient to operate simply at the document level. Our indexing and retrieval infrastructure divides documents up into fine-grained units. These sub-document units are individually surfaced and scored against the original query parameters, allowing our API to return documents with the most relevant snippets already ranked. This means less preprocessing, faster integration, and more valuable downstream results.Bringing Perplexity’s investments in accuracy and trust to developersSince our founding, Perplexity has emphasized accuracy and trust across everything we do. We led the industry in corroborating AI answers with sources, and we’ve since doubled down on our R&D investments to ensure that Perplexity is the world’s most accurate and factual AI assistant.Our search infrastructure is designed with this north star in mind. Our own experience reveals that information staleness is one of the biggest failure modes for AI agents, and we’ve optimized our indexing workflows to make Perplexity a truly realtime assistant. Each second, our systems process tens of thousands of index update requests, ensuring that our index provides the freshest results available. These indexing operations leverage an AI-powered content understanding module that dynamically generates parsing logic to handle the messiness of the open web. The module optimizes itself via an iterative AI self-improvement process, powered by robust evaluations and realtime signals from the millions of user queries we service each hour.Thanks to these and other investments, our search infrastructure positions Perplexity as the market leader in accuracy and relevance. Now, developers have effortless access to that same infrastructure to innovate across countless industries and use cases.Market-leading performance, pricing, and ergonomicsPerplexity Search API sits at the Pareto frontier of relevance and speed. We’re releasing an open-source evaluation framework, search_evals, to enable researchers and developers to rigorously test any publicly available search API. On both single step search and deep research agentic workflows, Perplexity leads the competition across output quality and latency. Thanks to our infrastructure investments, we provide this superior performance at lower cost.Performance and cost are just the start. We’ve also carefully designed our API to prioritize ease of use for both human developers and AI agents. This includes our newly-launched API Platform (which now houses the developer console and documentation for both our Search and Sonar APIs) and our Search SDK. Our own engineers have been able to use the Search SDK alongside their favorite AI coding tools to develop impressive product prototypes in under an hour. We anticipate even more impressive feats from startups and solo developers, mature enterprises, and everyone in between.The Search API team will join our San Francisco API Day and London hackathon next month, and you can also reach them online through our developer community. Developers who choose our Search API can look forward to benefitting from the same research and engineering improvements that we deploy within Perplexity’s user-facing products, meaning even better performance and cost-effectiveness over time.Democratizing access to knowledge across humans and codeOur journey to democratize access to knowledge began with satisfying our users’ boundless curiosity. With the Perplexity Search API, that journey continues with empowering the world’s millions of developers. We can’t wait to see what they build with internet-scale search at their fingertips.[Get started on API Platform][Install our SDK][Read more at Perplexity Research]'),\n",
       " Document(metadata={'source': 'https://www.perplexity.ai/hub/blog/a-personal-assistant-for-your-inbox'}, page_content='A Personal Assistant for Your InboxIntroducing Email AssistantToday we\\'re launching Perplexity Email Assistant, exclusive for Perplexity Max subscribers. Email Assistant brings a powerful personal assistant to your email, transforming your inbox to action.\\xa0\\xa0Perplexity Max subscribers have shown powerful adoption of the Comet Assistant that launched with Comet and Comet Max Assistant launched in July exclusively for Max subscribers.\\xa0In fact, early data shows the Comet assistants increase the number of questions and tasks users accomplish per day by 3-18X.\\xa0\\xa0Simply put, a powerful and personal AI assistant helps you get a lot more done.\\xa0\\xa0Your inbox deserves the same intelligent partnership, which is why we’ve built Perplexity Email Assistant. Email Assistant connects directly with your email account across your phone and computer. It drafts replies, organizes messages, schedules meetings, and more.\\xa0You power through inbox tasks, in less time, while staying in control.A powerful, personal assistant Email is more than a message center. Your inbox contains your professional memory, your relationships, calendaring, and coordination.\\xa0Meanwhile, your outbox is your productivity and your reputation.\\xa0\\xa0The most successful workers and thinkers hire personal assistants for their email instead of relying on AI tools and algorithms that handle rote tasks. There’s too much at stake. Perplexity\\'s Email Assistant is more than a tool, and available to anyone.\\xa0\\xa0Email Assistant learns your communication style and priorities. It drafts responses matching your tone and suggests meeting times based on your calendar preferences, saving time on routine tasks.Email Assistant is also secure. Email Assistant is SOC 2 and GDPR compliance by default and never trains on your data.\\xa0Just askBeginning today, Max Subscribers can sign up for Email Assistant here. Then get started simply by emailing assistant@perplexity.com from your own inbox. The email assistant knows it\\'s you and gets right to work.Calendar meetings by cc’ing your assistant on any email.\\xa0The assistant will work with your contacts in your style on the back-and-forth exchanges that steal hours from your day, all while ensuring you still have total control.\\xa0\\xa0You can also ask your Email Assistant questions about your inbox: \"What emails should I prioritize before my board meeting?\" \"Summarize all messages about the Q4 budget.\" \"Show me anything urgent from the design team this week.” Comet users already enjoy this capability, where questions can be asked of any web interface.\\xa0Email assistant powers your curiosity beyond predefined functions. Ask anything about your inbox and discover new ways to extract value from your email data. The more you ask, the more you uncover possibilities we haven\\'t even considered yet.Work smarter, fasterEmail Assistant connects with Gmail and Outlook. Smart labels automatically organize your inbox, showing what\\'s completed, what needs action, and what requires your attention. No more scanning hundreds of messages to find what’s critical.Another benefit is auto-draft. Email Assistant writes responses for you to edit or send, eliminating the biggest email bottleneck: getting started.\\xa0Email Assistant adapts to your communication patterns and priorities. It drafts responses matching your tone, suggests meeting times based on your calendar habits, and applies labels you actually use. Every interaction teaches it to work more like you would.Built for youEmail Assistant works where you work, answering the questions that drive your most important decisions. It’s built to help you focus on what matters, no matter where. The time you reclaim from routine correspondence and rote tasks in your inbox becomes time for deeper work, meaningful conversations, and even bigger questions.Email Assistant is available now for Perplexity Max subscribers. Just go to the Email Assistant hub and connect your email.Then, start with a simple question: \"What needs my attention first?\"\\xa0 Add your email assistant to any conversation, and turn your inbox into action.A Personal Assistant for Your InboxIntroducing Email AssistantToday we\\'re launching Perplexity Email Assistant, exclusive for Perplexity Max subscribers. Email Assistant brings a powerful personal assistant to your email, transforming your inbox to action.\\xa0\\xa0Perplexity Max subscribers have shown powerful adoption of the Comet Assistant that launched with Comet and Comet Max Assistant launched in July exclusively for Max subscribers.\\xa0In fact, early data shows the Comet assistants increase the number of questions and tasks users accomplish per day by 3-18X.\\xa0\\xa0Simply put, a powerful and personal AI assistant helps you get a lot more done.\\xa0\\xa0Your inbox deserves the same intelligent partnership, which is why we’ve built Perplexity Email Assistant. Email Assistant connects directly with your email account across your phone and computer. It drafts replies, organizes messages, schedules meetings, and more.\\xa0You power through inbox tasks, in less time, while staying in control.A powerful, personal assistant Email is more than a message center. Your inbox contains your professional memory, your relationships, calendaring, and coordination.\\xa0Meanwhile, your outbox is your productivity and your reputation.\\xa0\\xa0The most successful workers and thinkers hire personal assistants for their email instead of relying on AI tools and algorithms that handle rote tasks. There’s too much at stake. Perplexity\\'s Email Assistant is more than a tool, and available to anyone.\\xa0\\xa0Email Assistant learns your communication style and priorities. It drafts responses matching your tone and suggests meeting times based on your calendar preferences, saving time on routine tasks.Email Assistant is also secure. Email Assistant is SOC 2 and GDPR compliance by default and never trains on your data.\\xa0Just askBeginning today, Max Subscribers can sign up for Email Assistant here. Then get started simply by emailing assistant@perplexity.com from your own inbox. The email assistant knows it\\'s you and gets right to work.Calendar meetings by cc’ing your assistant on any email.\\xa0The assistant will work with your contacts in your style on the back-and-forth exchanges that steal hours from your day, all while ensuring you still have total control.\\xa0\\xa0You can also ask your Email Assistant questions about your inbox: \"What emails should I prioritize before my board meeting?\" \"Summarize all messages about the Q4 budget.\" \"Show me anything urgent from the design team this week.” Comet users already enjoy this capability, where questions can be asked of any web interface.\\xa0Email assistant powers your curiosity beyond predefined functions. Ask anything about your inbox and discover new ways to extract value from your email data. The more you ask, the more you uncover possibilities we haven\\'t even considered yet.Work smarter, fasterEmail Assistant connects with Gmail and Outlook. Smart labels automatically organize your inbox, showing what\\'s completed, what needs action, and what requires your attention. No more scanning hundreds of messages to find what’s critical.Another benefit is auto-draft. Email Assistant writes responses for you to edit or send, eliminating the biggest email bottleneck: getting started.\\xa0Email Assistant adapts to your communication patterns and priorities. It drafts responses matching your tone, suggests meeting times based on your calendar habits, and applies labels you actually use. Every interaction teaches it to work more like you would.Built for youEmail Assistant works where you work, answering the questions that drive your most important decisions. It’s built to help you focus on what matters, no matter where. The time you reclaim from routine correspondence and rote tasks in your inbox becomes time for deeper work, meaningful conversations, and even bigger questions.Email Assistant is available now for Perplexity Max subscribers. Just go to the Email Assistant hub and connect your email.Then, start with a simple question: \"What needs my attention first?\"\\xa0 Add your email assistant to any conversation, and turn your inbox into action.A Personal Assistant for Your InboxIntroducing Email AssistantToday we\\'re launching Perplexity Email Assistant, exclusive for Perplexity Max subscribers. Email Assistant brings a powerful personal assistant to your email, transforming your inbox to action.\\xa0\\xa0Perplexity Max subscribers have shown powerful adoption of the Comet Assistant that launched with Comet and Comet Max Assistant launched in July exclusively for Max subscribers.\\xa0In fact, early data shows the Comet assistants increase the number of questions and tasks users accomplish per day by 3-18X.\\xa0\\xa0Simply put, a powerful and personal AI assistant helps you get a lot more done.\\xa0\\xa0Your inbox deserves the same intelligent partnership, which is why we’ve built Perplexity Email Assistant. Email Assistant connects directly with your email account across your phone and computer. It drafts replies, organizes messages, schedules meetings, and more.\\xa0You power through inbox tasks, in less time, while staying in control.A powerful, personal assistant Email is more than a message center. Your inbox contains your professional memory, your relationships, calendaring, and coordination.\\xa0Meanwhile, your outbox is your productivity and your reputation.\\xa0\\xa0The most successful workers and thinkers hire personal assistants for their email instead of relying on AI tools and algorithms that handle rote tasks. There’s too much at stake. Perplexity\\'s Email Assistant is more than a tool, and available to anyone.\\xa0\\xa0Email Assistant learns your communication style and priorities. It drafts responses matching your tone and suggests meeting times based on your calendar preferences, saving time on routine tasks.Email Assistant is also secure. Email Assistant is SOC 2 and GDPR compliance by default and never trains on your data.\\xa0Just askBeginning today, Max Subscribers can sign up for Email Assistant here. Then get started simply by emailing assistant@perplexity.com from your own inbox. The email assistant knows it\\'s you and gets right to work.Calendar meetings by cc’ing your assistant on any email.\\xa0The assistant will work with your contacts in your style on the back-and-forth exchanges that steal hours from your day, all while ensuring you still have total control.\\xa0\\xa0You can also ask your Email Assistant questions about your inbox: \"What emails should I prioritize before my board meeting?\" \"Summarize all messages about the Q4 budget.\" \"Show me anything urgent from the design team this week.” Comet users already enjoy this capability, where questions can be asked of any web interface.\\xa0Email assistant powers your curiosity beyond predefined functions. Ask anything about your inbox and discover new ways to extract value from your email data. The more you ask, the more you uncover possibilities we haven\\'t even considered yet.Work smarter, fasterEmail Assistant connects with Gmail and Outlook. Smart labels automatically organize your inbox, showing what\\'s completed, what needs action, and what requires your attention. No more scanning hundreds of messages to find what’s critical.Another benefit is auto-draft. Email Assistant writes responses for you to edit or send, eliminating the biggest email bottleneck: getting started.\\xa0Email Assistant adapts to your communication patterns and priorities. It drafts responses matching your tone, suggests meeting times based on your calendar habits, and applies labels you actually use. Every interaction teaches it to work more like you would.Built for youEmail Assistant works where you work, answering the questions that drive your most important decisions. It’s built to help you focus on what matters, no matter where. The time you reclaim from routine correspondence and rote tasks in your inbox becomes time for deeper work, meaningful conversations, and even bigger questions.Email Assistant is available now for Perplexity Max subscribers. Just go to the Email Assistant hub and connect your email.Then, start with a simple question: \"What needs my attention first?\"\\xa0 Add your email assistant to any conversation, and turn your inbox into action.'),\n",
       " Document(metadata={'source': 'https://www.perplexity.ai/hub/blog/power-your-organization-s-full-potential'}, page_content=\"Power your organization’s full potential\\xa0Introducing Perplexity Enterprise MaxEarly this summer we launched Perplexity Max for AI users who demand unlimited access to the best models and newest features from Perplexity.\\xa0Since then, we've seen powerful adoption by Max users, from financial analysts conducting deep market research to startup founders building out every aspect of their growing companies. Max users push the boundaries of what's possible with AI-assisted work.Meanwhile, one signal has been clear: some Max users are deploying Perplexity throughout their organizations in ways that also demand enterprise-level security and control. Today, we're excited to announce Perplexity Enterprise Max, our most advanced tier that combines all the advanced capabilities of Max with the enterprise-grade security and controls that organizations demand. Now your team members can access more powerful tools for research and content creation while IT maintains complete visibility and compliance.Here's what makes Enterprise Max transformative for organizations ready to maximize their AI investment, all while maintaining the same SOC 2 Type II compliance standards.What’s in Perplexity Enterprise Max?Unlimited Research and Labs Queries\\xa0Perplexity’s Research mode can source up to 100+ citations and give your team in-depth reports on complex topics while Labs helps transform ideas into tangible results through advanced data analysis, data visualization, and web apps. Enterprise Max provides unlimited Research Labs usage, enabling your team to analyze massive datasets, build sophisticated dashboards, and prototype solutions without hitting monthly caps to iterate continuously, explore deeper insights, and deliver comprehensive analysis.\\xa0Advanced model selectionDifferent challenges require different AI capabilities, and Enterprise Max gives your team access to our most advanced models including o3-pro and Opus 4.1 Thinking. These advanced models excel at complex reasoning, strategic analysis, and technical problem-solving.Comet Max AssistantMax Assistant is the most powerful and personal AI assistant yet. The same functionality is now available in Enterprise Max. Enterprise Max users who use Comet as their browser will access powerful models in their Assistant sidecar when prompting Perplexity in Comet.\\xa010x more file limits\\xa0Enterprise Max increases your file storage capacity to 10,000 files in your workspace and 5,000 files in Spaces — a dramatic increase from Pro's 500 and 5,000 respectively. This expanded capacity allows your teams to upload entire project libraries, cross-reference massive document sets, and build comprehensive knowledge bases from your organization's collective expertise. So your team can run deeper research across your files and the web.\\xa0Enhanced video generation\\xa0Create up to 15 high-quality videos per month with improved resolution and audio using Veo 3, directly within Perplexity. This capability transforms how your team communicates complex ideas, whether for executive presentations, client deliverables, or internal training materials.\\xa0Organization-wide access to premium security featuresPreviously, customers needed a 50 seat minimum to access premium security features. With just one Max user, your entire organization gains access to audit logs, SCIM integration, configurable data retention, and real-time usage analytics. You get comprehensive security visibility and control across your organization while maintaining cost efficiency.Early AccessPerplexity Enterprise Max is the fastest way to get new Perplexity features and products first. Enterprise Max users will get access to premium data sources and other releases like our upcoming Email Assistant that will be exclusive to Enterprise Max subscribers. We are constantly launching new products and features at Perplexity, and we’re excited to share them first with AI users who demand the newest and best.Available todayEnterprise Pro remains $40 per user per month ($400 per year), while Enterprise Max is $325 per user per month ($3,250 per year). Admins can upgrade users immediately in Account Settings. Within an organization, you can have both Enterprise Pro and Max users–giving advanced features to those who need them most.\\xa0Need help determining the right deployment strategy for your team? Reach out anytime at enterprise@perplexity.ai for guidance.\\xa0From the power users who first pushed Max's boundaries to the enterprise teams ready to transform how they work—Enterprise Max bridges the gap between breakthrough capabilities and organizational needs. Your team is ready for what's next.Stay curious. Build smarter. Move faster.Power your organization’s full potential\\xa0Introducing Perplexity Enterprise MaxEarly this summer we launched Perplexity Max for AI users who demand unlimited access to the best models and newest features from Perplexity.\\xa0Since then, we've seen powerful adoption by Max users, from financial analysts conducting deep market research to startup founders building out every aspect of their growing companies. Max users push the boundaries of what's possible with AI-assisted work.Meanwhile, one signal has been clear: some Max users are deploying Perplexity throughout their organizations in ways that also demand enterprise-level security and control. Today, we're excited to announce Perplexity Enterprise Max, our most advanced tier that combines all the advanced capabilities of Max with the enterprise-grade security and controls that organizations demand. Now your team members can access more powerful tools for research and content creation while IT maintains complete visibility and compliance.Here's what makes Enterprise Max transformative for organizations ready to maximize their AI investment, all while maintaining the same SOC 2 Type II compliance standards.What’s in Perplexity Enterprise Max?Unlimited Research and Labs Queries\\xa0Perplexity’s Research mode can source up to 100+ citations and give your team in-depth reports on complex topics while Labs helps transform ideas into tangible results through advanced data analysis, data visualization, and web apps. Enterprise Max provides unlimited Research Labs usage, enabling your team to analyze massive datasets, build sophisticated dashboards, and prototype solutions without hitting monthly caps to iterate continuously, explore deeper insights, and deliver comprehensive analysis.\\xa0Advanced model selectionDifferent challenges require different AI capabilities, and Enterprise Max gives your team access to our most advanced models including o3-pro and Opus 4.1 Thinking. These advanced models excel at complex reasoning, strategic analysis, and technical problem-solving.Comet Max AssistantMax Assistant is the most powerful and personal AI assistant yet. The same functionality is now available in Enterprise Max. Enterprise Max users who use Comet as their browser will access powerful models in their Assistant sidecar when prompting Perplexity in Comet.\\xa010x more file limits\\xa0Enterprise Max increases your file storage capacity to 10,000 files in your workspace and 5,000 files in Spaces — a dramatic increase from Pro's 500 and 5,000 respectively. This expanded capacity allows your teams to upload entire project libraries, cross-reference massive document sets, and build comprehensive knowledge bases from your organization's collective expertise. So your team can run deeper research across your files and the web.\\xa0Enhanced video generation\\xa0Create up to 15 high-quality videos per month with improved resolution and audio using Veo 3, directly within Perplexity. This capability transforms how your team communicates complex ideas, whether for executive presentations, client deliverables, or internal training materials.\\xa0Organization-wide access to premium security featuresPreviously, customers needed a 50 seat minimum to access premium security features. With just one Max user, your entire organization gains access to audit logs, SCIM integration, configurable data retention, and real-time usage analytics. You get comprehensive security visibility and control across your organization while maintaining cost efficiency.Early AccessPerplexity Enterprise Max is the fastest way to get new Perplexity features and products first. Enterprise Max users will get access to premium data sources and other releases like our upcoming Email Assistant that will be exclusive to Enterprise Max subscribers. We are constantly launching new products and features at Perplexity, and we’re excited to share them first with AI users who demand the newest and best.Available todayEnterprise Pro remains $40 per user per month ($400 per year), while Enterprise Max is $325 per user per month ($3,250 per year). Admins can upgrade users immediately in Account Settings. Within an organization, you can have both Enterprise Pro and Max users–giving advanced features to those who need them most.\\xa0Need help determining the right deployment strategy for your team? Reach out anytime at enterprise@perplexity.ai for guidance.\\xa0From the power users who first pushed Max's boundaries to the enterprise teams ready to transform how they work—Enterprise Max bridges the gap between breakthrough capabilities and organizational needs. Your team is ready for what's next.Stay curious. Build smarter. Move faster.Power your organization’s full potential\\xa0Introducing Perplexity Enterprise MaxEarly this summer we launched Perplexity Max for AI users who demand unlimited access to the best models and newest features from Perplexity.\\xa0Since then, we've seen powerful adoption by Max users, from financial analysts conducting deep market research to startup founders building out every aspect of their growing companies. Max users push the boundaries of what's possible with AI-assisted work.Meanwhile, one signal has been clear: some Max users are deploying Perplexity throughout their organizations in ways that also demand enterprise-level security and control. Today, we're excited to announce Perplexity Enterprise Max, our most advanced tier that combines all the advanced capabilities of Max with the enterprise-grade security and controls that organizations demand. Now your team members can access more powerful tools for research and content creation while IT maintains complete visibility and compliance.Here's what makes Enterprise Max transformative for organizations ready to maximize their AI investment, all while maintaining the same SOC 2 Type II compliance standards.What’s in Perplexity Enterprise Max?Unlimited Research and Labs Queries\\xa0Perplexity’s Research mode can source up to 100+ citations and give your team in-depth reports on complex topics while Labs helps transform ideas into tangible results through advanced data analysis, data visualization, and web apps. Enterprise Max provides unlimited Research Labs usage, enabling your team to analyze massive datasets, build sophisticated dashboards, and prototype solutions without hitting monthly caps to iterate continuously, explore deeper insights, and deliver comprehensive analysis.\\xa0Advanced model selectionDifferent challenges require different AI capabilities, and Enterprise Max gives your team access to our most advanced models including o3-pro and Opus 4.1 Thinking. These advanced models excel at complex reasoning, strategic analysis, and technical problem-solving.Comet Max AssistantMax Assistant is the most powerful and personal AI assistant yet. The same functionality is now available in Enterprise Max. Enterprise Max users who use Comet as their browser will access powerful models in their Assistant sidecar when prompting Perplexity in Comet.\\xa010x more file limits\\xa0Enterprise Max increases your file storage capacity to 10,000 files in your workspace and 5,000 files in Spaces — a dramatic increase from Pro's 500 and 5,000 respectively. This expanded capacity allows your teams to upload entire project libraries, cross-reference massive document sets, and build comprehensive knowledge bases from your organization's collective expertise. So your team can run deeper research across your files and the web.\\xa0Enhanced video generation\\xa0Create up to 15 high-quality videos per month with improved resolution and audio using Veo 3, directly within Perplexity. This capability transforms how your team communicates complex ideas, whether for executive presentations, client deliverables, or internal training materials.\\xa0Organization-wide access to premium security featuresPreviously, customers needed a 50 seat minimum to access premium security features. With just one Max user, your entire organization gains access to audit logs, SCIM integration, configurable data retention, and real-time usage analytics. You get comprehensive security visibility and control across your organization while maintaining cost efficiency.Early AccessPerplexity Enterprise Max is the fastest way to get new Perplexity features and products first. Enterprise Max users will get access to premium data sources and other releases like our upcoming Email Assistant that will be exclusive to Enterprise Max subscribers. We are constantly launching new products and features at Perplexity, and we’re excited to share them first with AI users who demand the newest and best.Available todayEnterprise Pro remains $40 per user per month ($400 per year), while Enterprise Max is $325 per user per month ($3,250 per year). Admins can upgrade users immediately in Account Settings. Within an organization, you can have both Enterprise Pro and Max users–giving advanced features to those who need them most.\\xa0Need help determining the right deployment strategy for your team? Reach out anytime at enterprise@perplexity.ai for guidance.\\xa0From the power users who first pushed Max's boundaries to the enterprise teams ready to transform how they work—Enterprise Max bridges the gap between breakthrough capabilities and organizational needs. Your team is ready for what's next.Stay curious. Build smarter. Move faster.\"),\n",
       " Document(metadata={'source': 'https://www.perplexity.ai/hub/blog/secure-credentials-on-comet-with-1password'}, page_content=\"Secure Credentials on Comet with 1PasswordPerplexity Partners with 1Password in First AI Browser Partnership for Password SecurityToday we’re announcing a partnership with 1Password, a trusted leader in extended access management. This collaboration integrates 1Password directly into Comet, ensuring users can power their full curiosity with personal security.\\xa0Before even launching to full global access, Comet has already become one of the most popular AI browsers in the world.\\xa0 Comet users explain that Comet is much more than a browser: Comet is a powerful AI assistant.Building a personal AI assistant comes with new security challenges, because a truly personal assistant also needs personal information to do great work.Security that keeps up with AIComet is a personal AI assistant that goes where you go on the internet. It learns with you, handles tasks on your behalf, and becomes smarter over time. But as Comet takes more actions on your behalf, from booking reservations to managing accounts, your credentials need protection that's just as intelligent.By combining Comet’s ability to reason, anticipate, and automate tasks with 1Password’s privacy-first security model, users stay protected as they adopt these new methods for working and learning online.\\xa0“For Perplexity, building security into Comet was a priority from the beginning,” said Kyle Polley, security staff at Perplexity.\\xa0 “1Password is a natural partner to provide the best credential security that users love.“1Password is committed to keeping your secrets secure” said Anand Srinivas, VP of Product and AI at 1Password.\\xa0 “We’re excited to bring our trusted access management to the new Comet browser.”\\xa0Effortless security for effortless browsingThe 1Password browser extension comes built into Comet, delivering protection without interrupting your flow:Keep your secrets a secret. Passwords and passkeys are encrypted by default, even as Comet takes action on your behalf.\\xa0Authentication that moves at your pace. Log into any account instantly without breaking your flow.\\xa0Transparency across all your actions. See how and when your credentials are being used no matter where you go.\\xa0Security that goes with you. Credentials sync seamlessly across browsers, devices, and operating systems.Privacy by designComet stores your browsing data locally on your device, and 1Password adds another layer of credential protection with end-to-end encryption. When Comet needs personal context to assist you, your data stays secure and never reaches Perplexity servers.Together, they create a browsing experience where security works as intelligently as your AI assistant. Experience secure, intelligent browsing where your AI assistant and your security work together seamlessly.The 1Password browser extension is available now in Comet, free for all existing 1Password users. New to 1Password? Get 25% off when you use it with Comet.Secure Credentials on Comet with 1PasswordPerplexity Partners with 1Password in First AI Browser Partnership for Password SecurityToday we’re announcing a partnership with 1Password, a trusted leader in extended access management. This collaboration integrates 1Password directly into Comet, ensuring users can power their full curiosity with personal security.\\xa0Before even launching to full global access, Comet has already become one of the most popular AI browsers in the world.\\xa0 Comet users explain that Comet is much more than a browser: Comet is a powerful AI assistant.Building a personal AI assistant comes with new security challenges, because a truly personal assistant also needs personal information to do great work.Security that keeps up with AIComet is a personal AI assistant that goes where you go on the internet. It learns with you, handles tasks on your behalf, and becomes smarter over time. But as Comet takes more actions on your behalf, from booking reservations to managing accounts, your credentials need protection that's just as intelligent.By combining Comet’s ability to reason, anticipate, and automate tasks with 1Password’s privacy-first security model, users stay protected as they adopt these new methods for working and learning online.\\xa0“For Perplexity, building security into Comet was a priority from the beginning,” said Kyle Polley, security staff at Perplexity.\\xa0 “1Password is a natural partner to provide the best credential security that users love.“1Password is committed to keeping your secrets secure” said Anand Srinivas, VP of Product and AI at 1Password.\\xa0 “We’re excited to bring our trusted access management to the new Comet browser.”\\xa0Effortless security for effortless browsingThe 1Password browser extension comes built into Comet, delivering protection without interrupting your flow:Keep your secrets a secret. Passwords and passkeys are encrypted by default, even as Comet takes action on your behalf.\\xa0Authentication that moves at your pace. Log into any account instantly without breaking your flow.\\xa0Transparency across all your actions. See how and when your credentials are being used no matter where you go.\\xa0Security that goes with you. Credentials sync seamlessly across browsers, devices, and operating systems.Privacy by designComet stores your browsing data locally on your device, and 1Password adds another layer of credential protection with end-to-end encryption. When Comet needs personal context to assist you, your data stays secure and never reaches Perplexity servers.Together, they create a browsing experience where security works as intelligently as your AI assistant. Experience secure, intelligent browsing where your AI assistant and your security work together seamlessly.The 1Password browser extension is available now in Comet, free for all existing 1Password users. New to 1Password? Get 25% off when you use it with Comet.Secure Credentials on Comet with 1PasswordPerplexity Partners with 1Password in First AI Browser Partnership for Password SecurityToday we’re announcing a partnership with 1Password, a trusted leader in extended access management. This collaboration integrates 1Password directly into Comet, ensuring users can power their full curiosity with personal security.\\xa0Before even launching to full global access, Comet has already become one of the most popular AI browsers in the world.\\xa0 Comet users explain that Comet is much more than a browser: Comet is a powerful AI assistant.Building a personal AI assistant comes with new security challenges, because a truly personal assistant also needs personal information to do great work.Security that keeps up with AIComet is a personal AI assistant that goes where you go on the internet. It learns with you, handles tasks on your behalf, and becomes smarter over time. But as Comet takes more actions on your behalf, from booking reservations to managing accounts, your credentials need protection that's just as intelligent.By combining Comet’s ability to reason, anticipate, and automate tasks with 1Password’s privacy-first security model, users stay protected as they adopt these new methods for working and learning online.\\xa0“For Perplexity, building security into Comet was a priority from the beginning,” said Kyle Polley, security staff at Perplexity.\\xa0 “1Password is a natural partner to provide the best credential security that users love.“1Password is committed to keeping your secrets secure” said Anand Srinivas, VP of Product and AI at 1Password.\\xa0 “We’re excited to bring our trusted access management to the new Comet browser.”\\xa0Effortless security for effortless browsingThe 1Password browser extension comes built into Comet, delivering protection without interrupting your flow:Keep your secrets a secret. Passwords and passkeys are encrypted by default, even as Comet takes action on your behalf.\\xa0Authentication that moves at your pace. Log into any account instantly without breaking your flow.\\xa0Transparency across all your actions. See how and when your credentials are being used no matter where you go.\\xa0Security that goes with you. Credentials sync seamlessly across browsers, devices, and operating systems.Privacy by designComet stores your browsing data locally on your device, and 1Password adds another layer of credential protection with end-to-end encryption. When Comet needs personal context to assist you, your data stays secure and never reaches Perplexity servers.Together, they create a browsing experience where security works as intelligently as your AI assistant. Experience secure, intelligent browsing where your AI assistant and your security work together seamlessly.The 1Password browser extension is available now in Comet, free for all existing 1Password users. New to 1Password? Get 25% off when you use it with Comet.\"),\n",
       " Document(metadata={'source': 'https://www.perplexity.ai/hub/blog/introducing-perplexity-for-government'}, page_content=\"Introducing Perplexity for GovernmentToday, we are announcing Perplexity for Government, an initiative that centralizes and expands our efforts to serve those who serve America.Perplexity’s mission is to build accurate, trustworthy AI that delivers universal access to reliable knowledge. Millions of people and thousands of enterprises rely on Perplexity to conduct research, organize knowledge, and interact with the world around them. Nowhere are these needs more pressing than the public sector, where dedicated public servants increasingly rely on frontier AI technologies to serve citizens and advance the national interest.Perplexity is proud to already serve thousands of U.S. federal employees each day. Among our own ranks are alumni of multiple branches and levels of government, who bring a rich understanding of agency missions to our engineering and policy teams. Through the AI Action Plan, OMB’s M-25-21 and M-25-22, and other executive actions, the Trump Administration has issued a bold call to action for government and industry to shape the future of public sector AI. We are answering that call with two new programs:First, all federal users of Perplexity will benefit from access to the most powerful AI models available today. Such use will be secure-by-default, with automatic enforcement of zero data usage protections. These benefits apply across all agencies and components starting today—no contract or paid subscription needed.Second, we are announcing Perplexity Enterprise Pro for Government, a custom offering of Perplexity’s best-in-class enterprise AI platform relied upon by America’s leading companies. Our offering will bring the benefits of Enterprise Pro to government agencies at a nominal cost of $0.25 and adapted to agencies’ unique needs.Empowering and Securing America’s Agencies with Secure-By-Default AIToday, most AI use within government is facilitated not through subscription-based products, but rather via publicly available web tools. These tools—whether integrated into search engines or accessed through dedicated websites—generally offer middling capabilities months to years behind the current AI frontier. Even more concerningly, these public tools offer scant protection for agency data. When federal users interact with these tools, their interactions are typically reused by the developer for model training and other undisclosed purposes.Perplexity believes that America's government deserves frontier AI capabilities that are secure by default—not by discretion. Starting today, Perplexity will automatically enforce zero data usage on all requests that we identify as originating from a U.S. government agency. Data from these requests will not be retained for training, improving, or evaluating AI models. These protections will apply to both logged in and logged out requests. In addition, we will automatically uplift all identified U.S. government requests to our most advanced models and product capabilities. We use a portfolio of technical measures to identify qualifying requests. To make these measures even more robust, agencies can submit additional network ranges to ensure full coverage.There is no Perplexity subscription required to enforce these safeguards—we are an American AI company and proud to do our part.Perplexity Enterprise Pro for GovernmentPerplexity is proud to offer a best-in-class enterprise AI platform: Perplexity Enterprise Pro. With Enterprise Pro, organizations are unleashing the full potential of their teams through secure AI that seamlessly integrates with their institutional knowledge. Importantly, Enterprise Pro offers a comprehensive suite of frontier AI models developed by Perplexity and third-party model developers. This approach empowers organizations with maximum optionality and derisks their AI strategies from shortcomings as to any individual model or provider.We are announcing Perplexity Enterprise Pro for Government, which will bring our platform to U.S. government agencies through a custom edition adapted to their unique requirements. Pursuant to GSA’s OneGov strategy, we are in active discussions with federal officials to offer Perplexity Enterprise Pro for Government through government-wide vehicles such as the Multiple Award Schedule, at $0.25 per agency for the first 15 months.Our Commitment to AmericaPerplexity for Government represents our enduring commitment to America’s success. We’re proud to step up for our nation by offering the only publicly available AI tool that is secure-by-default for federal use, along with a powerful enterprise offering that eliminates the risks of a single-model approach. We’re excited about the AI opportunities that lie ahead for public sector AI and we will have even more to announce in the coming months.Please visit our US Government hub or contact usgov@perplexity.ai to learn more about how we’re serving those who serve.Introducing Perplexity for GovernmentToday, we are announcing Perplexity for Government, an initiative that centralizes and expands our efforts to serve those who serve America.Perplexity’s mission is to build accurate, trustworthy AI that delivers universal access to reliable knowledge. Millions of people and thousands of enterprises rely on Perplexity to conduct research, organize knowledge, and interact with the world around them. Nowhere are these needs more pressing than the public sector, where dedicated public servants increasingly rely on frontier AI technologies to serve citizens and advance the national interest.Perplexity is proud to already serve thousands of U.S. federal employees each day. Among our own ranks are alumni of multiple branches and levels of government, who bring a rich understanding of agency missions to our engineering and policy teams. Through the AI Action Plan, OMB’s M-25-21 and M-25-22, and other executive actions, the Trump Administration has issued a bold call to action for government and industry to shape the future of public sector AI. We are answering that call with two new programs:First, all federal users of Perplexity will benefit from access to the most powerful AI models available today. Such use will be secure-by-default, with automatic enforcement of zero data usage protections. These benefits apply across all agencies and components starting today—no contract or paid subscription needed.Second, we are announcing Perplexity Enterprise Pro for Government, a custom offering of Perplexity’s best-in-class enterprise AI platform relied upon by America’s leading companies. Our offering will bring the benefits of Enterprise Pro to government agencies at a nominal cost of $0.25 and adapted to agencies’ unique needs.Empowering and Securing America’s Agencies with Secure-By-Default AIToday, most AI use within government is facilitated not through subscription-based products, but rather via publicly available web tools. These tools—whether integrated into search engines or accessed through dedicated websites—generally offer middling capabilities months to years behind the current AI frontier. Even more concerningly, these public tools offer scant protection for agency data. When federal users interact with these tools, their interactions are typically reused by the developer for model training and other undisclosed purposes.Perplexity believes that America's government deserves frontier AI capabilities that are secure by default—not by discretion. Starting today, Perplexity will automatically enforce zero data usage on all requests that we identify as originating from a U.S. government agency. Data from these requests will not be retained for training, improving, or evaluating AI models. These protections will apply to both logged in and logged out requests. In addition, we will automatically uplift all identified U.S. government requests to our most advanced models and product capabilities. We use a portfolio of technical measures to identify qualifying requests. To make these measures even more robust, agencies can submit additional network ranges to ensure full coverage.There is no Perplexity subscription required to enforce these safeguards—we are an American AI company and proud to do our part.Perplexity Enterprise Pro for GovernmentPerplexity is proud to offer a best-in-class enterprise AI platform: Perplexity Enterprise Pro. With Enterprise Pro, organizations are unleashing the full potential of their teams through secure AI that seamlessly integrates with their institutional knowledge. Importantly, Enterprise Pro offers a comprehensive suite of frontier AI models developed by Perplexity and third-party model developers. This approach empowers organizations with maximum optionality and derisks their AI strategies from shortcomings as to any individual model or provider.We are announcing Perplexity Enterprise Pro for Government, which will bring our platform to U.S. government agencies through a custom edition adapted to their unique requirements. Pursuant to GSA’s OneGov strategy, we are in active discussions with federal officials to offer Perplexity Enterprise Pro for Government through government-wide vehicles such as the Multiple Award Schedule, at $0.25 per agency for the first 15 months.Our Commitment to AmericaPerplexity for Government represents our enduring commitment to America’s success. We’re proud to step up for our nation by offering the only publicly available AI tool that is secure-by-default for federal use, along with a powerful enterprise offering that eliminates the risks of a single-model approach. We’re excited about the AI opportunities that lie ahead for public sector AI and we will have even more to announce in the coming months.Please visit our US Government hub or contact usgov@perplexity.ai to learn more about how we’re serving those who serve.Introducing Perplexity for GovernmentToday, we are announcing Perplexity for Government, an initiative that centralizes and expands our efforts to serve those who serve America.Perplexity’s mission is to build accurate, trustworthy AI that delivers universal access to reliable knowledge. Millions of people and thousands of enterprises rely on Perplexity to conduct research, organize knowledge, and interact with the world around them. Nowhere are these needs more pressing than the public sector, where dedicated public servants increasingly rely on frontier AI technologies to serve citizens and advance the national interest.Perplexity is proud to already serve thousands of U.S. federal employees each day. Among our own ranks are alumni of multiple branches and levels of government, who bring a rich understanding of agency missions to our engineering and policy teams. Through the AI Action Plan, OMB’s M-25-21 and M-25-22, and other executive actions, the Trump Administration has issued a bold call to action for government and industry to shape the future of public sector AI. We are answering that call with two new programs:First, all federal users of Perplexity will benefit from access to the most powerful AI models available today. Such use will be secure-by-default, with automatic enforcement of zero data usage protections. These benefits apply across all agencies and components starting today—no contract or paid subscription needed.Second, we are announcing Perplexity Enterprise Pro for Government, a custom offering of Perplexity’s best-in-class enterprise AI platform relied upon by America’s leading companies. Our offering will bring the benefits of Enterprise Pro to government agencies at a nominal cost of $0.25 and adapted to agencies’ unique needs.Empowering and Securing America’s Agencies with Secure-By-Default AIToday, most AI use within government is facilitated not through subscription-based products, but rather via publicly available web tools. These tools—whether integrated into search engines or accessed through dedicated websites—generally offer middling capabilities months to years behind the current AI frontier. Even more concerningly, these public tools offer scant protection for agency data. When federal users interact with these tools, their interactions are typically reused by the developer for model training and other undisclosed purposes.Perplexity believes that America's government deserves frontier AI capabilities that are secure by default—not by discretion. Starting today, Perplexity will automatically enforce zero data usage on all requests that we identify as originating from a U.S. government agency. Data from these requests will not be retained for training, improving, or evaluating AI models. These protections will apply to both logged in and logged out requests. In addition, we will automatically uplift all identified U.S. government requests to our most advanced models and product capabilities. We use a portfolio of technical measures to identify qualifying requests. To make these measures even more robust, agencies can submit additional network ranges to ensure full coverage.There is no Perplexity subscription required to enforce these safeguards—we are an American AI company and proud to do our part.Perplexity Enterprise Pro for GovernmentPerplexity is proud to offer a best-in-class enterprise AI platform: Perplexity Enterprise Pro. With Enterprise Pro, organizations are unleashing the full potential of their teams through secure AI that seamlessly integrates with their institutional knowledge. Importantly, Enterprise Pro offers a comprehensive suite of frontier AI models developed by Perplexity and third-party model developers. This approach empowers organizations with maximum optionality and derisks their AI strategies from shortcomings as to any individual model or provider.We are announcing Perplexity Enterprise Pro for Government, which will bring our platform to U.S. government agencies through a custom edition adapted to their unique requirements. Pursuant to GSA’s OneGov strategy, we are in active discussions with federal officials to offer Perplexity Enterprise Pro for Government through government-wide vehicles such as the Multiple Award Schedule, at $0.25 per agency for the first 15 months.Our Commitment to AmericaPerplexity for Government represents our enduring commitment to America’s success. We’re proud to step up for our nation by offering the only publicly available AI tool that is secure-by-default for federal use, along with a powerful enterprise offering that eliminates the risks of a single-model approach. We’re excited about the AI opportunities that lie ahead for public sector AI and we will have even more to announce in the coming months.Please visit our US Government hub or contact usgov@perplexity.ai to learn more about how we’re serving those who serve.\"),\n",
       " Document(metadata={'source': 'https://www.perplexity.ai/hub/blog/introducing-comet-plus'}, page_content=\"Introducing Comet PlusThe internet is no longer something we simply browse—it's where we live, work, and think.\\xa0Today we are announcing Comet Plus, a new subscription that gives Perplexity users access to premium content from a group of trusted publishers and journalists.\\xa0As the web has become central to our lives, our expectations of it have changed. That’s one reason we built Comet. Today, people use the internet to find and access content they love. They use AI on the internet to find answers to any question. They ask their AI assistants and agents to complete tasks across a combination of the open internet,\\xa0 their personal subscriptions, and their private tools.A better internet requires a better modelAI is an intrinsic part of how we browse the internet. When you read the news on a website with your morning coffee, that's human traffic. When you ask Perplexity to synthesize recent coverage of an industry trend, that's indexed traffic. When Comet Assistant scans your calendar and suggests articles relevant to your day’s meetings, that's agent traffic. Each represents real value creation, yet traditional models only compensate for the first.Meanwhile, publishers are trapped in the now-primitive economics of clicks and pageviews, compensation models built for a world that is rapidly changing. The quality of knowledge has never mattered more, but the systems that reward it haven't evolved.Comet Plus transforms how publishers are compensated in the AI age. As users demand a better internet in the age of AI, it’s time for a business model to ensure that publishers and journalists benefit from their contributions to a better internet.\\xa0A business model for The AI ageComet Plus is the first business model to reflect what users demand from the internet today, as AI empowers them to discover, learn, and do more. Subscribers of Comet Plus gain direct access to the sites and content of participating publishers, empower their AI assistants to complete tasks on those sites, and benefit from direct answers informed by the highest-quality content on the web.Publishers become more useful to their readers and their readers’ assistants, offering custom experiences for both human and user agent traffic while ensuring their important journalism and content contributes to a better internet for the users who demand it. In exchange, we’re distributing all of that revenue to participating publishers, minus a small portion for Perplexity’s compute costs.\\xa0Comet Plus is the first compensation model that allocates revenue to our partners based on three types of internet traffic: human visits, search citations, and agent actions. Publishers should settle for nothing less. The revenue allocation recognizes the reality that users now choose how they want to consume information: browsing manually, asking for AI-generated answers, or deploying agents for complex tasks. Publishers deserve compensation that matches this new reality.Publishers and AI, working together for usersThe internet is better on Comet. As the web has evolved beyond information to include knowledge, action, and opportunities, excellent content from publishers and journalists matters even more.\\xa0 Users are exhausted by\\xa0 low-quality clickbait, high-friction experiences, and endless blue links to human slop and no answers.\\xa0 They’re not alone. Publishers who’ve been forced to adapt to that model of the web have lost trust from readers and eroded their brands to stay afloat.\\xa0\\xa0AI has already given internet users the power to get more from the web, and the Comet browser is now the best way to ensure the web continues to get better, for everyone.\\xa0Comet Plus will be a $5 standalone subscription. Pro and Max subscribers get Comet Plus included in their memberships. We’ll announce our initial roster of publishing partners when Comet becomes available to all users for free. Publishers interested in participating can join the program by emailing us via publishers@perplexity.ai.Introducing Comet PlusThe internet is no longer something we simply browse—it's where we live, work, and think.\\xa0Today we are announcing Comet Plus, a new subscription that gives Perplexity users access to premium content from a group of trusted publishers and journalists.\\xa0As the web has become central to our lives, our expectations of it have changed. That’s one reason we built Comet. Today, people use the internet to find and access content they love. They use AI on the internet to find answers to any question. They ask their AI assistants and agents to complete tasks across a combination of the open internet,\\xa0 their personal subscriptions, and their private tools.A better internet requires a better modelAI is an intrinsic part of how we browse the internet. When you read the news on a website with your morning coffee, that's human traffic. When you ask Perplexity to synthesize recent coverage of an industry trend, that's indexed traffic. When Comet Assistant scans your calendar and suggests articles relevant to your day’s meetings, that's agent traffic. Each represents real value creation, yet traditional models only compensate for the first.Meanwhile, publishers are trapped in the now-primitive economics of clicks and pageviews, compensation models built for a world that is rapidly changing. The quality of knowledge has never mattered more, but the systems that reward it haven't evolved.Comet Plus transforms how publishers are compensated in the AI age. As users demand a better internet in the age of AI, it’s time for a business model to ensure that publishers and journalists benefit from their contributions to a better internet.\\xa0A business model for The AI ageComet Plus is the first business model to reflect what users demand from the internet today, as AI empowers them to discover, learn, and do more. Subscribers of Comet Plus gain direct access to the sites and content of participating publishers, empower their AI assistants to complete tasks on those sites, and benefit from direct answers informed by the highest-quality content on the web.Publishers become more useful to their readers and their readers’ assistants, offering custom experiences for both human and user agent traffic while ensuring their important journalism and content contributes to a better internet for the users who demand it. In exchange, we’re distributing all of that revenue to participating publishers, minus a small portion for Perplexity’s compute costs.\\xa0Comet Plus is the first compensation model that allocates revenue to our partners based on three types of internet traffic: human visits, search citations, and agent actions. Publishers should settle for nothing less. The revenue allocation recognizes the reality that users now choose how they want to consume information: browsing manually, asking for AI-generated answers, or deploying agents for complex tasks. Publishers deserve compensation that matches this new reality.Publishers and AI, working together for usersThe internet is better on Comet. As the web has evolved beyond information to include knowledge, action, and opportunities, excellent content from publishers and journalists matters even more.\\xa0 Users are exhausted by\\xa0 low-quality clickbait, high-friction experiences, and endless blue links to human slop and no answers.\\xa0 They’re not alone. Publishers who’ve been forced to adapt to that model of the web have lost trust from readers and eroded their brands to stay afloat.\\xa0\\xa0AI has already given internet users the power to get more from the web, and the Comet browser is now the best way to ensure the web continues to get better, for everyone.\\xa0Comet Plus will be a $5 standalone subscription. Pro and Max subscribers get Comet Plus included in their memberships. We’ll announce our initial roster of publishing partners when Comet becomes available to all users for free. Publishers interested in participating can join the program by emailing us via publishers@perplexity.ai.Introducing Comet PlusThe internet is no longer something we simply browse—it's where we live, work, and think.\\xa0Today we are announcing Comet Plus, a new subscription that gives Perplexity users access to premium content from a group of trusted publishers and journalists.\\xa0As the web has become central to our lives, our expectations of it have changed. That’s one reason we built Comet. Today, people use the internet to find and access content they love. They use AI on the internet to find answers to any question. They ask their AI assistants and agents to complete tasks across a combination of the open internet,\\xa0 their personal subscriptions, and their private tools.A better internet requires a better modelAI is an intrinsic part of how we browse the internet. When you read the news on a website with your morning coffee, that's human traffic. When you ask Perplexity to synthesize recent coverage of an industry trend, that's indexed traffic. When Comet Assistant scans your calendar and suggests articles relevant to your day’s meetings, that's agent traffic. Each represents real value creation, yet traditional models only compensate for the first.Meanwhile, publishers are trapped in the now-primitive economics of clicks and pageviews, compensation models built for a world that is rapidly changing. The quality of knowledge has never mattered more, but the systems that reward it haven't evolved.Comet Plus transforms how publishers are compensated in the AI age. As users demand a better internet in the age of AI, it’s time for a business model to ensure that publishers and journalists benefit from their contributions to a better internet.\\xa0A business model for The AI ageComet Plus is the first business model to reflect what users demand from the internet today, as AI empowers them to discover, learn, and do more. Subscribers of Comet Plus gain direct access to the sites and content of participating publishers, empower their AI assistants to complete tasks on those sites, and benefit from direct answers informed by the highest-quality content on the web.Publishers become more useful to their readers and their readers’ assistants, offering custom experiences for both human and user agent traffic while ensuring their important journalism and content contributes to a better internet for the users who demand it. In exchange, we’re distributing all of that revenue to participating publishers, minus a small portion for Perplexity’s compute costs.\\xa0Comet Plus is the first compensation model that allocates revenue to our partners based on three types of internet traffic: human visits, search citations, and agent actions. Publishers should settle for nothing less. The revenue allocation recognizes the reality that users now choose how they want to consume information: browsing manually, asking for AI-generated answers, or deploying agents for complex tasks. Publishers deserve compensation that matches this new reality.Publishers and AI, working together for usersThe internet is better on Comet. As the web has evolved beyond information to include knowledge, action, and opportunities, excellent content from publishers and journalists matters even more.\\xa0 Users are exhausted by\\xa0 low-quality clickbait, high-friction experiences, and endless blue links to human slop and no answers.\\xa0 They’re not alone. Publishers who’ve been forced to adapt to that model of the web have lost trust from readers and eroded their brands to stay afloat.\\xa0\\xa0AI has already given internet users the power to get more from the web, and the Comet browser is now the best way to ensure the web continues to get better, for everyone.\\xa0Comet Plus will be a $5 standalone subscription. Pro and Max subscribers get Comet Plus included in their memberships. We’ll announce our initial roster of publishing partners when Comet becomes available to all users for free. Publishers interested in participating can join the program by emailing us via publishers@perplexity.ai.\"),\n",
       " Document(metadata={'source': 'https://www.perplexity.ai/hub/blog/perplexity-x-theo-von-s-this-past-weekend'}, page_content='Perplexity x Theo Von’s This Past WeekendCuriosity has a new co-host.Today we’re announcing a new partnership with The Roost Podcast Network and Theo Von’s hit show, “This Past Weekend.” Our partnership pairs one of the world’s most famously inquisitive minds with the AI-powered answer engine built for people who are curious.If you’ve ever listened to Theo Von, you’re familiar with the moments of spontaneity and curiosity that make him a joy to listen to. Those impulsive, unscripted detours, when suddenly a question comes to mind and Theo asks “Can we look that up?” Those moments resonate because all of us are curious.\\xa0Starting today, Perplexity will power these “look it up” moments to deliver accurate and trustworthy answers on-screen for Theo, his guests, and his fans.\\xa0This isn’t a typical ad spot or podcast sponsor shoutout. Perplexity’s presence is baked in. Perplexity is integrated directly into the flow of the conversation, because curiosity is a natural part of all conversation.\\xa0\\xa0Theo’s curiosity is never staged or scripted, so our answers are always organic. No more Theo or team digging through endless blue links. No more results, just instant, accurate answers.Theo and his team wanted an answer partner that could keep up with the show’s signature pace, unpredictability, and off-the-cuff questions. The result? Theo puts it best:“We get so many moments where I just want an answer, in the moment, while staying in the flow of things,” said Theo. “As I’ve been using Perplexity it became clear that I should be using this not just in my life but in the show too.”“Theo’s natural curiosity has propelled him to the top of the podcast charts and inspired millions of people to ask more of the world around them,” said Dmitry Shevelenko, Perplexity’s Chief Business Officer. “This partnership is a natural way to show how the greatest minds don’t have the most answers, they have the most questions.”With Perplexity supporting the show, every tangent and every rabbit hole is now powered by accurate, trustworthy answers. It’s culture-shaping curiosity, supercharged by an AI-powered answer engine. And for millions of listeners? It’s simply the future of how we ask, answer, and explore.Perplexity x Theo Von’s This Past WeekendCuriosity has a new co-host.Today we’re announcing a new partnership with The Roost Podcast Network and Theo Von’s hit show, “This Past Weekend.” Our partnership pairs one of the world’s most famously inquisitive minds with the AI-powered answer engine built for people who are curious.If you’ve ever listened to Theo Von, you’re familiar with the moments of spontaneity and curiosity that make him a joy to listen to. Those impulsive, unscripted detours, when suddenly a question comes to mind and Theo asks “Can we look that up?” Those moments resonate because all of us are curious.\\xa0Starting today, Perplexity will power these “look it up” moments to deliver accurate and trustworthy answers on-screen for Theo, his guests, and his fans.\\xa0This isn’t a typical ad spot or podcast sponsor shoutout. Perplexity’s presence is baked in. Perplexity is integrated directly into the flow of the conversation, because curiosity is a natural part of all conversation.\\xa0\\xa0Theo’s curiosity is never staged or scripted, so our answers are always organic. No more Theo or team digging through endless blue links. No more results, just instant, accurate answers.Theo and his team wanted an answer partner that could keep up with the show’s signature pace, unpredictability, and off-the-cuff questions. The result? Theo puts it best:“We get so many moments where I just want an answer, in the moment, while staying in the flow of things,” said Theo. “As I’ve been using Perplexity it became clear that I should be using this not just in my life but in the show too.”“Theo’s natural curiosity has propelled him to the top of the podcast charts and inspired millions of people to ask more of the world around them,” said Dmitry Shevelenko, Perplexity’s Chief Business Officer. “This partnership is a natural way to show how the greatest minds don’t have the most answers, they have the most questions.”With Perplexity supporting the show, every tangent and every rabbit hole is now powered by accurate, trustworthy answers. It’s culture-shaping curiosity, supercharged by an AI-powered answer engine. And for millions of listeners? It’s simply the future of how we ask, answer, and explore.Perplexity x Theo Von’s This Past WeekendCuriosity has a new co-host.Today we’re announcing a new partnership with The Roost Podcast Network and Theo Von’s hit show, “This Past Weekend.” Our partnership pairs one of the world’s most famously inquisitive minds with the AI-powered answer engine built for people who are curious.If you’ve ever listened to Theo Von, you’re familiar with the moments of spontaneity and curiosity that make him a joy to listen to. Those impulsive, unscripted detours, when suddenly a question comes to mind and Theo asks “Can we look that up?” Those moments resonate because all of us are curious.\\xa0Starting today, Perplexity will power these “look it up” moments to deliver accurate and trustworthy answers on-screen for Theo, his guests, and his fans.\\xa0This isn’t a typical ad spot or podcast sponsor shoutout. Perplexity’s presence is baked in. Perplexity is integrated directly into the flow of the conversation, because curiosity is a natural part of all conversation.\\xa0\\xa0Theo’s curiosity is never staged or scripted, so our answers are always organic. No more Theo or team digging through endless blue links. No more results, just instant, accurate answers.Theo and his team wanted an answer partner that could keep up with the show’s signature pace, unpredictability, and off-the-cuff questions. The result? Theo puts it best:“We get so many moments where I just want an answer, in the moment, while staying in the flow of things,” said Theo. “As I’ve been using Perplexity it became clear that I should be using this not just in my life but in the show too.”“Theo’s natural curiosity has propelled him to the top of the podcast charts and inspired millions of people to ask more of the world around them,” said Dmitry Shevelenko, Perplexity’s Chief Business Officer. “This partnership is a natural way to show how the greatest minds don’t have the most answers, they have the most questions.”With Perplexity supporting the show, every tangent and every rabbit hole is now powered by accurate, trustworthy answers. It’s culture-shaping curiosity, supercharged by an AI-powered answer engine. And for millions of listeners? It’s simply the future of how we ask, answer, and explore.'),\n",
       " Document(metadata={'source': 'https://www.perplexity.ai/hub/blog/the-intelligent-business-introducing-comet-for-enterprise-pro'}, page_content=\"The Intelligent Business: Introducing Comet for Enterprise ProYesterday we launched Comet to all US Perplexity Pro subscribers. Today we are shipping Comet to all Perplexity Enterprise Pro users. Comet transforms how businesses interact with the internet. Since launching Comet to our Max subscribers, we've witnessed something remarkable. Organizations worldwide have discovered that Comet is more than a browser. The powerful personal Comet assistant eliminates the hundreds of tasks each day that steal the joy from modern work.\\xa0Already Enterprise-Ready: Security and Privacy by DesignWhat makes this enterprise announcement unique is that Comet has been enterprise-ready from day one. The security features, privacy, and compliance standards your business demands are already built into the core of Comet.Perplexity in Comet still operates with SOC 2 Type II certification and GDPR and HIPAA compliance. Administrator permissions allow adoption insights and download/upload controls for Perplexity. Browsing history, search queries, and AI interactions are stored locally on user devices with end-to-end encryption. Only when you explicitly request personalized assistance does Comet access minimal, purpose-specific data to deliver intelligent responses. Your competitive intelligence, strategic research, and confidential communications remain private and secure.The Business Benefits: Where Time Savings Meet JoyComet transforms everyday workflows from friction-filled tasks into fluid, intelligent experiences. Imagine asking your browser to schedule meetings by analyzing your calendar and creating Zoom links automatically, to draft email responses, research and compile comprehensive reports, manage your tabs intelligently, or book reservations while comparing options across platforms. These aren't futuristic possibilities, they're everyday realities for Comet users today.Comet's integration with enterprise applications creates unprecedented workflow efficiency. Gmail integration lets you summarize email threads and draft responses intelligently. Calendar connectivity helps schedule meetings and coordinate across time zones. Many popular enterprise tools don’t even require an integration. View Slack in Comet to monitor channels and respond to urgent messages. Take actions in your CRM. Most web interfaces work seamlessly with the Comet assistant, allowing you to onramp quickly into any new UI or navigate with new velocity through a challenging one.\\xa0As Comet reduces friction and pain points throughout your day, you’ll start to notice something powerful: All of these time savings help you keep your focus and reclaim the joy in work.Bringing Joy Back to Work Through Agentic AIOne of the coolest impacts we've observed is how Comet transforms the emotional experience of work. By eliminating tedious aspects of digital tasks, teams rediscover enthusiasm for collaboration and creativity. That meeting that could’ve been an email? It was.\\xa0What sets Comet apart is its agentic capabilities, the ability to find information and act on it intelligently and autonomously. Comet excels at multi-step business processes like vendor research, content creation, customer analysis, and market intelligence. When everyone on your team has an AI assistant that understands context and can act autonomously, collaboration is more dynamic and efficient.Enterprise Deployment and the FutureBuilt on Chromium, Comet ensures complete compatibility with existing Chrome extensions, enterprise security policies, web applications, and single sign-on systems. IT administrators can monitor usage patterns, configure user permissions, and deploy updates seamlessly across the organization.Comet will continue evolving with enhanced enterprise integrations, advanced automation capabilities, improved collaborative features, and expanded AI model options as frontier capabilities emerge.\\xa0We are just getting started.Bottom Line: The Intelligent Enterprise Starts TodayThe internet has evolved from something we simply browse to where we live, connect, and work. Comet is designed for this evolution and provides the intelligent interface a modern organization deserves.By combining enterprise-grade security, powerful AI capabilities, and intuitive user experiences, Comet makes work more enjoyable. When routine tasks become automated, when information becomes instantly accessible, when workflows become intelligent, teams rediscover what's possible when technology truly serves human potential.Comet is built on the foundation of curiosity, security, and the unwavering belief that technology should amplify human intelligence, not replace it. Because when work becomes intelligent, it becomes joyful again.Join us for a live demo on August 28th, 2025 at 9:00am PT.\\xa0Learn more at https://www.perplexity.ai/enterprise/comet\\xa0The Intelligent Business: Introducing Comet for Enterprise ProYesterday we launched Comet to all US Perplexity Pro subscribers. Today we are shipping Comet to all Perplexity Enterprise Pro users. Comet transforms how businesses interact with the internet. Since launching Comet to our Max subscribers, we've witnessed something remarkable. Organizations worldwide have discovered that Comet is more than a browser. The powerful personal Comet assistant eliminates the hundreds of tasks each day that steal the joy from modern work.\\xa0Already Enterprise-Ready: Security and Privacy by DesignWhat makes this enterprise announcement unique is that Comet has been enterprise-ready from day one. The security features, privacy, and compliance standards your business demands are already built into the core of Comet.Perplexity in Comet still operates with SOC 2 Type II certification and GDPR and HIPAA compliance. Administrator permissions allow adoption insights and download/upload controls for Perplexity. Browsing history, search queries, and AI interactions are stored locally on user devices with end-to-end encryption. Only when you explicitly request personalized assistance does Comet access minimal, purpose-specific data to deliver intelligent responses. Your competitive intelligence, strategic research, and confidential communications remain private and secure.The Business Benefits: Where Time Savings Meet JoyComet transforms everyday workflows from friction-filled tasks into fluid, intelligent experiences. Imagine asking your browser to schedule meetings by analyzing your calendar and creating Zoom links automatically, to draft email responses, research and compile comprehensive reports, manage your tabs intelligently, or book reservations while comparing options across platforms. These aren't futuristic possibilities, they're everyday realities for Comet users today.Comet's integration with enterprise applications creates unprecedented workflow efficiency. Gmail integration lets you summarize email threads and draft responses intelligently. Calendar connectivity helps schedule meetings and coordinate across time zones. Many popular enterprise tools don’t even require an integration. View Slack in Comet to monitor channels and respond to urgent messages. Take actions in your CRM. Most web interfaces work seamlessly with the Comet assistant, allowing you to onramp quickly into any new UI or navigate with new velocity through a challenging one.\\xa0As Comet reduces friction and pain points throughout your day, you’ll start to notice something powerful: All of these time savings help you keep your focus and reclaim the joy in work.Bringing Joy Back to Work Through Agentic AIOne of the coolest impacts we've observed is how Comet transforms the emotional experience of work. By eliminating tedious aspects of digital tasks, teams rediscover enthusiasm for collaboration and creativity. That meeting that could’ve been an email? It was.\\xa0What sets Comet apart is its agentic capabilities, the ability to find information and act on it intelligently and autonomously. Comet excels at multi-step business processes like vendor research, content creation, customer analysis, and market intelligence. When everyone on your team has an AI assistant that understands context and can act autonomously, collaboration is more dynamic and efficient.Enterprise Deployment and the FutureBuilt on Chromium, Comet ensures complete compatibility with existing Chrome extensions, enterprise security policies, web applications, and single sign-on systems. IT administrators can monitor usage patterns, configure user permissions, and deploy updates seamlessly across the organization.Comet will continue evolving with enhanced enterprise integrations, advanced automation capabilities, improved collaborative features, and expanded AI model options as frontier capabilities emerge.\\xa0We are just getting started.Bottom Line: The Intelligent Enterprise Starts TodayThe internet has evolved from something we simply browse to where we live, connect, and work. Comet is designed for this evolution and provides the intelligent interface a modern organization deserves.By combining enterprise-grade security, powerful AI capabilities, and intuitive user experiences, Comet makes work more enjoyable. When routine tasks become automated, when information becomes instantly accessible, when workflows become intelligent, teams rediscover what's possible when technology truly serves human potential.Comet is built on the foundation of curiosity, security, and the unwavering belief that technology should amplify human intelligence, not replace it. Because when work becomes intelligent, it becomes joyful again.Join us for a live demo on August 28th, 2025 at 9:00am PT.\\xa0Learn more at https://www.perplexity.ai/enterprise/comet\\xa0The Intelligent Business: Introducing Comet for Enterprise ProYesterday we launched Comet to all US Perplexity Pro subscribers. Today we are shipping Comet to all Perplexity Enterprise Pro users. Comet transforms how businesses interact with the internet. Since launching Comet to our Max subscribers, we've witnessed something remarkable. Organizations worldwide have discovered that Comet is more than a browser. The powerful personal Comet assistant eliminates the hundreds of tasks each day that steal the joy from modern work.\\xa0Already Enterprise-Ready: Security and Privacy by DesignWhat makes this enterprise announcement unique is that Comet has been enterprise-ready from day one. The security features, privacy, and compliance standards your business demands are already built into the core of Comet.Perplexity in Comet still operates with SOC 2 Type II certification and GDPR and HIPAA compliance. Administrator permissions allow adoption insights and download/upload controls for Perplexity. Browsing history, search queries, and AI interactions are stored locally on user devices with end-to-end encryption. Only when you explicitly request personalized assistance does Comet access minimal, purpose-specific data to deliver intelligent responses. Your competitive intelligence, strategic research, and confidential communications remain private and secure.The Business Benefits: Where Time Savings Meet JoyComet transforms everyday workflows from friction-filled tasks into fluid, intelligent experiences. Imagine asking your browser to schedule meetings by analyzing your calendar and creating Zoom links automatically, to draft email responses, research and compile comprehensive reports, manage your tabs intelligently, or book reservations while comparing options across platforms. These aren't futuristic possibilities, they're everyday realities for Comet users today.Comet's integration with enterprise applications creates unprecedented workflow efficiency. Gmail integration lets you summarize email threads and draft responses intelligently. Calendar connectivity helps schedule meetings and coordinate across time zones. Many popular enterprise tools don’t even require an integration. View Slack in Comet to monitor channels and respond to urgent messages. Take actions in your CRM. Most web interfaces work seamlessly with the Comet assistant, allowing you to onramp quickly into any new UI or navigate with new velocity through a challenging one.\\xa0As Comet reduces friction and pain points throughout your day, you’ll start to notice something powerful: All of these time savings help you keep your focus and reclaim the joy in work.Bringing Joy Back to Work Through Agentic AIOne of the coolest impacts we've observed is how Comet transforms the emotional experience of work. By eliminating tedious aspects of digital tasks, teams rediscover enthusiasm for collaboration and creativity. That meeting that could’ve been an email? It was.\\xa0What sets Comet apart is its agentic capabilities, the ability to find information and act on it intelligently and autonomously. Comet excels at multi-step business processes like vendor research, content creation, customer analysis, and market intelligence. When everyone on your team has an AI assistant that understands context and can act autonomously, collaboration is more dynamic and efficient.Enterprise Deployment and the FutureBuilt on Chromium, Comet ensures complete compatibility with existing Chrome extensions, enterprise security policies, web applications, and single sign-on systems. IT administrators can monitor usage patterns, configure user permissions, and deploy updates seamlessly across the organization.Comet will continue evolving with enhanced enterprise integrations, advanced automation capabilities, improved collaborative features, and expanded AI model options as frontier capabilities emerge.\\xa0We are just getting started.Bottom Line: The Intelligent Enterprise Starts TodayThe internet has evolved from something we simply browse to where we live, connect, and work. Comet is designed for this evolution and provides the intelligent interface a modern organization deserves.By combining enterprise-grade security, powerful AI capabilities, and intuitive user experiences, Comet makes work more enjoyable. When routine tasks become automated, when information becomes instantly accessible, when workflows become intelligent, teams rediscover what's possible when technology truly serves human potential.Comet is built on the foundation of curiosity, security, and the unwavering belief that technology should amplify human intelligence, not replace it. Because when work becomes intelligent, it becomes joyful again.Join us for a live demo on August 28th, 2025 at 9:00am PT.\\xa0Learn more at https://www.perplexity.ai/enterprise/comet\\xa0\"),\n",
       " Document(metadata={'source': 'https://www.perplexity.ai/hub/blog/gpt-oss-on-day-0'}, page_content='GPT-OSS on Day 0Perplexity is committed to making the best AI models available to curious people who demand accurate and trustworthy answers and agentic actions. OpenAI recently announced their open-weight models (gpt-oss-20b and gpt-oss-120b). As with many leading models, Perplexity is among the first organizations to evaluate gpt-oss-20b and gpt-oss-120b.In this post, we share the infrastructure decisions of our in-house inference stack that made Day-0 support possible. We focus on serving these models on NVIDIA H200 GPUs, detailing the kernel changes, deployment choices, and speed-cost trade-offs.GPT-OSS on HopperThe open-weight models are shipped using MXFP4 quantization, which helps them fit in the memory on consumer-grade hardware or achieve peak throughput on NVIDIA Blackwell. However, for initial evaluation, we wanted to run them on existing H200 Hopper clusters with minimal inference-engine changes. Hopper does not have dedicated FP4 tensor cores, which were introduced by Blackwell. Consequently, we decided to serve the models with FP8 precision instead to minimize the need for custom kernels and make best use of the available hardware.At a high level, transformer LLMs are structurally simple:Input embeddingA sequence of transformer layers. Each contains an attention block, and a dense MLP or sparse MoE block.Output logit projectionWhen bringing up a new transformer model, we are primarily interested in learning to what extent they match existing models to identify the changes required. OpenAI kindly shared an overview of their models ahead of time, allowing us to tweak our inference setup accordingly. We found that the model did require quite a number of small changes.Attention: GQA with Attention SinksEach attention head has a pre-trained sink parameter which is factored into SoftmaxYaRN positional encoding, similar to DeepSeek-V3QKV projections have biases, similar to Qwen 2Output projection has biasMLP: Sparse Mixture-of-Expert (MoE)SwiGLU activation function, different from most open-weight modelsSoftmax after Top-K for expert weightsExperts have biasesExpecting the number of parameters to be around 100B-200B, we decided to:Extend FlashInfer to support attention sinksRe-use the MoE implementation already built for the DeepSeek-V3 family of modelsChoose FP8 128x128 block scaling as the quantization scheme, extending the DeepGEMM kernels to support a bias term.In-house Inference EngineOur inference efforts are backed by our in-house inference engine, ROSE. We are building ROSE out to be a flexible framework that allows new models to be quickly brought up. Additionally, once we are committed to serving a model at scale, it provides a platform to optimize and boost performance to reach the state of the art.ROSE exposes an LLM engine, which can load a model and generate decoded tokens for it through multiple decoders, including speculative and MTP decoders. The models themselves are composed of a custom set of layers, which support various configuration knows for quantization and parallelism. Underneath the layers, it bundles both Triton and CUDA kernels that efficiently implement and fuse the computations performed by individual layers.ROSE is primarily built in Python and relies of PyTorch for model definitions. While Python allows us to be flexible and adapt to new models, we are migrating most performance-sensitive components, such as serving and batch scheduling, to Rust.To bring up a new model, we firstly define the model hierarchy and build a weight converter for it. After all weights are accounted for, we define a forward pass for the simplest TP=1 use case and ensure it works end-to-end. Finally, we implement and test support for various forms of parallelism before we release a new container and deploy the model on a cluster for evaluation. We then iterate on performance, relying on feedback from both dashboards and finer-grained benchmarks.ROSE operates on numeric tokens, with little knowledge of chat formats outside of integration tests. In parallel, we had to look into adjusting surrounding infrastructure to support the Harmony tokenizer and correctly map requests to input token sequences.With the GPT OSS model, we found that no changes were required to our decoders, CUDA graph harnesses or surrounding infrastructure, allowing us to focus our efforts on tweaking the kernels backing the model.Kernels for GPT-OSSSink AttentionSink attention introduces an extra bias factor prior to softmax, accumulated with the product of Q and K:attn = softmax(q @ k^T * sm_scale + sink) * VWhile a naive implementation is fairly trivial, efficient attention implementations, such as FlashInfer, parallelize the computation of attention across both independent heads and across the KV sequence length. Additionally, for numerical stability, Softmax is computed online while also subtracting the maximal element for numerical stability:softmax(x) = e^x / sum(e^x) = e ^ (x - max(x)) / sum(e ^ (x - max(x))The online implementation tracked a running maximum m initialized to -inf and a scale d to 1. By initializing the maximum to the sink value of the current head, we would ensure that it would get accumulated into the Softmax correctly. We had to take care to ensure that this maximum would only be included in the first chunk if attention was split across multiple blocks along the sequence length.We have adjusted the FlashInfer FA2 kernel template, used for both prefill and decode, to include the sink parameters.MoEThe main difference between the MoE layer of different models is usually the routing scheme. Expecting substantial variability, ROSE implements this kernel in Triton, allowing us to easily adapt it to new models. In the case of GPT-OSS, we had to adjust weight scoring to perform softmax only across the Top-K select experts, unlike some DeepSeek-style models which compute softmax across all experts. We found it is critical for accuracy to compute the expert weights in bfloat16 precision.The challenge with the MoE layer was due to our decision to re-use the existing DeepGEMM kernels, which lacked support for bias terms. Fortunately, over time we integrated deeply with them, making changes to better connect them with our custom all-to-all dispatch kernels. We added bias support to DeepGEMM by initializing the WGMMA accumulator registers to the bias values loaded from memory. This is a one-off during the processing of an MN block, thus the cost is likely negligible and the L2 cache ensures efficient access to these values.Since these models do not readily benefit from Data Parallelism, we re-used TP-only dispatch and combine kernels. The inputs to dispatch are replicated across all rank, thus dispatch selects the tokens for the locally available experts. Combine performs the weighted accumulation across NVLink.Performance TweaksEven though our goal was to bring up the models quickly, we did want to ensure that performance was still reasonable. After ensuring correctness against a reference implementation provided by OpenAI, we profiled our implementation using the default PyTorch profiler, watching out for block and thread allocations across kernels. The functionality we re-used was built out for models which had a larger hidden dimension of 7168, substantially larger than the 2880 of the GPT-OSS models. We specialized kernel launchers to pick better grid dimensions, significantly boosting throughput.Tokenization and ChannelsHarmony’s chat format has several important concepts for structuring conversations, reasoning content, and function calls. In this format, each message consists of a header and content:<|start|>{header}<|message|>{content}<The header carries metadata — most importantly the role (system, developer, user, assistant, tool), the channel, and a recipient such as to=functions.<function-name> when invoking a tool call.Channels (such as analysis, commentary, and final answer) make the model’s outputs more transparent and segmented. Because the format enforces this separation, the model can reason transparently (and be inspected for debugging) without leaking chain-of-thought into production. Recipient tags (to= headers) tell the runtime exactly which actor should receive the next payload — whether that’s a function, the model itself, or the end user.Our inference engine ROSE is format-agnostic. It receives a batch of input tokens and streams back output tokens. In our inference stack, the task of formatting inputs into a tokenized prompt and parsing LLM results back into structured messages is handled by the dedicated JSON API frontend service. We integrated Harmony by adding a new pluggable formatter/parser to this frontend, allowing the backend to remain unchanged while the frontend cleanly manages chat-format rules, tool calls, and streaming. This separation of concerns keeps the architecture modular, enabling quick adoption of new formats, response features with minimal backend changes.Cost and PerformanceGiven the relatively small size of GPT-OSS models, we limit the deployment of a replica to a single node to avoid inter-node communication overhead. Our implementation supports flexible combinations of expert parallelism (EP), data parallelism (DP), and tensor parallelism (TP). To determine the optimal deployment setup, we benchmarked all combinations of sharding strategies across batch sizes from 1 to 128. Thanks to Prefill-Decode Disaggregation, we are able to discuss the performance-cost trade-off separately for prefill and decode.For cost estimates, we assume a nominal H200 market rate of $3.00/hour (note: this is not our actual GPU price).We discovered that setting batch_size=1 and DP=1 yields the best prefill performance. Different TP configurations then provide varying trade-offs between first-token latency and cost. The tables below present prefill latency and cost per million input tokens for various configurations and input lengths.GPT-OSS 120B:Input Length81923276865536128000EP1 DP1 TP10.364s, $0.0372.421s, $0.0627.375s, $0.09423.902s, $0.156EP2 DP1 TP20.190s, $0.0391.245s, $0.0633.786s, $0.09612.116s, $0.158EP4 DP1 TP40.165s, $0.0670.900s, $0.0922.449s, $0.1257.165s, $0.187EP8 DP1 TP80.179s, $0.1450.839s, $0.1712.003s, $0.2045.119s, $0.267GPT-OSS 20B:Input Length81923276865536128000EP1 DP1 TP10.239s, $0.0241.610s, $0.0414.895s, $0.06216.037s, $0.104EP2 DP1 TP20.127s, $0.0260.833s, $0.0422.522s, $0.0648.143s, $0.106EP4 DP1 TP40.108s, $0.0440.593s, $0.0601.616s, $0.0824.766s, $0.124EP8 DP1 TP80.116s, $0.0940.547s, $0.1111.310s, $0.1333.365s, $0.175For decode, we discovered that EP4 DP1 TP4 is the best configuration for most cases. The four plots below illustrate the trade-off between decode speed and cost per million output tokens for both the 120B and 20B models at sequence lengths of 8192 and 32768.We have also considered using the smaller 20B model as a speculative decoding draft model for the larger 120B model. However, because the number of activated parameters is similar (3B vs. 5B), the resulting speedups were marginal.ReferencesDisaggregated Prefill and DecodeAccelerating Sonar Through Speculation\\u200bLower Latency and Higher Throughput with Multi-node DeepSeek Deployment\\u200bEfficient and Portable Mixture-of-Experts Communication\\u200bpplx-kernelsDeepGEMMFlashInfer: Efficient and Customizable Attention Engine for LLM Inference ServingEfficient Streaming Language Models with Attention SinksDeepSeek-V3 Technical Report\\u200b\\u200bOpenAI Harmony Response Format | OpenAI Cookbook\\u200bGPT-OSS on Day 0Perplexity is committed to making the best AI models available to curious people who demand accurate and trustworthy answers and agentic actions. OpenAI recently announced their open-weight models (gpt-oss-20b and gpt-oss-120b). As with many leading models, Perplexity is among the first organizations to evaluate gpt-oss-20b and gpt-oss-120b.In this post, we share the infrastructure decisions of our in-house inference stack that made Day-0 support possible. We focus on serving these models on NVIDIA H200 GPUs, detailing the kernel changes, deployment choices, and speed-cost trade-offs.GPT-OSS on HopperThe open-weight models are shipped using MXFP4 quantization, which helps them fit in the memory on consumer-grade hardware or achieve peak throughput on NVIDIA Blackwell. However, for initial evaluation, we wanted to run them on existing H200 Hopper clusters with minimal inference-engine changes. Hopper does not have dedicated FP4 tensor cores, which were introduced by Blackwell. Consequently, we decided to serve the models with FP8 precision instead to minimize the need for custom kernels and make best use of the available hardware.At a high level, transformer LLMs are structurally simple:Input embeddingA sequence of transformer layers. Each contains an attention block, and a dense MLP or sparse MoE block.Output logit projectionWhen bringing up a new transformer model, we are primarily interested in learning to what extent they match existing models to identify the changes required. OpenAI kindly shared an overview of their models ahead of time, allowing us to tweak our inference setup accordingly. We found that the model did require quite a number of small changes.Attention: GQA with Attention SinksEach attention head has a pre-trained sink parameter which is factored into SoftmaxYaRN positional encoding, similar to DeepSeek-V3QKV projections have biases, similar to Qwen 2Output projection has biasMLP: Sparse Mixture-of-Expert (MoE)SwiGLU activation function, different from most open-weight modelsSoftmax after Top-K for expert weightsExperts have biasesExpecting the number of parameters to be around 100B-200B, we decided to:Extend FlashInfer to support attention sinksRe-use the MoE implementation already built for the DeepSeek-V3 family of modelsChoose FP8 128x128 block scaling as the quantization scheme, extending the DeepGEMM kernels to support a bias term.In-house Inference EngineOur inference efforts are backed by our in-house inference engine, ROSE. We are building ROSE out to be a flexible framework that allows new models to be quickly brought up. Additionally, once we are committed to serving a model at scale, it provides a platform to optimize and boost performance to reach the state of the art.ROSE exposes an LLM engine, which can load a model and generate decoded tokens for it through multiple decoders, including speculative and MTP decoders. The models themselves are composed of a custom set of layers, which support various configuration knows for quantization and parallelism. Underneath the layers, it bundles both Triton and CUDA kernels that efficiently implement and fuse the computations performed by individual layers.ROSE is primarily built in Python and relies of PyTorch for model definitions. While Python allows us to be flexible and adapt to new models, we are migrating most performance-sensitive components, such as serving and batch scheduling, to Rust.To bring up a new model, we firstly define the model hierarchy and build a weight converter for it. After all weights are accounted for, we define a forward pass for the simplest TP=1 use case and ensure it works end-to-end. Finally, we implement and test support for various forms of parallelism before we release a new container and deploy the model on a cluster for evaluation. We then iterate on performance, relying on feedback from both dashboards and finer-grained benchmarks.ROSE operates on numeric tokens, with little knowledge of chat formats outside of integration tests. In parallel, we had to look into adjusting surrounding infrastructure to support the Harmony tokenizer and correctly map requests to input token sequences.With the GPT OSS model, we found that no changes were required to our decoders, CUDA graph harnesses or surrounding infrastructure, allowing us to focus our efforts on tweaking the kernels backing the model.Kernels for GPT-OSSSink AttentionSink attention introduces an extra bias factor prior to softmax, accumulated with the product of Q and K:attn = softmax(q @ k^T * sm_scale + sink) * VWhile a naive implementation is fairly trivial, efficient attention implementations, such as FlashInfer, parallelize the computation of attention across both independent heads and across the KV sequence length. Additionally, for numerical stability, Softmax is computed online while also subtracting the maximal element for numerical stability:softmax(x) = e^x / sum(e^x) = e ^ (x - max(x)) / sum(e ^ (x - max(x))The online implementation tracked a running maximum m initialized to -inf and a scale d to 1. By initializing the maximum to the sink value of the current head, we would ensure that it would get accumulated into the Softmax correctly. We had to take care to ensure that this maximum would only be included in the first chunk if attention was split across multiple blocks along the sequence length.We have adjusted the FlashInfer FA2 kernel template, used for both prefill and decode, to include the sink parameters.MoEThe main difference between the MoE layer of different models is usually the routing scheme. Expecting substantial variability, ROSE implements this kernel in Triton, allowing us to easily adapt it to new models. In the case of GPT-OSS, we had to adjust weight scoring to perform softmax only across the Top-K select experts, unlike some DeepSeek-style models which compute softmax across all experts. We found it is critical for accuracy to compute the expert weights in bfloat16 precision.The challenge with the MoE layer was due to our decision to re-use the existing DeepGEMM kernels, which lacked support for bias terms. Fortunately, over time we integrated deeply with them, making changes to better connect them with our custom all-to-all dispatch kernels. We added bias support to DeepGEMM by initializing the WGMMA accumulator registers to the bias values loaded from memory. This is a one-off during the processing of an MN block, thus the cost is likely negligible and the L2 cache ensures efficient access to these values.Since these models do not readily benefit from Data Parallelism, we re-used TP-only dispatch and combine kernels. The inputs to dispatch are replicated across all rank, thus dispatch selects the tokens for the locally available experts. Combine performs the weighted accumulation across NVLink.Performance TweaksEven though our goal was to bring up the models quickly, we did want to ensure that performance was still reasonable. After ensuring correctness against a reference implementation provided by OpenAI, we profiled our implementation using the default PyTorch profiler, watching out for block and thread allocations across kernels. The functionality we re-used was built out for models which had a larger hidden dimension of 7168, substantially larger than the 2880 of the GPT-OSS models. We specialized kernel launchers to pick better grid dimensions, significantly boosting throughput.Tokenization and ChannelsHarmony’s chat format has several important concepts for structuring conversations, reasoning content, and function calls. In this format, each message consists of a header and content:<|start|>{header}<|message|>{content}<The header carries metadata — most importantly the role (system, developer, user, assistant, tool), the channel, and a recipient such as to=functions.<function-name> when invoking a tool call.Channels (such as analysis, commentary, and final answer) make the model’s outputs more transparent and segmented. Because the format enforces this separation, the model can reason transparently (and be inspected for debugging) without leaking chain-of-thought into production. Recipient tags (to= headers) tell the runtime exactly which actor should receive the next payload — whether that’s a function, the model itself, or the end user.Our inference engine ROSE is format-agnostic. It receives a batch of input tokens and streams back output tokens. In our inference stack, the task of formatting inputs into a tokenized prompt and parsing LLM results back into structured messages is handled by the dedicated JSON API frontend service. We integrated Harmony by adding a new pluggable formatter/parser to this frontend, allowing the backend to remain unchanged while the frontend cleanly manages chat-format rules, tool calls, and streaming. This separation of concerns keeps the architecture modular, enabling quick adoption of new formats, response features with minimal backend changes.Cost and PerformanceGiven the relatively small size of GPT-OSS models, we limit the deployment of a replica to a single node to avoid inter-node communication overhead. Our implementation supports flexible combinations of expert parallelism (EP), data parallelism (DP), and tensor parallelism (TP). To determine the optimal deployment setup, we benchmarked all combinations of sharding strategies across batch sizes from 1 to 128. Thanks to Prefill-Decode Disaggregation, we are able to discuss the performance-cost trade-off separately for prefill and decode.For cost estimates, we assume a nominal H200 market rate of $3.00/hour (note: this is not our actual GPU price).We discovered that setting batch_size=1 and DP=1 yields the best prefill performance. Different TP configurations then provide varying trade-offs between first-token latency and cost. The tables below present prefill latency and cost per million input tokens for various configurations and input lengths.GPT-OSS 120B:Input Length81923276865536128000EP1 DP1 TP10.364s, $0.0372.421s, $0.0627.375s, $0.09423.902s, $0.156EP2 DP1 TP20.190s, $0.0391.245s, $0.0633.786s, $0.09612.116s, $0.158EP4 DP1 TP40.165s, $0.0670.900s, $0.0922.449s, $0.1257.165s, $0.187EP8 DP1 TP80.179s, $0.1450.839s, $0.1712.003s, $0.2045.119s, $0.267GPT-OSS 20B:Input Length81923276865536128000EP1 DP1 TP10.239s, $0.0241.610s, $0.0414.895s, $0.06216.037s, $0.104EP2 DP1 TP20.127s, $0.0260.833s, $0.0422.522s, $0.0648.143s, $0.106EP4 DP1 TP40.108s, $0.0440.593s, $0.0601.616s, $0.0824.766s, $0.124EP8 DP1 TP80.116s, $0.0940.547s, $0.1111.310s, $0.1333.365s, $0.175For decode, we discovered that EP4 DP1 TP4 is the best configuration for most cases. The four plots below illustrate the trade-off between decode speed and cost per million output tokens for both the 120B and 20B models at sequence lengths of 8192 and 32768.We have also considered using the smaller 20B model as a speculative decoding draft model for the larger 120B model. However, because the number of activated parameters is similar (3B vs. 5B), the resulting speedups were marginal.ReferencesDisaggregated Prefill and DecodeAccelerating Sonar Through Speculation\\u200bLower Latency and Higher Throughput with Multi-node DeepSeek Deployment\\u200bEfficient and Portable Mixture-of-Experts Communication\\u200bpplx-kernelsDeepGEMMFlashInfer: Efficient and Customizable Attention Engine for LLM Inference ServingEfficient Streaming Language Models with Attention SinksDeepSeek-V3 Technical Report\\u200b\\u200bOpenAI Harmony Response Format | OpenAI Cookbook\\u200bGPT-OSS on Day 0Perplexity is committed to making the best AI models available to curious people who demand accurate and trustworthy answers and agentic actions. OpenAI recently announced their open-weight models (gpt-oss-20b and gpt-oss-120b). As with many leading models, Perplexity is among the first organizations to evaluate gpt-oss-20b and gpt-oss-120b.In this post, we share the infrastructure decisions of our in-house inference stack that made Day-0 support possible. We focus on serving these models on NVIDIA H200 GPUs, detailing the kernel changes, deployment choices, and speed-cost trade-offs.GPT-OSS on HopperThe open-weight models are shipped using MXFP4 quantization, which helps them fit in the memory on consumer-grade hardware or achieve peak throughput on NVIDIA Blackwell. However, for initial evaluation, we wanted to run them on existing H200 Hopper clusters with minimal inference-engine changes. Hopper does not have dedicated FP4 tensor cores, which were introduced by Blackwell. Consequently, we decided to serve the models with FP8 precision instead to minimize the need for custom kernels and make best use of the available hardware.At a high level, transformer LLMs are structurally simple:Input embeddingA sequence of transformer layers. Each contains an attention block, and a dense MLP or sparse MoE block.Output logit projectionWhen bringing up a new transformer model, we are primarily interested in learning to what extent they match existing models to identify the changes required. OpenAI kindly shared an overview of their models ahead of time, allowing us to tweak our inference setup accordingly. We found that the model did require quite a number of small changes.Attention: GQA with Attention SinksEach attention head has a pre-trained sink parameter which is factored into SoftmaxYaRN positional encoding, similar to DeepSeek-V3QKV projections have biases, similar to Qwen 2Output projection has biasMLP: Sparse Mixture-of-Expert (MoE)SwiGLU activation function, different from most open-weight modelsSoftmax after Top-K for expert weightsExperts have biasesExpecting the number of parameters to be around 100B-200B, we decided to:Extend FlashInfer to support attention sinksRe-use the MoE implementation already built for the DeepSeek-V3 family of modelsChoose FP8 128x128 block scaling as the quantization scheme, extending the DeepGEMM kernels to support a bias term.In-house Inference EngineOur inference efforts are backed by our in-house inference engine, ROSE. We are building ROSE out to be a flexible framework that allows new models to be quickly brought up. Additionally, once we are committed to serving a model at scale, it provides a platform to optimize and boost performance to reach the state of the art.ROSE exposes an LLM engine, which can load a model and generate decoded tokens for it through multiple decoders, including speculative and MTP decoders. The models themselves are composed of a custom set of layers, which support various configuration knows for quantization and parallelism. Underneath the layers, it bundles both Triton and CUDA kernels that efficiently implement and fuse the computations performed by individual layers.ROSE is primarily built in Python and relies of PyTorch for model definitions. While Python allows us to be flexible and adapt to new models, we are migrating most performance-sensitive components, such as serving and batch scheduling, to Rust.To bring up a new model, we firstly define the model hierarchy and build a weight converter for it. After all weights are accounted for, we define a forward pass for the simplest TP=1 use case and ensure it works end-to-end. Finally, we implement and test support for various forms of parallelism before we release a new container and deploy the model on a cluster for evaluation. We then iterate on performance, relying on feedback from both dashboards and finer-grained benchmarks.ROSE operates on numeric tokens, with little knowledge of chat formats outside of integration tests. In parallel, we had to look into adjusting surrounding infrastructure to support the Harmony tokenizer and correctly map requests to input token sequences.With the GPT OSS model, we found that no changes were required to our decoders, CUDA graph harnesses or surrounding infrastructure, allowing us to focus our efforts on tweaking the kernels backing the model.Kernels for GPT-OSSSink AttentionSink attention introduces an extra bias factor prior to softmax, accumulated with the product of Q and K:attn = softmax(q @ k^T * sm_scale + sink) * VWhile a naive implementation is fairly trivial, efficient attention implementations, such as FlashInfer, parallelize the computation of attention across both independent heads and across the KV sequence length. Additionally, for numerical stability, Softmax is computed online while also subtracting the maximal element for numerical stability:softmax(x) = e^x / sum(e^x) = e ^ (x - max(x)) / sum(e ^ (x - max(x))The online implementation tracked a running maximum m initialized to -inf and a scale d to 1. By initializing the maximum to the sink value of the current head, we would ensure that it would get accumulated into the Softmax correctly. We had to take care to ensure that this maximum would only be included in the first chunk if attention was split across multiple blocks along the sequence length.We have adjusted the FlashInfer FA2 kernel template, used for both prefill and decode, to include the sink parameters.MoEThe main difference between the MoE layer of different models is usually the routing scheme. Expecting substantial variability, ROSE implements this kernel in Triton, allowing us to easily adapt it to new models. In the case of GPT-OSS, we had to adjust weight scoring to perform softmax only across the Top-K select experts, unlike some DeepSeek-style models which compute softmax across all experts. We found it is critical for accuracy to compute the expert weights in bfloat16 precision.The challenge with the MoE layer was due to our decision to re-use the existing DeepGEMM kernels, which lacked support for bias terms. Fortunately, over time we integrated deeply with them, making changes to better connect them with our custom all-to-all dispatch kernels. We added bias support to DeepGEMM by initializing the WGMMA accumulator registers to the bias values loaded from memory. This is a one-off during the processing of an MN block, thus the cost is likely negligible and the L2 cache ensures efficient access to these values.Since these models do not readily benefit from Data Parallelism, we re-used TP-only dispatch and combine kernels. The inputs to dispatch are replicated across all rank, thus dispatch selects the tokens for the locally available experts. Combine performs the weighted accumulation across NVLink.Performance TweaksEven though our goal was to bring up the models quickly, we did want to ensure that performance was still reasonable. After ensuring correctness against a reference implementation provided by OpenAI, we profiled our implementation using the default PyTorch profiler, watching out for block and thread allocations across kernels. The functionality we re-used was built out for models which had a larger hidden dimension of 7168, substantially larger than the 2880 of the GPT-OSS models. We specialized kernel launchers to pick better grid dimensions, significantly boosting throughput.Tokenization and ChannelsHarmony’s chat format has several important concepts for structuring conversations, reasoning content, and function calls. In this format, each message consists of a header and content:<|start|>{header}<|message|>{content}<The header carries metadata — most importantly the role (system, developer, user, assistant, tool), the channel, and a recipient such as to=functions.<function-name> when invoking a tool call.Channels (such as analysis, commentary, and final answer) make the model’s outputs more transparent and segmented. Because the format enforces this separation, the model can reason transparently (and be inspected for debugging) without leaking chain-of-thought into production. Recipient tags (to= headers) tell the runtime exactly which actor should receive the next payload — whether that’s a function, the model itself, or the end user.Our inference engine ROSE is format-agnostic. It receives a batch of input tokens and streams back output tokens. In our inference stack, the task of formatting inputs into a tokenized prompt and parsing LLM results back into structured messages is handled by the dedicated JSON API frontend service. We integrated Harmony by adding a new pluggable formatter/parser to this frontend, allowing the backend to remain unchanged while the frontend cleanly manages chat-format rules, tool calls, and streaming. This separation of concerns keeps the architecture modular, enabling quick adoption of new formats, response features with minimal backend changes.Cost and PerformanceGiven the relatively small size of GPT-OSS models, we limit the deployment of a replica to a single node to avoid inter-node communication overhead. Our implementation supports flexible combinations of expert parallelism (EP), data parallelism (DP), and tensor parallelism (TP). To determine the optimal deployment setup, we benchmarked all combinations of sharding strategies across batch sizes from 1 to 128. Thanks to Prefill-Decode Disaggregation, we are able to discuss the performance-cost trade-off separately for prefill and decode.For cost estimates, we assume a nominal H200 market rate of $3.00/hour (note: this is not our actual GPU price).We discovered that setting batch_size=1 and DP=1 yields the best prefill performance. Different TP configurations then provide varying trade-offs between first-token latency and cost. The tables below present prefill latency and cost per million input tokens for various configurations and input lengths.GPT-OSS 120B:Input Length81923276865536128000EP1 DP1 TP10.364s, $0.0372.421s, $0.0627.375s, $0.09423.902s, $0.156EP2 DP1 TP20.190s, $0.0391.245s, $0.0633.786s, $0.09612.116s, $0.158EP4 DP1 TP40.165s, $0.0670.900s, $0.0922.449s, $0.1257.165s, $0.187EP8 DP1 TP80.179s, $0.1450.839s, $0.1712.003s, $0.2045.119s, $0.267GPT-OSS 20B:Input Length81923276865536128000EP1 DP1 TP10.239s, $0.0241.610s, $0.0414.895s, $0.06216.037s, $0.104EP2 DP1 TP20.127s, $0.0260.833s, $0.0422.522s, $0.0648.143s, $0.106EP4 DP1 TP40.108s, $0.0440.593s, $0.0601.616s, $0.0824.766s, $0.124EP8 DP1 TP80.116s, $0.0940.547s, $0.1111.310s, $0.1333.365s, $0.175For decode, we discovered that EP4 DP1 TP4 is the best configuration for most cases. The four plots below illustrate the trade-off between decode speed and cost per million output tokens for both the 120B and 20B models at sequence lengths of 8192 and 32768.We have also considered using the smaller 20B model as a speculative decoding draft model for the larger 120B model. However, because the number of activated parameters is similar (3B vs. 5B), the resulting speedups were marginal.ReferencesDisaggregated Prefill and DecodeAccelerating Sonar Through Speculation\\u200bLower Latency and Higher Throughput with Multi-node DeepSeek Deployment\\u200bEfficient and Portable Mixture-of-Experts Communication\\u200bpplx-kernelsDeepGEMMFlashInfer: Efficient and Customizable Attention Engine for LLM Inference ServingEfficient Streaming Language Models with Attention SinksDeepSeek-V3 Technical Report\\u200b\\u200bOpenAI Harmony Response Format | OpenAI Cookbook\\u200b'),\n",
       " Document(metadata={'source': 'https://www.perplexity.ai/hub/blog/agents-or-bots-making-sense-of-ai-on-the-open-web'}, page_content='Agents or Bots?\\xa0Making Sense of AI on the Open WebAs the internet evolves, so too do the ways in which we access and interact with information. In the earliest days of the web, automated bots played a simple, well-understood role: indexing sites for search, checking links, or scraping data according to clear rules set by website owners.\\xa0But with the rise of AI-powered assistants and user-driven agents, the boundary between what counts as \"just a bot\" and what serves the immediate needs of real people has become increasingly blurred.\\xa0The Rise of Digital AssistantsModern AI assistants work fundamentally differently from traditional web crawling. When you ask Perplexity a question that requires current information—say, \"What are the latest reviews for that new restaurant?\"—the AI doesn\\'t already have that information sitting in a database somewhere. Instead, it goes to the relevant websites, reads the content, and brings back a summary tailored to your specific question.This is fundamentally different from traditional web crawling, in which crawlers systematically visit millions of pages to build massive databases, whether anyone asked for that specific information or not. User-driven agents, by contrast, only fetch content when a real person requests something specific, and they use that content immediately to answer the user\\'s question. Perplexity’s user-driven agents do not store the information or train with it.Why This Distinction MattersThe difference between automated crawling and user-driven fetching isn\\'t just technical—it\\'s about who gets to access information on the open web. When Google\\'s search engine crawls to build its index, that\\'s different from when it fetches a webpage because you asked for a preview. Google\\'s \"user-triggered fetchers\" prioritize your experience over robots.txt restrictions because these requests happen on your behalf.The same applies to AI assistants. When Perplexity fetches a webpage, it\\'s because you asked a specific question requiring current information. The content isn\\'t stored for training—it\\'s used immediately to answer your question.When companies like Cloudflare mischaracterize user-driven AI assistants as malicious bots, they\\'re arguing that any automated tool serving users should be suspect—a position that would criminalize email clients and web browsers, or any other service a would-be gatekeeper decided they don’t like.\\xa0This controversy reveals that Cloudflare\\'s systems are fundamentally inadequate for distinguishing between legitimate AI assistants and actual threats.\\xa0 If you can\\'t tell a helpful digital assistant from a malicious scraper, then you probably shouldn\\'t be making decisions about what constitutes legitimate web traffic.This overblocking hurts everyone. Consider someone using AI to research medical conditions, compare product reviews, or access news from multiple sources. If their assistant gets blocked as a \"malicious bot,\" they lose access to valuable information.The result is a two-tiered internet where your access depends not on your needs, but on whether your chosen tools have been blessed by infrastructure controllers, who will care more about your means.\\xa0This undermines user choice and threatens the open web\\'s accessibility for innovative services competing with established giants.A Call for Clarity: How User Agents Actually WorkAn AI assistant works just like a human assistant. When you ask an AI assistant a question that requires current information, they don’t already know the answer. They look it up for you in order to complete whatever task you’ve asked.On Perplexity and all other agentic AI platforms, this happens in real-time, in response to your request, and the information is used immediately to answer your question. It\\'s not stored in massive databases for future use, and it\\'s not used to train AI models.User-driven agents only act when users make specific requests, and they only fetch the content needed to fulfill those requests.\\xa0This is the fundamental difference between a user agent and a bot.\\xa0Directly Addressing Cloudflare: A Question of CompetenceCloudflare\\'s recent blog post managed to get almost everything wrong about how modern AI assistants actually work.In addition to misunderstanding 20-25M user agent requests are not scrapers, Cloudflare claimed that Perplexity was engaging in \"stealth crawling,\" using hidden bots and impersonation tactics to bypass website restrictions. But the technical facts tell a different story.It appears Cloudflare confused Perplexity with 3-6M daily requests of unrelated traffic from BrowserBase, a third-party cloud browser service that Perplexity only occasionally uses for highly specialized tasks (less than 45,000 daily requests).\\xa0\\xa0Because Cloudflare has conveniently obfuscated their methodology and declined to answer questions helping our teams understand, we can only narrow this down to two possible explanations.Cloudflare needed a clever publicity moment and we–their own customer–happened to be a useful name to get them one.\\xa0Cloudflare fundamentally misattributed 3-6M daily requests from BrowserBase\\'s automated browser service to Perplexity, a basic traffic analysis failure that\\'s particularly embarrassing for a company whose core business is understanding and categorizing web traffic.\\xa0Whichever explanation is the truth, the technical errors in Cloudflare\\'s analysis aren\\'t just embarrassing—they\\'re disqualifying. When you misattribute millions of requests, publish completely inaccurate technical diagrams, and demonstrate a fundamental misunderstanding of how modern AI assistants work, you\\'ve forfeited any claim to expertise in this space.This controversy reveals that Cloudflare\\'s systems are fundamentally inadequate for distinguishing between legitimate AI assistants and actual threats.\\xa0 If you can\\'t tell a helpful digital assistant from a malicious scraper, then you probably shouldn\\'t be making decisions about what constitutes legitimate web traffic.The bluster around this issue also reveals that Cloudflare’s leadership is either dangerously misinformed on the basics of AI, or simply more flair than cloud. This matters because Cloudflare\\'s customers include businesses of all types, companies who can’t afford to trust their infrastructure with charlatan publicity stunts.\\xa0Even more embarrassing, Cloudflare published a technical diagram supposedly showing \"Perplexity\\'s crawling workflow\" that bears no resemblance to how Perplexity actually works. If Cloudfare were truly interested in understanding the data they were seeing, how our systems work, or these fundamental concepts outlined above, they could have done what we encourage all Perplexity users to do. Just ask.Agents or Bots?\\xa0Making Sense of AI on the Open WebAs the internet evolves, so too do the ways in which we access and interact with information. In the earliest days of the web, automated bots played a simple, well-understood role: indexing sites for search, checking links, or scraping data according to clear rules set by website owners.\\xa0But with the rise of AI-powered assistants and user-driven agents, the boundary between what counts as \"just a bot\" and what serves the immediate needs of real people has become increasingly blurred.\\xa0The Rise of Digital AssistantsModern AI assistants work fundamentally differently from traditional web crawling. When you ask Perplexity a question that requires current information—say, \"What are the latest reviews for that new restaurant?\"—the AI doesn\\'t already have that information sitting in a database somewhere. Instead, it goes to the relevant websites, reads the content, and brings back a summary tailored to your specific question.This is fundamentally different from traditional web crawling, in which crawlers systematically visit millions of pages to build massive databases, whether anyone asked for that specific information or not. User-driven agents, by contrast, only fetch content when a real person requests something specific, and they use that content immediately to answer the user\\'s question. Perplexity’s user-driven agents do not store the information or train with it.Why This Distinction MattersThe difference between automated crawling and user-driven fetching isn\\'t just technical—it\\'s about who gets to access information on the open web. When Google\\'s search engine crawls to build its index, that\\'s different from when it fetches a webpage because you asked for a preview. Google\\'s \"user-triggered fetchers\" prioritize your experience over robots.txt restrictions because these requests happen on your behalf.The same applies to AI assistants. When Perplexity fetches a webpage, it\\'s because you asked a specific question requiring current information. The content isn\\'t stored for training—it\\'s used immediately to answer your question.When companies like Cloudflare mischaracterize user-driven AI assistants as malicious bots, they\\'re arguing that any automated tool serving users should be suspect—a position that would criminalize email clients and web browsers, or any other service a would-be gatekeeper decided they don’t like.\\xa0This controversy reveals that Cloudflare\\'s systems are fundamentally inadequate for distinguishing between legitimate AI assistants and actual threats.\\xa0 If you can\\'t tell a helpful digital assistant from a malicious scraper, then you probably shouldn\\'t be making decisions about what constitutes legitimate web traffic.This overblocking hurts everyone. Consider someone using AI to research medical conditions, compare product reviews, or access news from multiple sources. If their assistant gets blocked as a \"malicious bot,\" they lose access to valuable information.The result is a two-tiered internet where your access depends not on your needs, but on whether your chosen tools have been blessed by infrastructure controllers, who will care more about your means.\\xa0This undermines user choice and threatens the open web\\'s accessibility for innovative services competing with established giants.A Call for Clarity: How User Agents Actually WorkAn AI assistant works just like a human assistant. When you ask an AI assistant a question that requires current information, they don’t already know the answer. They look it up for you in order to complete whatever task you’ve asked.On Perplexity and all other agentic AI platforms, this happens in real-time, in response to your request, and the information is used immediately to answer your question. It\\'s not stored in massive databases for future use, and it\\'s not used to train AI models.User-driven agents only act when users make specific requests, and they only fetch the content needed to fulfill those requests.\\xa0This is the fundamental difference between a user agent and a bot.\\xa0Directly Addressing Cloudflare: A Question of CompetenceCloudflare\\'s recent blog post managed to get almost everything wrong about how modern AI assistants actually work.In addition to misunderstanding 20-25M user agent requests are not scrapers, Cloudflare claimed that Perplexity was engaging in \"stealth crawling,\" using hidden bots and impersonation tactics to bypass website restrictions. But the technical facts tell a different story.It appears Cloudflare confused Perplexity with 3-6M daily requests of unrelated traffic from BrowserBase, a third-party cloud browser service that Perplexity only occasionally uses for highly specialized tasks (less than 45,000 daily requests).\\xa0\\xa0Because Cloudflare has conveniently obfuscated their methodology and declined to answer questions helping our teams understand, we can only narrow this down to two possible explanations.Cloudflare needed a clever publicity moment and we–their own customer–happened to be a useful name to get them one.\\xa0Cloudflare fundamentally misattributed 3-6M daily requests from BrowserBase\\'s automated browser service to Perplexity, a basic traffic analysis failure that\\'s particularly embarrassing for a company whose core business is understanding and categorizing web traffic.\\xa0Whichever explanation is the truth, the technical errors in Cloudflare\\'s analysis aren\\'t just embarrassing—they\\'re disqualifying. When you misattribute millions of requests, publish completely inaccurate technical diagrams, and demonstrate a fundamental misunderstanding of how modern AI assistants work, you\\'ve forfeited any claim to expertise in this space.This controversy reveals that Cloudflare\\'s systems are fundamentally inadequate for distinguishing between legitimate AI assistants and actual threats.\\xa0 If you can\\'t tell a helpful digital assistant from a malicious scraper, then you probably shouldn\\'t be making decisions about what constitutes legitimate web traffic.The bluster around this issue also reveals that Cloudflare’s leadership is either dangerously misinformed on the basics of AI, or simply more flair than cloud. This matters because Cloudflare\\'s customers include businesses of all types, companies who can’t afford to trust their infrastructure with charlatan publicity stunts.\\xa0Even more embarrassing, Cloudflare published a technical diagram supposedly showing \"Perplexity\\'s crawling workflow\" that bears no resemblance to how Perplexity actually works. If Cloudfare were truly interested in understanding the data they were seeing, how our systems work, or these fundamental concepts outlined above, they could have done what we encourage all Perplexity users to do. Just ask.Agents or Bots?\\xa0Making Sense of AI on the Open WebAs the internet evolves, so too do the ways in which we access and interact with information. In the earliest days of the web, automated bots played a simple, well-understood role: indexing sites for search, checking links, or scraping data according to clear rules set by website owners.\\xa0But with the rise of AI-powered assistants and user-driven agents, the boundary between what counts as \"just a bot\" and what serves the immediate needs of real people has become increasingly blurred.\\xa0The Rise of Digital AssistantsModern AI assistants work fundamentally differently from traditional web crawling. When you ask Perplexity a question that requires current information—say, \"What are the latest reviews for that new restaurant?\"—the AI doesn\\'t already have that information sitting in a database somewhere. Instead, it goes to the relevant websites, reads the content, and brings back a summary tailored to your specific question.This is fundamentally different from traditional web crawling, in which crawlers systematically visit millions of pages to build massive databases, whether anyone asked for that specific information or not. User-driven agents, by contrast, only fetch content when a real person requests something specific, and they use that content immediately to answer the user\\'s question. Perplexity’s user-driven agents do not store the information or train with it.Why This Distinction MattersThe difference between automated crawling and user-driven fetching isn\\'t just technical—it\\'s about who gets to access information on the open web. When Google\\'s search engine crawls to build its index, that\\'s different from when it fetches a webpage because you asked for a preview. Google\\'s \"user-triggered fetchers\" prioritize your experience over robots.txt restrictions because these requests happen on your behalf.The same applies to AI assistants. When Perplexity fetches a webpage, it\\'s because you asked a specific question requiring current information. The content isn\\'t stored for training—it\\'s used immediately to answer your question.When companies like Cloudflare mischaracterize user-driven AI assistants as malicious bots, they\\'re arguing that any automated tool serving users should be suspect—a position that would criminalize email clients and web browsers, or any other service a would-be gatekeeper decided they don’t like.\\xa0This controversy reveals that Cloudflare\\'s systems are fundamentally inadequate for distinguishing between legitimate AI assistants and actual threats.\\xa0 If you can\\'t tell a helpful digital assistant from a malicious scraper, then you probably shouldn\\'t be making decisions about what constitutes legitimate web traffic.This overblocking hurts everyone. Consider someone using AI to research medical conditions, compare product reviews, or access news from multiple sources. If their assistant gets blocked as a \"malicious bot,\" they lose access to valuable information.The result is a two-tiered internet where your access depends not on your needs, but on whether your chosen tools have been blessed by infrastructure controllers, who will care more about your means.\\xa0This undermines user choice and threatens the open web\\'s accessibility for innovative services competing with established giants.A Call for Clarity: How User Agents Actually WorkAn AI assistant works just like a human assistant. When you ask an AI assistant a question that requires current information, they don’t already know the answer. They look it up for you in order to complete whatever task you’ve asked.On Perplexity and all other agentic AI platforms, this happens in real-time, in response to your request, and the information is used immediately to answer your question. It\\'s not stored in massive databases for future use, and it\\'s not used to train AI models.User-driven agents only act when users make specific requests, and they only fetch the content needed to fulfill those requests.\\xa0This is the fundamental difference between a user agent and a bot.\\xa0Directly Addressing Cloudflare: A Question of CompetenceCloudflare\\'s recent blog post managed to get almost everything wrong about how modern AI assistants actually work.In addition to misunderstanding 20-25M user agent requests are not scrapers, Cloudflare claimed that Perplexity was engaging in \"stealth crawling,\" using hidden bots and impersonation tactics to bypass website restrictions. But the technical facts tell a different story.It appears Cloudflare confused Perplexity with 3-6M daily requests of unrelated traffic from BrowserBase, a third-party cloud browser service that Perplexity only occasionally uses for highly specialized tasks (less than 45,000 daily requests).\\xa0\\xa0Because Cloudflare has conveniently obfuscated their methodology and declined to answer questions helping our teams understand, we can only narrow this down to two possible explanations.Cloudflare needed a clever publicity moment and we–their own customer–happened to be a useful name to get them one.\\xa0Cloudflare fundamentally misattributed 3-6M daily requests from BrowserBase\\'s automated browser service to Perplexity, a basic traffic analysis failure that\\'s particularly embarrassing for a company whose core business is understanding and categorizing web traffic.\\xa0Whichever explanation is the truth, the technical errors in Cloudflare\\'s analysis aren\\'t just embarrassing—they\\'re disqualifying. When you misattribute millions of requests, publish completely inaccurate technical diagrams, and demonstrate a fundamental misunderstanding of how modern AI assistants work, you\\'ve forfeited any claim to expertise in this space.This controversy reveals that Cloudflare\\'s systems are fundamentally inadequate for distinguishing between legitimate AI assistants and actual threats.\\xa0 If you can\\'t tell a helpful digital assistant from a malicious scraper, then you probably shouldn\\'t be making decisions about what constitutes legitimate web traffic.The bluster around this issue also reveals that Cloudflare’s leadership is either dangerously misinformed on the basics of AI, or simply more flair than cloud. This matters because Cloudflare\\'s customers include businesses of all types, companies who can’t afford to trust their infrastructure with charlatan publicity stunts.\\xa0Even more embarrassing, Cloudflare published a technical diagram supposedly showing \"Perplexity\\'s crawling workflow\" that bears no resemblance to how Perplexity actually works. If Cloudfare were truly interested in understanding the data they were seeing, how our systems work, or these fundamental concepts outlined above, they could have done what we encourage all Perplexity users to do. Just ask.'),\n",
       " Document(metadata={'source': 'https://www.perplexity.ai/hub/blog/book-a-table-with-perplexity-and-opentable'}, page_content='Book a Table with Perplexity and OpenTableSearching for the perfect restaurant is a sport. You\\'re matching a mood, a vibe, a dietary quirk, and wrangling friends, family, or a partner who “doesn’t care” but also has strong opinions. The endless browsing? Overwhelming. The pressure? Real. But it doesn’t have to be.Meet the new way to find and book your next great meal—right inside Perplexity, powered by OpenTable.THE TABLE IS THE ANSWERPerplexity has always been about answers—not just flooding you with links. Looking for an Italian spot in San Francisco with a romantic ambiance and great cacio e pepe? Just ask.Perplexity understands exactly what you mean, then serves up choices among OpenTable’s 60k global restaurant partners that meet your criteria. No more losing your place, no more decision fatigue—inspiration to action in seconds.\"As more diners rely on AI to uncover new dining experiences, this integration connects Perplexity’s millions of users directly to our global restaurant network,\" said Sagar Mehta, CTO of OpenTable. \"Integrating with trusted AI platforms is yet another way we’re making it effortless to find and secure the right table.\"FROM “WHERE SHOULD WE GO?” TO GOINGPerplexity removes the headache and clarifies the action. Here\\'s how it feels in real life:A group of friends blows into town at the last minute—it’s late, everyone’s hungry, half the crew is vegetarian, and someone wants “a view.” Just ask.You want to impress on date night: sushi that’s actually good, quiet enough to talk, allergen-safe for your shellfish-averse date. Ask Perplexity.Whatever you’re looking for, just type or speak your wishlist into Perplexity. Instantly, you’ll see restaurants on OpenTable that actually fit—and you can grab a reservation right there. No more shuffling between apps or reading reviews until you go cross-eyed.ACCURATE, PERSONAL, EFFORTLESSPerplexity isn’t just a shortcut. It’s your second brain, trained to care as much as you do about the details (and quirks) that make a meal memorable. It aggregates opinions from all over the web, but tailors the results to what you actually want—not just what’s generically “top-rated.”And yes, you can get as specific as you want—bring on the “fanciest decor in town” and “happy hour with giant margaritas” requests.BOOK WITH A TAPWhen you see the “Reserve” button, you’re set. It’s as direct as it sounds—your table is booked via OpenTable, confirmation and all. The future of dining out is fewer clicks, no more call-and-wait, and finally making good on your group chat’s dinner plans.Ready to make finding food way less stressful—and way more you? Ask Perplexity, book it, and get on with the fun part. Your next memorable meal is already waiting.Book a Table with Perplexity and OpenTableSearching for the perfect restaurant is a sport. You\\'re matching a mood, a vibe, a dietary quirk, and wrangling friends, family, or a partner who “doesn’t care” but also has strong opinions. The endless browsing? Overwhelming. The pressure? Real. But it doesn’t have to be.Meet the new way to find and book your next great meal—right inside Perplexity, powered by OpenTable.THE TABLE IS THE ANSWERPerplexity has always been about answers—not just flooding you with links. Looking for an Italian spot in San Francisco with a romantic ambiance and great cacio e pepe? Just ask.Perplexity understands exactly what you mean, then serves up choices among OpenTable’s 60k global restaurant partners that meet your criteria. No more losing your place, no more decision fatigue—inspiration to action in seconds.\"As more diners rely on AI to uncover new dining experiences, this integration connects Perplexity’s millions of users directly to our global restaurant network,\" said Sagar Mehta, CTO of OpenTable. \"Integrating with trusted AI platforms is yet another way we’re making it effortless to find and secure the right table.\"FROM “WHERE SHOULD WE GO?” TO GOINGPerplexity removes the headache and clarifies the action. Here\\'s how it feels in real life:A group of friends blows into town at the last minute—it’s late, everyone’s hungry, half the crew is vegetarian, and someone wants “a view.” Just ask.You want to impress on date night: sushi that’s actually good, quiet enough to talk, allergen-safe for your shellfish-averse date. Ask Perplexity.Whatever you’re looking for, just type or speak your wishlist into Perplexity. Instantly, you’ll see restaurants on OpenTable that actually fit—and you can grab a reservation right there. No more shuffling between apps or reading reviews until you go cross-eyed.ACCURATE, PERSONAL, EFFORTLESSPerplexity isn’t just a shortcut. It’s your second brain, trained to care as much as you do about the details (and quirks) that make a meal memorable. It aggregates opinions from all over the web, but tailors the results to what you actually want—not just what’s generically “top-rated.”And yes, you can get as specific as you want—bring on the “fanciest decor in town” and “happy hour with giant margaritas” requests.BOOK WITH A TAPWhen you see the “Reserve” button, you’re set. It’s as direct as it sounds—your table is booked via OpenTable, confirmation and all. The future of dining out is fewer clicks, no more call-and-wait, and finally making good on your group chat’s dinner plans.Ready to make finding food way less stressful—and way more you? Ask Perplexity, book it, and get on with the fun part. Your next memorable meal is already waiting.Book a Table with Perplexity and OpenTableSearching for the perfect restaurant is a sport. You\\'re matching a mood, a vibe, a dietary quirk, and wrangling friends, family, or a partner who “doesn’t care” but also has strong opinions. The endless browsing? Overwhelming. The pressure? Real. But it doesn’t have to be.Meet the new way to find and book your next great meal—right inside Perplexity, powered by OpenTable.THE TABLE IS THE ANSWERPerplexity has always been about answers—not just flooding you with links. Looking for an Italian spot in San Francisco with a romantic ambiance and great cacio e pepe? Just ask.Perplexity understands exactly what you mean, then serves up choices among OpenTable’s 60k global restaurant partners that meet your criteria. No more losing your place, no more decision fatigue—inspiration to action in seconds.\"As more diners rely on AI to uncover new dining experiences, this integration connects Perplexity’s millions of users directly to our global restaurant network,\" said Sagar Mehta, CTO of OpenTable. \"Integrating with trusted AI platforms is yet another way we’re making it effortless to find and secure the right table.\"FROM “WHERE SHOULD WE GO?” TO GOINGPerplexity removes the headache and clarifies the action. Here\\'s how it feels in real life:A group of friends blows into town at the last minute—it’s late, everyone’s hungry, half the crew is vegetarian, and someone wants “a view.” Just ask.You want to impress on date night: sushi that’s actually good, quiet enough to talk, allergen-safe for your shellfish-averse date. Ask Perplexity.Whatever you’re looking for, just type or speak your wishlist into Perplexity. Instantly, you’ll see restaurants on OpenTable that actually fit—and you can grab a reservation right there. No more shuffling between apps or reading reviews until you go cross-eyed.ACCURATE, PERSONAL, EFFORTLESSPerplexity isn’t just a shortcut. It’s your second brain, trained to care as much as you do about the details (and quirks) that make a meal memorable. It aggregates opinions from all over the web, but tailors the results to what you actually want—not just what’s generically “top-rated.”And yes, you can get as specific as you want—bring on the “fanciest decor in town” and “happy hour with giant margaritas” requests.BOOK WITH A TAPWhen you see the “Reserve” button, you’re set. It’s as direct as it sounds—your table is booked via OpenTable, confirmation and all. The future of dining out is fewer clicks, no more call-and-wait, and finally making good on your group chat’s dinner plans.Ready to make finding food way less stressful—and way more you? Ask Perplexity, book it, and get on with the fun part. Your next memorable meal is already waiting.'),\n",
       " Document(metadata={'source': 'https://www.perplexity.ai/hub/blog/welcoming-gannett-to-the-perplexity-publisher-program'}, page_content='Welcoming Gannett to the Perplexity Publisher ProgramToday, we\\'re excited to announce that Gannett Co., Inc., the largest local-to-national media organization in the United States, has joined the Perplexity Publisher Program. Through this strategic content licensing agreement, premium journalism from USA TODAY and the USA TODAY Network\\'s 200+ local publications will be integrated into Perplexity\\'s AI-powered search experiences.As one of our largest U.S.-based media partners, Gannett\\'s trusted content will be accessible in Perplexity answers, including on our newly released agentic web browser, Comet, currently available to Perplexity Max subscribers and daily releases from our growing waitlist.\\xa0Additionally, Gannett will have access to Perplexity\\'s Sonar API and Enterprise Pro for all employees. As a diversified media company dedicated to empowering communities through trusted journalism, Gannett understands the value of trustworthy, accurate answers from all aspects of a business, making our Sonar API a natural choice for operational processes and Enterprise Pro a powerful tool for every employee.Elevating Quality Content in the Digital AgeFor too long, the internet has been dominated by content optimized for search algorithms rather than human readers. Publishers have been forced to game SEO systems, stuffing keywords and chasing metrics that don\\'t reflect genuine value to audiences. The result? A digital landscape cluttered with content designed to satisfy results instead of answers.\\xa0It is clear that AI is irreversibly changing that dynamic, and readers are excited about it. At Perplexity, we\\'re focused on accurate and trustworthy answers and AI agents that serve the world’s curiosity.\\xa0 Our AI-powered answer engine rewards quality, accuracy, and relevance.\\xa0When users ask Comet for personal and proactive assistance they demand accurate and helpful action. When users ask questions, they get accurate answers with clear citations to authoritative sources, giving credit where it\\'s due while providing the comprehensive information they actually need.This creates a fundamental shift in incentives. The advance of AI will allow publishers to focus on what they have always done best: producing in-depth, credible journalism that serves their communities. Quality content gets recognized and surfaced appropriately, while readers benefit from answers that are both accurate and comprehensive.Building the Future of Publishing TogetherAs AI reshapes a digital landscape that was optimized for a different era, our priority is to partner with the world\\'s best publishers to ensure they benefit from the new business models we discover together.\"The partnership with Gannett exemplifies this vision. Rather than replacing traditional journalism, we\\'re creating new pathways for quality content to reach audiences who need reliable information. This collaboration acknowledges that the future of digital publishing requires innovative approaches that serve both publishers and readers.\\xa0\\xa0Like every generational shift in technology, the earliest adopters innovate and thrive in each new era. The digital transformation of the AI Age isn\\'t about choosing between human journalism and AI technology—it\\'s about working together to create better outcomes for everyone. Publishers gain new revenue streams and broader reach for their quality content, while users receive more accurate, contextual answers to their questions.Through partnerships like this one with Gannett, we\\'re building an information ecosystem where excellence in journalism is rewarded, readers get trustworthy answers, and publishers can thrive in the AI era. This is just the beginning of reimagining how quality information flows through our digital world and everyone who creates it and consumes it benefits.Welcoming Gannett to the Perplexity Publisher ProgramToday, we\\'re excited to announce that Gannett Co., Inc., the largest local-to-national media organization in the United States, has joined the Perplexity Publisher Program. Through this strategic content licensing agreement, premium journalism from USA TODAY and the USA TODAY Network\\'s 200+ local publications will be integrated into Perplexity\\'s AI-powered search experiences.As one of our largest U.S.-based media partners, Gannett\\'s trusted content will be accessible in Perplexity answers, including on our newly released agentic web browser, Comet, currently available to Perplexity Max subscribers and daily releases from our growing waitlist.\\xa0Additionally, Gannett will have access to Perplexity\\'s Sonar API and Enterprise Pro for all employees. As a diversified media company dedicated to empowering communities through trusted journalism, Gannett understands the value of trustworthy, accurate answers from all aspects of a business, making our Sonar API a natural choice for operational processes and Enterprise Pro a powerful tool for every employee.Elevating Quality Content in the Digital AgeFor too long, the internet has been dominated by content optimized for search algorithms rather than human readers. Publishers have been forced to game SEO systems, stuffing keywords and chasing metrics that don\\'t reflect genuine value to audiences. The result? A digital landscape cluttered with content designed to satisfy results instead of answers.\\xa0It is clear that AI is irreversibly changing that dynamic, and readers are excited about it. At Perplexity, we\\'re focused on accurate and trustworthy answers and AI agents that serve the world’s curiosity.\\xa0 Our AI-powered answer engine rewards quality, accuracy, and relevance.\\xa0When users ask Comet for personal and proactive assistance they demand accurate and helpful action. When users ask questions, they get accurate answers with clear citations to authoritative sources, giving credit where it\\'s due while providing the comprehensive information they actually need.This creates a fundamental shift in incentives. The advance of AI will allow publishers to focus on what they have always done best: producing in-depth, credible journalism that serves their communities. Quality content gets recognized and surfaced appropriately, while readers benefit from answers that are both accurate and comprehensive.Building the Future of Publishing TogetherAs AI reshapes a digital landscape that was optimized for a different era, our priority is to partner with the world\\'s best publishers to ensure they benefit from the new business models we discover together.\"The partnership with Gannett exemplifies this vision. Rather than replacing traditional journalism, we\\'re creating new pathways for quality content to reach audiences who need reliable information. This collaboration acknowledges that the future of digital publishing requires innovative approaches that serve both publishers and readers.\\xa0\\xa0Like every generational shift in technology, the earliest adopters innovate and thrive in each new era. The digital transformation of the AI Age isn\\'t about choosing between human journalism and AI technology—it\\'s about working together to create better outcomes for everyone. Publishers gain new revenue streams and broader reach for their quality content, while users receive more accurate, contextual answers to their questions.Through partnerships like this one with Gannett, we\\'re building an information ecosystem where excellence in journalism is rewarded, readers get trustworthy answers, and publishers can thrive in the AI era. This is just the beginning of reimagining how quality information flows through our digital world and everyone who creates it and consumes it benefits.Welcoming Gannett to the Perplexity Publisher ProgramToday, we\\'re excited to announce that Gannett Co., Inc., the largest local-to-national media organization in the United States, has joined the Perplexity Publisher Program. Through this strategic content licensing agreement, premium journalism from USA TODAY and the USA TODAY Network\\'s 200+ local publications will be integrated into Perplexity\\'s AI-powered search experiences.As one of our largest U.S.-based media partners, Gannett\\'s trusted content will be accessible in Perplexity answers, including on our newly released agentic web browser, Comet, currently available to Perplexity Max subscribers and daily releases from our growing waitlist.\\xa0Additionally, Gannett will have access to Perplexity\\'s Sonar API and Enterprise Pro for all employees. As a diversified media company dedicated to empowering communities through trusted journalism, Gannett understands the value of trustworthy, accurate answers from all aspects of a business, making our Sonar API a natural choice for operational processes and Enterprise Pro a powerful tool for every employee.Elevating Quality Content in the Digital AgeFor too long, the internet has been dominated by content optimized for search algorithms rather than human readers. Publishers have been forced to game SEO systems, stuffing keywords and chasing metrics that don\\'t reflect genuine value to audiences. The result? A digital landscape cluttered with content designed to satisfy results instead of answers.\\xa0It is clear that AI is irreversibly changing that dynamic, and readers are excited about it. At Perplexity, we\\'re focused on accurate and trustworthy answers and AI agents that serve the world’s curiosity.\\xa0 Our AI-powered answer engine rewards quality, accuracy, and relevance.\\xa0When users ask Comet for personal and proactive assistance they demand accurate and helpful action. When users ask questions, they get accurate answers with clear citations to authoritative sources, giving credit where it\\'s due while providing the comprehensive information they actually need.This creates a fundamental shift in incentives. The advance of AI will allow publishers to focus on what they have always done best: producing in-depth, credible journalism that serves their communities. Quality content gets recognized and surfaced appropriately, while readers benefit from answers that are both accurate and comprehensive.Building the Future of Publishing TogetherAs AI reshapes a digital landscape that was optimized for a different era, our priority is to partner with the world\\'s best publishers to ensure they benefit from the new business models we discover together.\"The partnership with Gannett exemplifies this vision. Rather than replacing traditional journalism, we\\'re creating new pathways for quality content to reach audiences who need reliable information. This collaboration acknowledges that the future of digital publishing requires innovative approaches that serve both publishers and readers.\\xa0\\xa0Like every generational shift in technology, the earliest adopters innovate and thrive in each new era. The digital transformation of the AI Age isn\\'t about choosing between human journalism and AI technology—it\\'s about working together to create better outcomes for everyone. Publishers gain new revenue streams and broader reach for their quality content, while users receive more accurate, contextual answers to their questions.Through partnerships like this one with Gannett, we\\'re building an information ecosystem where excellence in journalism is rewarded, readers get trustworthy answers, and publishers can thrive in the AI era. This is just the beginning of reimagining how quality information flows through our digital world and everyone who creates it and consumes it benefits.'),\n",
       " Document(metadata={'source': 'https://www.perplexity.ai/hub/blog/introducing-comet'}, page_content=\"Today we are launching Comet.\\xa0Comet is a web browser built for today’s internet. In the last 30 years, the internet has evolved from something we simply “browse” or “search.” The internet is where we live, work, and connect.\\xa0It’s also where we ask questions.\\xa0Curious minds have questions everywhere, and they find answers on every page, in every idea, through every task. Yet we've been trapped in long lines of tabs and hyperlinks, disjointed experiences that interrupt our natural flow of thought.In other words, the internet has become humanity's extended mind while our tools for using it remain primitive. Our interface for the web should be as fluid and responsive as human thought itself.\\xa0We built Comet to let the internet do what it has been begging to do: to amplify our intelligence.From Navigation To CognitionComet powers a shift from browsing to thinking.\\xa0Tabs that piled up waiting for your return now join one intelligent interface that understands how your mind works.\\xa0 Context-switching between dozens of applications, sites, and interfaces has stolen the focus and flow that bring joy to our work and fuel our curiosity.\\xa0The Comet assistant removes friction with every thought, actively conducting entire browsing sessions while you focus on what matters. Ask Comet which other sites have the same bike but ship it faster.\\xa0 Ask Comet to compare what you’re reading to something you already read.Comet allows you to ask questions anywhere they occur to you, whether you want to understand a complex concept, find hidden connections, create new possibilities, or solve problems that have been puzzling you.From Answers To ActionComet transforms entire browsing sessions into single, seamless interactions, collapsing complex workflows into fluid conversations.Ask Comet to book a meeting or send an email, based on something you saw. Ask Comet to buy something you forgot. Ask Comet to brief you for your day.\\xa0With Comet, you don't search for information—you think out loud, and Comet executes complete workflows while keeping perfect context. Research becomes conversation. Analysis becomes natural. Annoying tasks evaporate. The internet becomes an extension of your mind.\\xa0From Believability To AccuracyIn the DNA of Comet is Perplexity’s obsession with accurate and trustworthy answers. Answers are the foundation of curiosity for one reason: more knowledge gives us better questions.\\xa0Every day, trillions of dollars of decisions are made online, and the quality of those decisions depends on the reliability of the information behind them. Ask Comet to compare insurance plans. Ask Comet to help you understand a technology enough to decide whether to invest.\\xa0\\xa0Accurate answers are the foundation of decision-making. This will compound in importance with agentic AI, when assistants make decisions for us, faster and more often. Comet is like a second brain, helping with the best possible decisions in every situation.\\xa0From Consumption To CuriosityPerplexity’s mission is to serve the world’s curiosity.\\xa0Comet transforms any webpage into a portal of curiosity. Highlight any text to get instant explanations. Explore tangential ideas without losing your original context. Ask specialized questions or broad ones—Comet understands that genuine curiosity doesn't follow predetermined paths.Ask Comet what you’re missing. Ask Comet for counterpoints. Ask Comet where you should explore.Curiosity is personal. This allows your own curiosity to become the context for a reliable, proactive, and personalized assistant.\\xa0Comet learns how you think, in order to think better with you.\\xa0From Here To What's NextBeginning today, Comet is available to Perplexity Max subscribers.\\xa0Invite-only access will roll out slowly to our waitlist over the summer. New users will also receive a limited number of invites to share.In the meantime, you can join the waitlist here.It’ll be worth the wait, because Comet is just getting started.With our own roadmap, and with every new advancement in AI, we will continue to launch new features and functionality for Comet, improve experiences based on your feedback, and focus relentlessly–as we always have–on building accurate and trustworthy AI that fuels human curiosity. The future belongs to the people who never stop asking questions.\\xa0We're curious to see what you'll ask of Comet.Today we are launching Comet.\\xa0Comet is a web browser built for today’s internet. In the last 30 years, the internet has evolved from something we simply “browse” or “search.” The internet is where we live, work, and connect.\\xa0It’s also where we ask questions.\\xa0Curious minds have questions everywhere, and they find answers on every page, in every idea, through every task. Yet we've been trapped in long lines of tabs and hyperlinks, disjointed experiences that interrupt our natural flow of thought.In other words, the internet has become humanity's extended mind while our tools for using it remain primitive. Our interface for the web should be as fluid and responsive as human thought itself.\\xa0We built Comet to let the internet do what it has been begging to do: to amplify our intelligence.From Navigation To CognitionComet powers a shift from browsing to thinking.\\xa0Tabs that piled up waiting for your return now join one intelligent interface that understands how your mind works.\\xa0 Context-switching between dozens of applications, sites, and interfaces has stolen the focus and flow that bring joy to our work and fuel our curiosity.\\xa0The Comet assistant removes friction with every thought, actively conducting entire browsing sessions while you focus on what matters. Ask Comet which other sites have the same bike but ship it faster.\\xa0 Ask Comet to compare what you’re reading to something you already read.Comet allows you to ask questions anywhere they occur to you, whether you want to understand a complex concept, find hidden connections, create new possibilities, or solve problems that have been puzzling you.From Answers To ActionComet transforms entire browsing sessions into single, seamless interactions, collapsing complex workflows into fluid conversations.Ask Comet to book a meeting or send an email, based on something you saw. Ask Comet to buy something you forgot. Ask Comet to brief you for your day.\\xa0With Comet, you don't search for information—you think out loud, and Comet executes complete workflows while keeping perfect context. Research becomes conversation. Analysis becomes natural. Annoying tasks evaporate. The internet becomes an extension of your mind.\\xa0From Believability To AccuracyIn the DNA of Comet is Perplexity’s obsession with accurate and trustworthy answers. Answers are the foundation of curiosity for one reason: more knowledge gives us better questions.\\xa0Every day, trillions of dollars of decisions are made online, and the quality of those decisions depends on the reliability of the information behind them. Ask Comet to compare insurance plans. Ask Comet to help you understand a technology enough to decide whether to invest.\\xa0\\xa0Accurate answers are the foundation of decision-making. This will compound in importance with agentic AI, when assistants make decisions for us, faster and more often. Comet is like a second brain, helping with the best possible decisions in every situation.\\xa0From Consumption To CuriosityPerplexity’s mission is to serve the world’s curiosity.\\xa0Comet transforms any webpage into a portal of curiosity. Highlight any text to get instant explanations. Explore tangential ideas without losing your original context. Ask specialized questions or broad ones—Comet understands that genuine curiosity doesn't follow predetermined paths.Ask Comet what you’re missing. Ask Comet for counterpoints. Ask Comet where you should explore.Curiosity is personal. This allows your own curiosity to become the context for a reliable, proactive, and personalized assistant.\\xa0Comet learns how you think, in order to think better with you.\\xa0From Here To What's NextBeginning today, Comet is available to Perplexity Max subscribers.\\xa0Invite-only access will roll out slowly to our waitlist over the summer. New users will also receive a limited number of invites to share.In the meantime, you can join the waitlist here.It’ll be worth the wait, because Comet is just getting started.With our own roadmap, and with every new advancement in AI, we will continue to launch new features and functionality for Comet, improve experiences based on your feedback, and focus relentlessly–as we always have–on building accurate and trustworthy AI that fuels human curiosity. The future belongs to the people who never stop asking questions.\\xa0We're curious to see what you'll ask of Comet.Today we are launching Comet.\\xa0Comet is a web browser built for today’s internet. In the last 30 years, the internet has evolved from something we simply “browse” or “search.” The internet is where we live, work, and connect.\\xa0It’s also where we ask questions.\\xa0Curious minds have questions everywhere, and they find answers on every page, in every idea, through every task. Yet we've been trapped in long lines of tabs and hyperlinks, disjointed experiences that interrupt our natural flow of thought.In other words, the internet has become humanity's extended mind while our tools for using it remain primitive. Our interface for the web should be as fluid and responsive as human thought itself.\\xa0We built Comet to let the internet do what it has been begging to do: to amplify our intelligence.From Navigation To CognitionComet powers a shift from browsing to thinking.\\xa0Tabs that piled up waiting for your return now join one intelligent interface that understands how your mind works.\\xa0 Context-switching between dozens of applications, sites, and interfaces has stolen the focus and flow that bring joy to our work and fuel our curiosity.\\xa0The Comet assistant removes friction with every thought, actively conducting entire browsing sessions while you focus on what matters. Ask Comet which other sites have the same bike but ship it faster.\\xa0 Ask Comet to compare what you’re reading to something you already read.Comet allows you to ask questions anywhere they occur to you, whether you want to understand a complex concept, find hidden connections, create new possibilities, or solve problems that have been puzzling you.From Answers To ActionComet transforms entire browsing sessions into single, seamless interactions, collapsing complex workflows into fluid conversations.Ask Comet to book a meeting or send an email, based on something you saw. Ask Comet to buy something you forgot. Ask Comet to brief you for your day.\\xa0With Comet, you don't search for information—you think out loud, and Comet executes complete workflows while keeping perfect context. Research becomes conversation. Analysis becomes natural. Annoying tasks evaporate. The internet becomes an extension of your mind.\\xa0From Believability To AccuracyIn the DNA of Comet is Perplexity’s obsession with accurate and trustworthy answers. Answers are the foundation of curiosity for one reason: more knowledge gives us better questions.\\xa0Every day, trillions of dollars of decisions are made online, and the quality of those decisions depends on the reliability of the information behind them. Ask Comet to compare insurance plans. Ask Comet to help you understand a technology enough to decide whether to invest.\\xa0\\xa0Accurate answers are the foundation of decision-making. This will compound in importance with agentic AI, when assistants make decisions for us, faster and more often. Comet is like a second brain, helping with the best possible decisions in every situation.\\xa0From Consumption To CuriosityPerplexity’s mission is to serve the world’s curiosity.\\xa0Comet transforms any webpage into a portal of curiosity. Highlight any text to get instant explanations. Explore tangential ideas without losing your original context. Ask specialized questions or broad ones—Comet understands that genuine curiosity doesn't follow predetermined paths.Ask Comet what you’re missing. Ask Comet for counterpoints. Ask Comet where you should explore.Curiosity is personal. This allows your own curiosity to become the context for a reliable, proactive, and personalized assistant.\\xa0Comet learns how you think, in order to think better with you.\\xa0From Here To What's NextBeginning today, Comet is available to Perplexity Max subscribers.\\xa0Invite-only access will roll out slowly to our waitlist over the summer. New users will also receive a limited number of invites to share.In the meantime, you can join the waitlist here.It’ll be worth the wait, because Comet is just getting started.With our own roadmap, and with every new advancement in AI, we will continue to launch new features and functionality for Comet, improve experiences based on your feedback, and focus relentlessly–as we always have–on building accurate and trustworthy AI that fuels human curiosity. The future belongs to the people who never stop asking questions.\\xa0We're curious to see what you'll ask of Comet.\"),\n",
       " Document(metadata={'source': 'https://www.perplexity.ai/hub/blog/introducing-perplexity-max'}, page_content='Introducing Perplexity Max\\xa0Perplexity’s most powerful subscription tier yetThe ultimate Perplexity subscription is here: Perplexity Max.\\xa0Perplexity Max is our most advanced subscription tier yet, built for those who demand limitless AI productivity and immediate access to the newest products and features from Perplexity.\\xa0 With Perplexity Max, you can reach the maximum power of your curiosity.What’s in Perplexity Max?\\xa0Unlimited LabsLabs are a powerful way to bring any idea to life. Since we launched Labs, Perplexity Pro subscribers have been using the advanced orchestration tools in Labs to create dashboards, spreadsheets, presentations, web applications, and more. Today we are excited to announce Perplexity Max provides unlimited Labs usage per month, to accelerate the value you can create.Early AccessPerplexity Max is the fastest way to get new Perplexity features and products first. For instance, Max subscribers will be the first to have access to Comet. Comet is our new browser built from the ground up to be a powerful thought partner for everything you do on the web. We’ll also be launching even more premium data sources and benefits from leading brands exclusive to Max subscribers.\\xa0We are constantly launching new products and features at Perplexity, and we’re excited to share them first with AI users who demand the newest and best.Advanced Model Options and Priority SupportPerplexity Max includes access to top tier advanced AI models like OpenAI o3-pro and Claude Opus 4, with new frontier models added as they evolve. Max users who demand the top AI models for their projects also receive priority support as part of the Max subscription.Who is Max For?Perplexity Max is designed for:Professionals who need unlimited access to comprehensive analysis toolsContent creators and writers who require extensive research capabilitiesBusiness strategists conducting competitive intelligence and market researchAcademic researchers working on complex, multi-faceted projectsPerplexity Pro ($20/month) remains available with generous usage limits for most users. Enterprise Pro continues to be the best plan for organizations with team management, security features, and internal knowledge base integrations. We will be launching an Enterprise version of Max with unlimited Labs queries in the near future.Available NowPerplexity Max is available on web and iOS starting today. New and existing users can upgrade their subscription in settings.Introducing Perplexity Max\\xa0Perplexity’s most powerful subscription tier yetThe ultimate Perplexity subscription is here: Perplexity Max.\\xa0Perplexity Max is our most advanced subscription tier yet, built for those who demand limitless AI productivity and immediate access to the newest products and features from Perplexity.\\xa0 With Perplexity Max, you can reach the maximum power of your curiosity.What’s in Perplexity Max?\\xa0Unlimited LabsLabs are a powerful way to bring any idea to life. Since we launched Labs, Perplexity Pro subscribers have been using the advanced orchestration tools in Labs to create dashboards, spreadsheets, presentations, web applications, and more. Today we are excited to announce Perplexity Max provides unlimited Labs usage per month, to accelerate the value you can create.Early AccessPerplexity Max is the fastest way to get new Perplexity features and products first. For instance, Max subscribers will be the first to have access to Comet. Comet is our new browser built from the ground up to be a powerful thought partner for everything you do on the web. We’ll also be launching even more premium data sources and benefits from leading brands exclusive to Max subscribers.\\xa0We are constantly launching new products and features at Perplexity, and we’re excited to share them first with AI users who demand the newest and best.Advanced Model Options and Priority SupportPerplexity Max includes access to top tier advanced AI models like OpenAI o3-pro and Claude Opus 4, with new frontier models added as they evolve. Max users who demand the top AI models for their projects also receive priority support as part of the Max subscription.Who is Max For?Perplexity Max is designed for:Professionals who need unlimited access to comprehensive analysis toolsContent creators and writers who require extensive research capabilitiesBusiness strategists conducting competitive intelligence and market researchAcademic researchers working on complex, multi-faceted projectsPerplexity Pro ($20/month) remains available with generous usage limits for most users. Enterprise Pro continues to be the best plan for organizations with team management, security features, and internal knowledge base integrations. We will be launching an Enterprise version of Max with unlimited Labs queries in the near future.Available NowPerplexity Max is available on web and iOS starting today. New and existing users can upgrade their subscription in settings.Introducing Perplexity Max\\xa0Perplexity’s most powerful subscription tier yetThe ultimate Perplexity subscription is here: Perplexity Max.\\xa0Perplexity Max is our most advanced subscription tier yet, built for those who demand limitless AI productivity and immediate access to the newest products and features from Perplexity.\\xa0 With Perplexity Max, you can reach the maximum power of your curiosity.What’s in Perplexity Max?\\xa0Unlimited LabsLabs are a powerful way to bring any idea to life. Since we launched Labs, Perplexity Pro subscribers have been using the advanced orchestration tools in Labs to create dashboards, spreadsheets, presentations, web applications, and more. Today we are excited to announce Perplexity Max provides unlimited Labs usage per month, to accelerate the value you can create.Early AccessPerplexity Max is the fastest way to get new Perplexity features and products first. For instance, Max subscribers will be the first to have access to Comet. Comet is our new browser built from the ground up to be a powerful thought partner for everything you do on the web. We’ll also be launching even more premium data sources and benefits from leading brands exclusive to Max subscribers.\\xa0We are constantly launching new products and features at Perplexity, and we’re excited to share them first with AI users who demand the newest and best.Advanced Model Options and Priority SupportPerplexity Max includes access to top tier advanced AI models like OpenAI o3-pro and Claude Opus 4, with new frontier models added as they evolve. Max users who demand the top AI models for their projects also receive priority support as part of the Max subscription.Who is Max For?Perplexity Max is designed for:Professionals who need unlimited access to comprehensive analysis toolsContent creators and writers who require extensive research capabilitiesBusiness strategists conducting competitive intelligence and market researchAcademic researchers working on complex, multi-faceted projectsPerplexity Pro ($20/month) remains available with generous usage limits for most users. Enterprise Pro continues to be the best plan for organizations with team management, security features, and internal knowledge base integrations. We will be launching an Enterprise version of Max with unlimited Labs queries in the near future.Available NowPerplexity Max is available on web and iOS starting today. New and existing users can upgrade their subscription in settings.'),\n",
       " Document(metadata={'source': 'https://www.perplexity.ai/hub/blog/bringing-european-ai-models-to-global-audiences'}, page_content=\"Bringing European AI Models to Global AudiencesToday we're proud to announce that Perplexity will integrate sovereign European AI models into our answer engine, bringing locally-optimized, culturally-aware AI to users worldwide while supporting Europe's digital sovereignty initiatives.This integration is more than just a technical advancement—it's a step toward ensuring that AI reflects the rich linguistic diversity and cultural nuances that make our global community stronger. As part of NVIDIA's groundbreaking work with European model builders and cloud providers, these sovereign models will soon be available through Perplexity, optimized with NVIDIA Nemotron techniques and deployed as NVIDIA NIM microservices.Why European Sovereign AI MattersEurope's diversity has always been its superpower, fostering creativity and innovation across 24 official languages and countless cultural contexts. The sovereign AI models we're integrating reflect this diversity, with specialized models from leading European institutions across France, Germany, Italy, Poland, Spain, Sweden, and more.These models aren't just translations—they're purpose-built to understand the nuances of European languages, cultural contexts, and local knowledge that global models often miss. Whether you're asking about French literature, German engineering, Italian art, or Nordic sustainability, these models provide insights grounded in authentic European perspectives.Perplexity's Role in Europe's AI FutureAt Perplexity, we've always believed that accurate, trustworthy information should be accessible to everyone. By integrating these sovereign AI models, we're ensuring European enterprises, publishers, and organizations have access to AI that truly understands their context and needs.Technical Excellence Through PartnershipThe power behind these sovereign models comes from NVIDIA Nemotron post-training techniques, including neural architecture search, reinforcement learning, and optimization with NVIDIA-curated synthetic data. These optimizations boost performance and reduce costs by generating tokens faster during inference.The models will run on European AI infrastructure from NVIDIA Cloud Partners, ensuring data stays in Europe while maintaining high performance. Through NVIDIA NIM microservices, developers can deploy these models across various platforms, from data centers to on-premises installations.A Global Platform for Local IntelligenceWhat excites us most about our partnership with NVIDIA is how it embodies our core belief: that the best AI systems should be both globally accessible and locally relevant. By bringing European sovereign AI to Perplexity, we're enriching the knowledge available to our global community.This integration supports Europe's vision of digital sovereignty while opening the impact of these models globally. When users worldwide ask questions about European history, culture, business, or technology, they'll receive answers informed by AI models that truly understand these domains.Looking AheadThe first European sovereign AI models are expected to be available through Perplexity later this year. This is just the beginning of a new era in AI—one where global platforms can seamlessly integrate locally-optimized intelligence to serve users better.As we expand our capabilities and partnerships, we remain committed to our core promise: providing accurate, trustworthy answers to any question, for anyone, anywhere. The integration of European sovereign AI models brings us one step closer to that vision.Bringing European AI Models to Global AudiencesToday we're proud to announce that Perplexity will integrate sovereign European AI models into our answer engine, bringing locally-optimized, culturally-aware AI to users worldwide while supporting Europe's digital sovereignty initiatives.This integration is more than just a technical advancement—it's a step toward ensuring that AI reflects the rich linguistic diversity and cultural nuances that make our global community stronger. As part of NVIDIA's groundbreaking work with European model builders and cloud providers, these sovereign models will soon be available through Perplexity, optimized with NVIDIA Nemotron techniques and deployed as NVIDIA NIM microservices.Why European Sovereign AI MattersEurope's diversity has always been its superpower, fostering creativity and innovation across 24 official languages and countless cultural contexts. The sovereign AI models we're integrating reflect this diversity, with specialized models from leading European institutions across France, Germany, Italy, Poland, Spain, Sweden, and more.These models aren't just translations—they're purpose-built to understand the nuances of European languages, cultural contexts, and local knowledge that global models often miss. Whether you're asking about French literature, German engineering, Italian art, or Nordic sustainability, these models provide insights grounded in authentic European perspectives.Perplexity's Role in Europe's AI FutureAt Perplexity, we've always believed that accurate, trustworthy information should be accessible to everyone. By integrating these sovereign AI models, we're ensuring European enterprises, publishers, and organizations have access to AI that truly understands their context and needs.Technical Excellence Through PartnershipThe power behind these sovereign models comes from NVIDIA Nemotron post-training techniques, including neural architecture search, reinforcement learning, and optimization with NVIDIA-curated synthetic data. These optimizations boost performance and reduce costs by generating tokens faster during inference.The models will run on European AI infrastructure from NVIDIA Cloud Partners, ensuring data stays in Europe while maintaining high performance. Through NVIDIA NIM microservices, developers can deploy these models across various platforms, from data centers to on-premises installations.A Global Platform for Local IntelligenceWhat excites us most about our partnership with NVIDIA is how it embodies our core belief: that the best AI systems should be both globally accessible and locally relevant. By bringing European sovereign AI to Perplexity, we're enriching the knowledge available to our global community.This integration supports Europe's vision of digital sovereignty while opening the impact of these models globally. When users worldwide ask questions about European history, culture, business, or technology, they'll receive answers informed by AI models that truly understand these domains.Looking AheadThe first European sovereign AI models are expected to be available through Perplexity later this year. This is just the beginning of a new era in AI—one where global platforms can seamlessly integrate locally-optimized intelligence to serve users better.As we expand our capabilities and partnerships, we remain committed to our core promise: providing accurate, trustworthy answers to any question, for anyone, anywhere. The integration of European sovereign AI models brings us one step closer to that vision.Bringing European AI Models to Global AudiencesToday we're proud to announce that Perplexity will integrate sovereign European AI models into our answer engine, bringing locally-optimized, culturally-aware AI to users worldwide while supporting Europe's digital sovereignty initiatives.This integration is more than just a technical advancement—it's a step toward ensuring that AI reflects the rich linguistic diversity and cultural nuances that make our global community stronger. As part of NVIDIA's groundbreaking work with European model builders and cloud providers, these sovereign models will soon be available through Perplexity, optimized with NVIDIA Nemotron techniques and deployed as NVIDIA NIM microservices.Why European Sovereign AI MattersEurope's diversity has always been its superpower, fostering creativity and innovation across 24 official languages and countless cultural contexts. The sovereign AI models we're integrating reflect this diversity, with specialized models from leading European institutions across France, Germany, Italy, Poland, Spain, Sweden, and more.These models aren't just translations—they're purpose-built to understand the nuances of European languages, cultural contexts, and local knowledge that global models often miss. Whether you're asking about French literature, German engineering, Italian art, or Nordic sustainability, these models provide insights grounded in authentic European perspectives.Perplexity's Role in Europe's AI FutureAt Perplexity, we've always believed that accurate, trustworthy information should be accessible to everyone. By integrating these sovereign AI models, we're ensuring European enterprises, publishers, and organizations have access to AI that truly understands their context and needs.Technical Excellence Through PartnershipThe power behind these sovereign models comes from NVIDIA Nemotron post-training techniques, including neural architecture search, reinforcement learning, and optimization with NVIDIA-curated synthetic data. These optimizations boost performance and reduce costs by generating tokens faster during inference.The models will run on European AI infrastructure from NVIDIA Cloud Partners, ensuring data stays in Europe while maintaining high performance. Through NVIDIA NIM microservices, developers can deploy these models across various platforms, from data centers to on-premises installations.A Global Platform for Local IntelligenceWhat excites us most about our partnership with NVIDIA is how it embodies our core belief: that the best AI systems should be both globally accessible and locally relevant. By bringing European sovereign AI to Perplexity, we're enriching the knowledge available to our global community.This integration supports Europe's vision of digital sovereignty while opening the impact of these models globally. When users worldwide ask questions about European history, culture, business, or technology, they'll receive answers informed by AI models that truly understand these domains.Looking AheadThe first European sovereign AI models are expected to be available through Perplexity later this year. This is just the beginning of a new era in AI—one where global platforms can seamlessly integrate locally-optimized intelligence to serve users better.As we expand our capabilities and partnerships, we remain committed to our core promise: providing accurate, trustworthy answers to any question, for anyone, anywhere. The integration of European sovereign AI models brings us one step closer to that vision.\"),\n",
       " Document(metadata={'source': 'https://www.perplexity.ai/hub/blog/accelerating-sonar-through-speculation'}, page_content='Accelerating Sonar Through SpeculationSpeculative decoding speeds up the generation speed of Large Language Models (LLMs) by using a quick and small draft model to produce completion candidates that are verified by the larger target model. Under this scheme, instead of a run of the expensive target producing a single token, multiple are emitted in a single step. Here we present the implementation details of various kinds of speculative decoding, applied at Perplexity to reduce inter-token latency on Sonar models.Speculative DecodingSpeculative Decoding leverages the structure of natural languages and the auto-regressive nature of transformers to speed up token generation. Even though larger models, such as Llama-70B, carry more knowledge than smaller ones, such as Llama-1B, on some simpler tasks they perform similarly. This overlap does suggest that certain sequences are better generated by the less expensive models, leaving complex problems to larger ones. The challenge lies in determining which completions are better and whether the generation of the smaller model is of the same quality as that of the larger one.Fortunately, LLMs are auto-regressive transformers: when given a sequence of tokens, they output the probability distribution of the next token. Additionally, the logits derived from the intermediate features associated with the tokens in the input sequence also indicate how likely it is for the model to issue those exact tokens. This property enables speculation: if a sequence of tokens is generated by a smaller one starting from an input prefix, it can be run through the larger one to determine how well it lines up with the target model. Each prefix of the candidates is scored with a probability and the longest one above an acceptance threshold is picked. As a bonus, the target model also provides a subsequent token for free: if a draft model generates n tokens, up to n + 1 can be emitted in one step.At inference time, speculative sampling process can be split into roughly 4 stages:Prefill: both the target and the draft models must be run on the input sequence to populate the KV cache entries. While some schemes, such as Medusa, use simpler dense layers for prediction, in this post we focus on transformer-based drafts that need their own KV caches.Draft generation: the draft model iterates to produce a number of fixed tokens. The draft sequence can be linear or the model can explore a tree-like structure up to a given depth (EAGLE, Medusa). Here, we focus on linear sequences.Acceptance: the target model runs on the draft sequence, building logits corresponding to each draft token. The length of the longest acceptable sequence is determined.Target generation: since the target generated logits, at the mismatched position or the tail end of the sequence the logits correspond to a subsequent token. These logits can be sampled to provide a robust token from the target, capping off the sequence.Various methods exist to implement speculative decoding. In this post, we will focus on the schemes we used to accelerate Sonar models using an in-house 1B model, as well as the prediction mechanisms we are building out to speed up models at the scale of DeepSeek.Target-DraftSpeculative decoding can be achieved by coupling an existing small LLM as a draft model to a target model to generate candidate sequences. In production, we have accelerated Sonar using a Llama-1B model fine-tuned on the same dataset as the target. While this approach did not require training a draft from scratch, the small model still uses significant KV cache capacity and introduces a slight prefill overhead, increasing TTFT.Under this scheme, the decoder only speculates on decode-only batches, generating tokens through standard sampling during prefill or on mixed prefill-decode batches. In the prefill stage, the target logits are immediately sampled to also prefill the newly-generated token in the KV cache of the draft. The draft is not sampled yet, but the logits it produces are carried over to the decode stage.In decode, the draft model is advanced, sampling the top token at each stage. After the desired draft length is reached, the tokens are run through the target model to produce the logits based on which the sampler identifies the accepted sequence length. Acceptance is determined by comparing the full probability distributions from the draft and the target. Since the target always outputs one set of logits following the accepted draft sequence, that is sampled to produce an additional output. Since the draft model has not yet seen that accepted token, it is re-run to populate its corresponding KV cache entries in preparation for the next decode step, carrying the logits over again.EAGLEEAGLE is speculative decoding scheme which explores multiple draft sequences, generated through a tree-like traversal of probable draft tokens. A fixed (EAGLE) or dynamically-shaped (EAGLE-2) tree is explored using consecutive executions of the draft tokens, considering the Top-K candidates at each node instead of following the highest scoring token in a linear sequence. The sequences are then scored and the longest suitable one is selected to continue, also appending an additional token from the target.In order to achieve more accurate prediction, an EAGLE draft model predicts not only based on tokens, but also using the target features (last layer hidden states) of the target model. The disadvantage of EAGLE is the need to train custom, small draft models which are accurate enough to generate suitable candidates within a low latency budget. Typically, a draft model is a single transformer layer identical to a decoder layer of the original model, which is tightly coupled to the target by tying to its embeddings and lm_head projections. Since this requires less KV cache capacity, EAGLE has a lower memory footprint.To verify tree-like sequences in the target model, custom attention masks must be used. Unfortunately, using a custom attention mask for a whole sequence significantly slows down attention for realistic input lengths (by up to 50%), nullifying some of the speedup achievable through speculation. We have not yet deployed full tree exploration to production for this reason, focusing instead on the special case of single-token prediction via MTP-like schemes presented in the DeepSeek-V3 Technical Report.MTPThis scheme is similar to draft-target decoding, with the exception of hidden states being used alongside tokens for prediction. Slightly more work must be done in both the prefill and decode stages compared to regular draft-target speculation. The draft model uses both tokens and hidden states: token t_{i+1} is sampled from the logits L_i corresponding to token t_i, which in turn are derived from the hidden states H_i. Consequently, the input token buffers must be shifted one step to the left relative to the hidden state vectors output by the target. The figure below marks the correspondences used for training, as well as the shift during inference.The decoding flow is quite similar to draft-target decoding, with the exception of both hidden states and logits being carried over. Our implementation shares all the associated sampling and logit processing logits, specializing only the model forward invocations. When multiple tokens are predicted, the draft model uses draft hidden states for prediction, also populating KV cache entries based on its own features. In the long run, this can degrade accuracy. Subsequently, when running the draft model to populate the KV cache entry for the target prediction, we run it on the whole sequence taking the more accurate target hidden states as inputs. Since these draft models are small, the added cost of processing the additional tokens is negligible.Training MTP HeadsIn order to benefit from MTP, we built the infrastructure required to train MTP heads attached to our fine-tuned models on Perplexity’s datasets, running on one node with 8xH100 devices. In about one day, we can build heads for models ranging from Llama-1B to Llama-70B and DeepSeek V2-Lite. For larger models, we rely on MTP heads built during the fine-tuning process.The target of MTP training is to match up the draft hidden states and logits extrapolated from the target hidden states to the next token logits and hidden states of the target. Since inference for hidden states is expensive, we pre-compute them using our inference-optimized implementation of the target model, to be used during training. However, to validate the inference MTP implementation and ensure that numerical differences due to quantization or optimizations do not hinder results, for validation loss and accuracy estimation we fully re-use the inference implementation of both the target and the draft models.When scaling from the ShareGPT dataset used in the original paper to larger samples, we noticed that the MTP head architecture outlined and implemented in the EAGLE paper failed to train for 70B-sized models. Unlike ShareGPT which contained a larger number of shorter sequences, we train on a slightly smaller number of substantially longer prompts. Since the original EAGLE heads slightly diverged in structure from a typical transformer, we re-introduced some RMS Normalization layers that were stripped. We found that this not only allowed training to converge, but it also boosted the accuracy of the heads by a few percentage points.Not only do layer norms facilitate training, re-introducing the norms is also mathematically intuitive. MTP heads re-use the embeddings and the logit projections of the target model, as they can be substantial in size (about 2 GB for Llama 70B). During training, these are frozen and the expectation is that the MTP layer learns to embed predictions into the same vector space as what the projection layer of the original model learnt during training. By dropping the norms, a single MLP is expected to learn the same function as an MLP followed by a norm, which hinders the matchup between the hidden states of the draft and the target models.Inference with Speculative DecodingIn the inference engine, in order to generate tokens for input sequences, they need to be first grouped into reasonably-sized batches, then pages must be allocated in the KV cache for the next tokens. The input tokens and the KV page information is then packed into a buffer broadcast to all parallel ranks running the model. Finally, the metadata is copied into GPU memory and the model is executed to produce the logits from which the next token is sampled.Unlike certain implementations which loosely couple a draft and target inference server via a wrapper that orchestrates requests between them, our draft-target pairs are tightly coupled and step through generation in unison. Batch scheduling and KV page allocation is shared between the models for all forms of speculative decoding: this unifies the logic that bridges a model with the overarching inference server, as they all expose the same interface.The inference runtime at Perplexity is shaped around FlashInfer, which determines the metadata that needs to be built in order to configure and schedule the attention kernel. Given some input sequences forming a batch, for prefill, decode or verification, CPU-side work must be done to allocate intermediate buffers and populate certain constant buffers used in attention. This work is in addition to the cost of batch scheduling and KV page allocation, which also incur latencies that must be hidden in order to maximize GPU utilization.While we fully parallelized CPU-side and GPU-side work for inference without speculation, we found that the CPU-GPU balance for speculative decoding is more intricate. The main challenge arises from the fact that the number of accepted tokens determines the sequence length for a subsequent run, introducing a difficult-to-avoid GPU-to-CPU synchronization point. We experimented with different scheduling schemes in order to best hide the latency of CPU work.Draft-Target ScheduleDespite being smaller than a target model, when an entire LLM is used as the draft, it still introduces considerable latency on the GPU, providing some headroom to hide expensive CPU operations. Since smaller models do not benefit from tensor parallelism, there is a mismatch between the number of ranks a target and a draft are sharded across. In our implementation, the draft model runs only on the leader rank of a TP group.As indicated before, a decode step carries over logits into the next run. This allows us to overlap one execution of the draft model with the CPU-side batch scheduling work. After the batch is put together, repeated calls to the sampler and the draft produce the draft tokens. In parallel, the batch for verification is put together for the target model and synchronized with the parallel workers. The target logits are verified and sampled to determine the accepted sequence lengths. At this point, GPU-to-CPU synchronization is necessary in order to determine subsequent sequence lengths. Since the draft model is only run on the leader node, its batch is set up sequentially and its execution is kicked off to populate its KV cache entries with the additional token that the target produced. The logits produced by this draft run in the current run will be used to sample the first draft token in the subsequent run. Most importantly, while the draft is running, the next batch can be scheduled.MTP Schedule for a Single TokenWhile the runtime does not yet provide Eagle-style draft tree exploration, we implemented a special case of this scheme, considering a linear sequence of draft tokens produced by a model the size of a single transformer decoder layer. This scheme can be used for draft prediction using the open-source weights of DeepSeek R1. The sub-case of predicting a single token is interesting, as large MTP layers achieve sufficiently high acceptance rates to justify their overhead.MTP scheduling is somewhat more complex, as the draft model is much faster, hiding less CPU-side latency. Additionally, the draft is sharded alongside the target model, requiring shared memory transfers for batch information. A run starts by transferring batch info and sampling the first token from carry-over logits, similarly to the previous scheme. Next, the target is run to validate tokens, processing 2 * D tokens, where D is the decode batch size. This is ideal for micro-batching in Mixture-of-Experts (MoE) models over slower interconnects such as InfiniBand, as the batch splits evenly into two halves. The hidden states of the target carry over to the next draft run, while the logits are passed into the sampler for verification.By performing a limited amount of additional work on the GPU, we avoid CPU-to-GPU synchronization after draft sequence acceptance. After the input tokens of the targets are shifted, a kernel plugs in the next target tokens into their corresponding locations. The draft is then re-run with the same batch information as the target, populating KV cache entries and building the logits and hidden states for the next run, doing some redundant work on tokens which were not accepted. In these situations, the latency of the unused work is barely measurable due to the small size of the draft model. In parallel with the draft run, sequence lengths are determined on the CPU and the scheduling of the next batch is kicked off, without having to wait for GPU work to terminate.The overhead of additional work in the draft layer is not noticeable in attention, however MLP layers are more problematic. Since matrix multiplication instructions pad to a boundary of 64 along the dimension of number of tokens, if doubling doesn’t require significantly more blocks, the overhead is hidden. For longer draft sequences the overhead is more expensive and the scheme used for regular draft-target models works better.ReferencesFast Inference from Transformers via Speculative DecodingEAGLE: Speculative Sampling Requires Rethinking Feature UncertaintyEAGLE-2: Faster Inference of Language Models with Dynamic Draft TreesEAGLE-3: Scaling up Inference Acceleration of Large Language Models via Training-Time TestMedusa: Simple LLM Inference Acceleration Framework with Multiple Decoding HeadsLayer NormalizationFlashInfer: Efficient and Customizable Attention Engine for LLM Inference ServingDeepSeek-V3 Technical ReportAccelerating Sonar Through SpeculationSpeculative decoding speeds up the generation speed of Large Language Models (LLMs) by using a quick and small draft model to produce completion candidates that are verified by the larger target model. Under this scheme, instead of a run of the expensive target producing a single token, multiple are emitted in a single step. Here we present the implementation details of various kinds of speculative decoding, applied at Perplexity to reduce inter-token latency on Sonar models.Speculative DecodingSpeculative Decoding leverages the structure of natural languages and the auto-regressive nature of transformers to speed up token generation. Even though larger models, such as Llama-70B, carry more knowledge than smaller ones, such as Llama-1B, on some simpler tasks they perform similarly. This overlap does suggest that certain sequences are better generated by the less expensive models, leaving complex problems to larger ones. The challenge lies in determining which completions are better and whether the generation of the smaller model is of the same quality as that of the larger one.Fortunately, LLMs are auto-regressive transformers: when given a sequence of tokens, they output the probability distribution of the next token. Additionally, the logits derived from the intermediate features associated with the tokens in the input sequence also indicate how likely it is for the model to issue those exact tokens. This property enables speculation: if a sequence of tokens is generated by a smaller one starting from an input prefix, it can be run through the larger one to determine how well it lines up with the target model. Each prefix of the candidates is scored with a probability and the longest one above an acceptance threshold is picked. As a bonus, the target model also provides a subsequent token for free: if a draft model generates n tokens, up to n + 1 can be emitted in one step.At inference time, speculative sampling process can be split into roughly 4 stages:Prefill: both the target and the draft models must be run on the input sequence to populate the KV cache entries. While some schemes, such as Medusa, use simpler dense layers for prediction, in this post we focus on transformer-based drafts that need their own KV caches.Draft generation: the draft model iterates to produce a number of fixed tokens. The draft sequence can be linear or the model can explore a tree-like structure up to a given depth (EAGLE, Medusa). Here, we focus on linear sequences.Acceptance: the target model runs on the draft sequence, building logits corresponding to each draft token. The length of the longest acceptable sequence is determined.Target generation: since the target generated logits, at the mismatched position or the tail end of the sequence the logits correspond to a subsequent token. These logits can be sampled to provide a robust token from the target, capping off the sequence.Various methods exist to implement speculative decoding. In this post, we will focus on the schemes we used to accelerate Sonar models using an in-house 1B model, as well as the prediction mechanisms we are building out to speed up models at the scale of DeepSeek.Target-DraftSpeculative decoding can be achieved by coupling an existing small LLM as a draft model to a target model to generate candidate sequences. In production, we have accelerated Sonar using a Llama-1B model fine-tuned on the same dataset as the target. While this approach did not require training a draft from scratch, the small model still uses significant KV cache capacity and introduces a slight prefill overhead, increasing TTFT.Under this scheme, the decoder only speculates on decode-only batches, generating tokens through standard sampling during prefill or on mixed prefill-decode batches. In the prefill stage, the target logits are immediately sampled to also prefill the newly-generated token in the KV cache of the draft. The draft is not sampled yet, but the logits it produces are carried over to the decode stage.In decode, the draft model is advanced, sampling the top token at each stage. After the desired draft length is reached, the tokens are run through the target model to produce the logits based on which the sampler identifies the accepted sequence length. Acceptance is determined by comparing the full probability distributions from the draft and the target. Since the target always outputs one set of logits following the accepted draft sequence, that is sampled to produce an additional output. Since the draft model has not yet seen that accepted token, it is re-run to populate its corresponding KV cache entries in preparation for the next decode step, carrying the logits over again.EAGLEEAGLE is speculative decoding scheme which explores multiple draft sequences, generated through a tree-like traversal of probable draft tokens. A fixed (EAGLE) or dynamically-shaped (EAGLE-2) tree is explored using consecutive executions of the draft tokens, considering the Top-K candidates at each node instead of following the highest scoring token in a linear sequence. The sequences are then scored and the longest suitable one is selected to continue, also appending an additional token from the target.In order to achieve more accurate prediction, an EAGLE draft model predicts not only based on tokens, but also using the target features (last layer hidden states) of the target model. The disadvantage of EAGLE is the need to train custom, small draft models which are accurate enough to generate suitable candidates within a low latency budget. Typically, a draft model is a single transformer layer identical to a decoder layer of the original model, which is tightly coupled to the target by tying to its embeddings and lm_head projections. Since this requires less KV cache capacity, EAGLE has a lower memory footprint.To verify tree-like sequences in the target model, custom attention masks must be used. Unfortunately, using a custom attention mask for a whole sequence significantly slows down attention for realistic input lengths (by up to 50%), nullifying some of the speedup achievable through speculation. We have not yet deployed full tree exploration to production for this reason, focusing instead on the special case of single-token prediction via MTP-like schemes presented in the DeepSeek-V3 Technical Report.MTPThis scheme is similar to draft-target decoding, with the exception of hidden states being used alongside tokens for prediction. Slightly more work must be done in both the prefill and decode stages compared to regular draft-target speculation. The draft model uses both tokens and hidden states: token t_{i+1} is sampled from the logits L_i corresponding to token t_i, which in turn are derived from the hidden states H_i. Consequently, the input token buffers must be shifted one step to the left relative to the hidden state vectors output by the target. The figure below marks the correspondences used for training, as well as the shift during inference.The decoding flow is quite similar to draft-target decoding, with the exception of both hidden states and logits being carried over. Our implementation shares all the associated sampling and logit processing logits, specializing only the model forward invocations. When multiple tokens are predicted, the draft model uses draft hidden states for prediction, also populating KV cache entries based on its own features. In the long run, this can degrade accuracy. Subsequently, when running the draft model to populate the KV cache entry for the target prediction, we run it on the whole sequence taking the more accurate target hidden states as inputs. Since these draft models are small, the added cost of processing the additional tokens is negligible.Training MTP HeadsIn order to benefit from MTP, we built the infrastructure required to train MTP heads attached to our fine-tuned models on Perplexity’s datasets, running on one node with 8xH100 devices. In about one day, we can build heads for models ranging from Llama-1B to Llama-70B and DeepSeek V2-Lite. For larger models, we rely on MTP heads built during the fine-tuning process.The target of MTP training is to match up the draft hidden states and logits extrapolated from the target hidden states to the next token logits and hidden states of the target. Since inference for hidden states is expensive, we pre-compute them using our inference-optimized implementation of the target model, to be used during training. However, to validate the inference MTP implementation and ensure that numerical differences due to quantization or optimizations do not hinder results, for validation loss and accuracy estimation we fully re-use the inference implementation of both the target and the draft models.When scaling from the ShareGPT dataset used in the original paper to larger samples, we noticed that the MTP head architecture outlined and implemented in the EAGLE paper failed to train for 70B-sized models. Unlike ShareGPT which contained a larger number of shorter sequences, we train on a slightly smaller number of substantially longer prompts. Since the original EAGLE heads slightly diverged in structure from a typical transformer, we re-introduced some RMS Normalization layers that were stripped. We found that this not only allowed training to converge, but it also boosted the accuracy of the heads by a few percentage points.Not only do layer norms facilitate training, re-introducing the norms is also mathematically intuitive. MTP heads re-use the embeddings and the logit projections of the target model, as they can be substantial in size (about 2 GB for Llama 70B). During training, these are frozen and the expectation is that the MTP layer learns to embed predictions into the same vector space as what the projection layer of the original model learnt during training. By dropping the norms, a single MLP is expected to learn the same function as an MLP followed by a norm, which hinders the matchup between the hidden states of the draft and the target models.Inference with Speculative DecodingIn the inference engine, in order to generate tokens for input sequences, they need to be first grouped into reasonably-sized batches, then pages must be allocated in the KV cache for the next tokens. The input tokens and the KV page information is then packed into a buffer broadcast to all parallel ranks running the model. Finally, the metadata is copied into GPU memory and the model is executed to produce the logits from which the next token is sampled.Unlike certain implementations which loosely couple a draft and target inference server via a wrapper that orchestrates requests between them, our draft-target pairs are tightly coupled and step through generation in unison. Batch scheduling and KV page allocation is shared between the models for all forms of speculative decoding: this unifies the logic that bridges a model with the overarching inference server, as they all expose the same interface.The inference runtime at Perplexity is shaped around FlashInfer, which determines the metadata that needs to be built in order to configure and schedule the attention kernel. Given some input sequences forming a batch, for prefill, decode or verification, CPU-side work must be done to allocate intermediate buffers and populate certain constant buffers used in attention. This work is in addition to the cost of batch scheduling and KV page allocation, which also incur latencies that must be hidden in order to maximize GPU utilization.While we fully parallelized CPU-side and GPU-side work for inference without speculation, we found that the CPU-GPU balance for speculative decoding is more intricate. The main challenge arises from the fact that the number of accepted tokens determines the sequence length for a subsequent run, introducing a difficult-to-avoid GPU-to-CPU synchronization point. We experimented with different scheduling schemes in order to best hide the latency of CPU work.Draft-Target ScheduleDespite being smaller than a target model, when an entire LLM is used as the draft, it still introduces considerable latency on the GPU, providing some headroom to hide expensive CPU operations. Since smaller models do not benefit from tensor parallelism, there is a mismatch between the number of ranks a target and a draft are sharded across. In our implementation, the draft model runs only on the leader rank of a TP group.As indicated before, a decode step carries over logits into the next run. This allows us to overlap one execution of the draft model with the CPU-side batch scheduling work. After the batch is put together, repeated calls to the sampler and the draft produce the draft tokens. In parallel, the batch for verification is put together for the target model and synchronized with the parallel workers. The target logits are verified and sampled to determine the accepted sequence lengths. At this point, GPU-to-CPU synchronization is necessary in order to determine subsequent sequence lengths. Since the draft model is only run on the leader node, its batch is set up sequentially and its execution is kicked off to populate its KV cache entries with the additional token that the target produced. The logits produced by this draft run in the current run will be used to sample the first draft token in the subsequent run. Most importantly, while the draft is running, the next batch can be scheduled.MTP Schedule for a Single TokenWhile the runtime does not yet provide Eagle-style draft tree exploration, we implemented a special case of this scheme, considering a linear sequence of draft tokens produced by a model the size of a single transformer decoder layer. This scheme can be used for draft prediction using the open-source weights of DeepSeek R1. The sub-case of predicting a single token is interesting, as large MTP layers achieve sufficiently high acceptance rates to justify their overhead.MTP scheduling is somewhat more complex, as the draft model is much faster, hiding less CPU-side latency. Additionally, the draft is sharded alongside the target model, requiring shared memory transfers for batch information. A run starts by transferring batch info and sampling the first token from carry-over logits, similarly to the previous scheme. Next, the target is run to validate tokens, processing 2 * D tokens, where D is the decode batch size. This is ideal for micro-batching in Mixture-of-Experts (MoE) models over slower interconnects such as InfiniBand, as the batch splits evenly into two halves. The hidden states of the target carry over to the next draft run, while the logits are passed into the sampler for verification.By performing a limited amount of additional work on the GPU, we avoid CPU-to-GPU synchronization after draft sequence acceptance. After the input tokens of the targets are shifted, a kernel plugs in the next target tokens into their corresponding locations. The draft is then re-run with the same batch information as the target, populating KV cache entries and building the logits and hidden states for the next run, doing some redundant work on tokens which were not accepted. In these situations, the latency of the unused work is barely measurable due to the small size of the draft model. In parallel with the draft run, sequence lengths are determined on the CPU and the scheduling of the next batch is kicked off, without having to wait for GPU work to terminate.The overhead of additional work in the draft layer is not noticeable in attention, however MLP layers are more problematic. Since matrix multiplication instructions pad to a boundary of 64 along the dimension of number of tokens, if doubling doesn’t require significantly more blocks, the overhead is hidden. For longer draft sequences the overhead is more expensive and the scheme used for regular draft-target models works better.ReferencesFast Inference from Transformers via Speculative DecodingEAGLE: Speculative Sampling Requires Rethinking Feature UncertaintyEAGLE-2: Faster Inference of Language Models with Dynamic Draft TreesEAGLE-3: Scaling up Inference Acceleration of Large Language Models via Training-Time TestMedusa: Simple LLM Inference Acceleration Framework with Multiple Decoding HeadsLayer NormalizationFlashInfer: Efficient and Customizable Attention Engine for LLM Inference ServingDeepSeek-V3 Technical ReportAccelerating Sonar Through SpeculationSpeculative decoding speeds up the generation speed of Large Language Models (LLMs) by using a quick and small draft model to produce completion candidates that are verified by the larger target model. Under this scheme, instead of a run of the expensive target producing a single token, multiple are emitted in a single step. Here we present the implementation details of various kinds of speculative decoding, applied at Perplexity to reduce inter-token latency on Sonar models.Speculative DecodingSpeculative Decoding leverages the structure of natural languages and the auto-regressive nature of transformers to speed up token generation. Even though larger models, such as Llama-70B, carry more knowledge than smaller ones, such as Llama-1B, on some simpler tasks they perform similarly. This overlap does suggest that certain sequences are better generated by the less expensive models, leaving complex problems to larger ones. The challenge lies in determining which completions are better and whether the generation of the smaller model is of the same quality as that of the larger one.Fortunately, LLMs are auto-regressive transformers: when given a sequence of tokens, they output the probability distribution of the next token. Additionally, the logits derived from the intermediate features associated with the tokens in the input sequence also indicate how likely it is for the model to issue those exact tokens. This property enables speculation: if a sequence of tokens is generated by a smaller one starting from an input prefix, it can be run through the larger one to determine how well it lines up with the target model. Each prefix of the candidates is scored with a probability and the longest one above an acceptance threshold is picked. As a bonus, the target model also provides a subsequent token for free: if a draft model generates n tokens, up to n + 1 can be emitted in one step.At inference time, speculative sampling process can be split into roughly 4 stages:Prefill: both the target and the draft models must be run on the input sequence to populate the KV cache entries. While some schemes, such as Medusa, use simpler dense layers for prediction, in this post we focus on transformer-based drafts that need their own KV caches.Draft generation: the draft model iterates to produce a number of fixed tokens. The draft sequence can be linear or the model can explore a tree-like structure up to a given depth (EAGLE, Medusa). Here, we focus on linear sequences.Acceptance: the target model runs on the draft sequence, building logits corresponding to each draft token. The length of the longest acceptable sequence is determined.Target generation: since the target generated logits, at the mismatched position or the tail end of the sequence the logits correspond to a subsequent token. These logits can be sampled to provide a robust token from the target, capping off the sequence.Various methods exist to implement speculative decoding. In this post, we will focus on the schemes we used to accelerate Sonar models using an in-house 1B model, as well as the prediction mechanisms we are building out to speed up models at the scale of DeepSeek.Target-DraftSpeculative decoding can be achieved by coupling an existing small LLM as a draft model to a target model to generate candidate sequences. In production, we have accelerated Sonar using a Llama-1B model fine-tuned on the same dataset as the target. While this approach did not require training a draft from scratch, the small model still uses significant KV cache capacity and introduces a slight prefill overhead, increasing TTFT.Under this scheme, the decoder only speculates on decode-only batches, generating tokens through standard sampling during prefill or on mixed prefill-decode batches. In the prefill stage, the target logits are immediately sampled to also prefill the newly-generated token in the KV cache of the draft. The draft is not sampled yet, but the logits it produces are carried over to the decode stage.In decode, the draft model is advanced, sampling the top token at each stage. After the desired draft length is reached, the tokens are run through the target model to produce the logits based on which the sampler identifies the accepted sequence length. Acceptance is determined by comparing the full probability distributions from the draft and the target. Since the target always outputs one set of logits following the accepted draft sequence, that is sampled to produce an additional output. Since the draft model has not yet seen that accepted token, it is re-run to populate its corresponding KV cache entries in preparation for the next decode step, carrying the logits over again.EAGLEEAGLE is speculative decoding scheme which explores multiple draft sequences, generated through a tree-like traversal of probable draft tokens. A fixed (EAGLE) or dynamically-shaped (EAGLE-2) tree is explored using consecutive executions of the draft tokens, considering the Top-K candidates at each node instead of following the highest scoring token in a linear sequence. The sequences are then scored and the longest suitable one is selected to continue, also appending an additional token from the target.In order to achieve more accurate prediction, an EAGLE draft model predicts not only based on tokens, but also using the target features (last layer hidden states) of the target model. The disadvantage of EAGLE is the need to train custom, small draft models which are accurate enough to generate suitable candidates within a low latency budget. Typically, a draft model is a single transformer layer identical to a decoder layer of the original model, which is tightly coupled to the target by tying to its embeddings and lm_head projections. Since this requires less KV cache capacity, EAGLE has a lower memory footprint.To verify tree-like sequences in the target model, custom attention masks must be used. Unfortunately, using a custom attention mask for a whole sequence significantly slows down attention for realistic input lengths (by up to 50%), nullifying some of the speedup achievable through speculation. We have not yet deployed full tree exploration to production for this reason, focusing instead on the special case of single-token prediction via MTP-like schemes presented in the DeepSeek-V3 Technical Report.MTPThis scheme is similar to draft-target decoding, with the exception of hidden states being used alongside tokens for prediction. Slightly more work must be done in both the prefill and decode stages compared to regular draft-target speculation. The draft model uses both tokens and hidden states: token t_{i+1} is sampled from the logits L_i corresponding to token t_i, which in turn are derived from the hidden states H_i. Consequently, the input token buffers must be shifted one step to the left relative to the hidden state vectors output by the target. The figure below marks the correspondences used for training, as well as the shift during inference.The decoding flow is quite similar to draft-target decoding, with the exception of both hidden states and logits being carried over. Our implementation shares all the associated sampling and logit processing logits, specializing only the model forward invocations. When multiple tokens are predicted, the draft model uses draft hidden states for prediction, also populating KV cache entries based on its own features. In the long run, this can degrade accuracy. Subsequently, when running the draft model to populate the KV cache entry for the target prediction, we run it on the whole sequence taking the more accurate target hidden states as inputs. Since these draft models are small, the added cost of processing the additional tokens is negligible.Training MTP HeadsIn order to benefit from MTP, we built the infrastructure required to train MTP heads attached to our fine-tuned models on Perplexity’s datasets, running on one node with 8xH100 devices. In about one day, we can build heads for models ranging from Llama-1B to Llama-70B and DeepSeek V2-Lite. For larger models, we rely on MTP heads built during the fine-tuning process.The target of MTP training is to match up the draft hidden states and logits extrapolated from the target hidden states to the next token logits and hidden states of the target. Since inference for hidden states is expensive, we pre-compute them using our inference-optimized implementation of the target model, to be used during training. However, to validate the inference MTP implementation and ensure that numerical differences due to quantization or optimizations do not hinder results, for validation loss and accuracy estimation we fully re-use the inference implementation of both the target and the draft models.When scaling from the ShareGPT dataset used in the original paper to larger samples, we noticed that the MTP head architecture outlined and implemented in the EAGLE paper failed to train for 70B-sized models. Unlike ShareGPT which contained a larger number of shorter sequences, we train on a slightly smaller number of substantially longer prompts. Since the original EAGLE heads slightly diverged in structure from a typical transformer, we re-introduced some RMS Normalization layers that were stripped. We found that this not only allowed training to converge, but it also boosted the accuracy of the heads by a few percentage points.Not only do layer norms facilitate training, re-introducing the norms is also mathematically intuitive. MTP heads re-use the embeddings and the logit projections of the target model, as they can be substantial in size (about 2 GB for Llama 70B). During training, these are frozen and the expectation is that the MTP layer learns to embed predictions into the same vector space as what the projection layer of the original model learnt during training. By dropping the norms, a single MLP is expected to learn the same function as an MLP followed by a norm, which hinders the matchup between the hidden states of the draft and the target models.Inference with Speculative DecodingIn the inference engine, in order to generate tokens for input sequences, they need to be first grouped into reasonably-sized batches, then pages must be allocated in the KV cache for the next tokens. The input tokens and the KV page information is then packed into a buffer broadcast to all parallel ranks running the model. Finally, the metadata is copied into GPU memory and the model is executed to produce the logits from which the next token is sampled.Unlike certain implementations which loosely couple a draft and target inference server via a wrapper that orchestrates requests between them, our draft-target pairs are tightly coupled and step through generation in unison. Batch scheduling and KV page allocation is shared between the models for all forms of speculative decoding: this unifies the logic that bridges a model with the overarching inference server, as they all expose the same interface.The inference runtime at Perplexity is shaped around FlashInfer, which determines the metadata that needs to be built in order to configure and schedule the attention kernel. Given some input sequences forming a batch, for prefill, decode or verification, CPU-side work must be done to allocate intermediate buffers and populate certain constant buffers used in attention. This work is in addition to the cost of batch scheduling and KV page allocation, which also incur latencies that must be hidden in order to maximize GPU utilization.While we fully parallelized CPU-side and GPU-side work for inference without speculation, we found that the CPU-GPU balance for speculative decoding is more intricate. The main challenge arises from the fact that the number of accepted tokens determines the sequence length for a subsequent run, introducing a difficult-to-avoid GPU-to-CPU synchronization point. We experimented with different scheduling schemes in order to best hide the latency of CPU work.Draft-Target ScheduleDespite being smaller than a target model, when an entire LLM is used as the draft, it still introduces considerable latency on the GPU, providing some headroom to hide expensive CPU operations. Since smaller models do not benefit from tensor parallelism, there is a mismatch between the number of ranks a target and a draft are sharded across. In our implementation, the draft model runs only on the leader rank of a TP group.As indicated before, a decode step carries over logits into the next run. This allows us to overlap one execution of the draft model with the CPU-side batch scheduling work. After the batch is put together, repeated calls to the sampler and the draft produce the draft tokens. In parallel, the batch for verification is put together for the target model and synchronized with the parallel workers. The target logits are verified and sampled to determine the accepted sequence lengths. At this point, GPU-to-CPU synchronization is necessary in order to determine subsequent sequence lengths. Since the draft model is only run on the leader node, its batch is set up sequentially and its execution is kicked off to populate its KV cache entries with the additional token that the target produced. The logits produced by this draft run in the current run will be used to sample the first draft token in the subsequent run. Most importantly, while the draft is running, the next batch can be scheduled.MTP Schedule for a Single TokenWhile the runtime does not yet provide Eagle-style draft tree exploration, we implemented a special case of this scheme, considering a linear sequence of draft tokens produced by a model the size of a single transformer decoder layer. This scheme can be used for draft prediction using the open-source weights of DeepSeek R1. The sub-case of predicting a single token is interesting, as large MTP layers achieve sufficiently high acceptance rates to justify their overhead.MTP scheduling is somewhat more complex, as the draft model is much faster, hiding less CPU-side latency. Additionally, the draft is sharded alongside the target model, requiring shared memory transfers for batch information. A run starts by transferring batch info and sampling the first token from carry-over logits, similarly to the previous scheme. Next, the target is run to validate tokens, processing 2 * D tokens, where D is the decode batch size. This is ideal for micro-batching in Mixture-of-Experts (MoE) models over slower interconnects such as InfiniBand, as the batch splits evenly into two halves. The hidden states of the target carry over to the next draft run, while the logits are passed into the sampler for verification.By performing a limited amount of additional work on the GPU, we avoid CPU-to-GPU synchronization after draft sequence acceptance. After the input tokens of the targets are shifted, a kernel plugs in the next target tokens into their corresponding locations. The draft is then re-run with the same batch information as the target, populating KV cache entries and building the logits and hidden states for the next run, doing some redundant work on tokens which were not accepted. In these situations, the latency of the unused work is barely measurable due to the small size of the draft model. In parallel with the draft run, sequence lengths are determined on the CPU and the scheduling of the next batch is kicked off, without having to wait for GPU work to terminate.The overhead of additional work in the draft layer is not noticeable in attention, however MLP layers are more problematic. Since matrix multiplication instructions pad to a boundary of 64 along the dimension of number of tokens, if doubling doesn’t require significantly more blocks, the overhead is hidden. For longer draft sequences the overhead is more expensive and the scheme used for regular draft-target models works better.ReferencesFast Inference from Transformers via Speculative DecodingEAGLE: Speculative Sampling Requires Rethinking Feature UncertaintyEAGLE-2: Faster Inference of Language Models with Dynamic Draft TreesEAGLE-3: Scaling up Inference Acceleration of Large Language Models via Training-Time TestMedusa: Simple LLM Inference Acceleration Framework with Multiple Decoding HeadsLayer NormalizationFlashInfer: Efficient and Customizable Attention Engine for LLM Inference ServingDeepSeek-V3 Technical Report'),\n",
       " Document(metadata={'source': 'https://www.perplexity.ai/hub/blog/answers-for-every-investor'}, page_content=\"Answers for Every InvestorAll investors share one thing: curiosity.From retirement savers building long-term wealth to day traders making hundreds of decisions a day. Small business owners researching competitors. Students learning about markets. Financial advisors serving clients. Bankers researching comps, and lawyers reviewing deal documents. All of them need questions.But not every investor can get clear, fast answers from the most important financial data there is: SEC Filings. Today we are bringing answers to every investor.Financial Intelligence For EveryoneStarting today, Perplexity is providing answers leveraging SEC data for all investors. Our new SEC/EDGAR integration provides direct access to comprehensive financial data for all investors, delivered through our answer engine, making complex information instantly understandable.These documents contain the deeper story behind public companies—their actual financials, their strategic plans, their material risks. Yet for too many investors, this critical information remains buried in dense, technical documents that are difficult to navigate and interpret.Every answer comes with direct citations to the source documents, so you can verify the information and dive deeper when needed.Beyond Traditional Financial PlatformsTraditional financial data platforms often require expensive subscriptions, complex interfaces, or specialized knowledge to navigate effectively. They're built for professional analysts and institutional investors, leaving individual investors to piece together information from fragmented sources or rely on simplified summaries that miss crucial details. We believe that everyone deserves access to the same financial information that drives professional investment decisions.Our SEC integration works seamlessly with Perplexity Search, Research, and newly launched Labs, so you can combine financial data with market analysis, news coverage, and industry research in a single conversation. Ask about a company's recent earnings, then immediately explore how those results compare to industry peers or what analysts are saying about the sector's outlook.Perplexity Enterprise Pro customers also can search through SEC filings alongside Factset's M&A and transcript data, Crunchbase's firmographic data, and company files.Ready to explore? Start asking your financial questions on Perplexity today.Answers for Every InvestorAll investors share one thing: curiosity.From retirement savers building long-term wealth to day traders making hundreds of decisions a day. Small business owners researching competitors. Students learning about markets. Financial advisors serving clients. Bankers researching comps, and lawyers reviewing deal documents. All of them need questions.But not every investor can get clear, fast answers from the most important financial data there is: SEC Filings. Today we are bringing answers to every investor.Financial Intelligence For EveryoneStarting today, Perplexity is providing answers leveraging SEC data for all investors. Our new SEC/EDGAR integration provides direct access to comprehensive financial data for all investors, delivered through our answer engine, making complex information instantly understandable.These documents contain the deeper story behind public companies—their actual financials, their strategic plans, their material risks. Yet for too many investors, this critical information remains buried in dense, technical documents that are difficult to navigate and interpret.Every answer comes with direct citations to the source documents, so you can verify the information and dive deeper when needed.Beyond Traditional Financial PlatformsTraditional financial data platforms often require expensive subscriptions, complex interfaces, or specialized knowledge to navigate effectively. They're built for professional analysts and institutional investors, leaving individual investors to piece together information from fragmented sources or rely on simplified summaries that miss crucial details. We believe that everyone deserves access to the same financial information that drives professional investment decisions.Our SEC integration works seamlessly with Perplexity Search, Research, and newly launched Labs, so you can combine financial data with market analysis, news coverage, and industry research in a single conversation. Ask about a company's recent earnings, then immediately explore how those results compare to industry peers or what analysts are saying about the sector's outlook.Perplexity Enterprise Pro customers also can search through SEC filings alongside Factset's M&A and transcript data, Crunchbase's firmographic data, and company files.Ready to explore? Start asking your financial questions on Perplexity today.Answers for Every InvestorAll investors share one thing: curiosity.From retirement savers building long-term wealth to day traders making hundreds of decisions a day. Small business owners researching competitors. Students learning about markets. Financial advisors serving clients. Bankers researching comps, and lawyers reviewing deal documents. All of them need questions.But not every investor can get clear, fast answers from the most important financial data there is: SEC Filings. Today we are bringing answers to every investor.Financial Intelligence For EveryoneStarting today, Perplexity is providing answers leveraging SEC data for all investors. Our new SEC/EDGAR integration provides direct access to comprehensive financial data for all investors, delivered through our answer engine, making complex information instantly understandable.These documents contain the deeper story behind public companies—their actual financials, their strategic plans, their material risks. Yet for too many investors, this critical information remains buried in dense, technical documents that are difficult to navigate and interpret.Every answer comes with direct citations to the source documents, so you can verify the information and dive deeper when needed.Beyond Traditional Financial PlatformsTraditional financial data platforms often require expensive subscriptions, complex interfaces, or specialized knowledge to navigate effectively. They're built for professional analysts and institutional investors, leaving individual investors to piece together information from fragmented sources or rely on simplified summaries that miss crucial details. We believe that everyone deserves access to the same financial information that drives professional investment decisions.Our SEC integration works seamlessly with Perplexity Search, Research, and newly launched Labs, so you can combine financial data with market analysis, news coverage, and industry research in a single conversation. Ask about a company's recent earnings, then immediately explore how those results compare to industry peers or what analysts are saying about the sector's outlook.Perplexity Enterprise Pro customers also can search through SEC filings alongside Factset's M&A and transcript data, Crunchbase's firmographic data, and company files.Ready to explore? Start asking your financial questions on Perplexity today.\"),\n",
       " Document(metadata={'source': 'https://www.perplexity.ai/hub/blog/introducing-perplexity-labs'}, page_content='Introducing Perplexity LabsPerplexity Labs is a way to bring your projects to life faster than ever before. Available today for Pro subscribers.We built Perplexity Search for people who want answers. Later, we launched Deep Research, for users who want deeper, richer analysis.\\xa0Today we are launching Perplexity Labs.\\xa0Using Perplexity is like having a dedicated answer machine available to you 24/7. Using Labs\\xa0is like having a team. A Perplexity Lab is for anyone who wants to bring an entire idea to life.\\xa0Labs can craft everything from reports and spreadsheets to dashboards and simple web apps — all backed by extensive research and analysis. Often performing 10 minutes or more of self-supervised work, Perplexity Labs use a suite of tools like deep web browsing, code execution, and chart and image creation to turn your ideas and to-do’s into work that’s been done.Labs can accomplish in 10 minutes what would previously have taken days of work, tedious research, and coordination of many different skills. Most importantly, the magic behind Labs\\xa0is what Perplexity is best known for — accurate answers that help you make better decisions.\\xa0Since launching the world’s first answer engine two and half years ago, millions of people turn to Perplexity every day for a more accessible way to fuel their curiosity. Labs allow you to convert your curiosity into action.\\xa0Starting today, Pro subscribers can start working with Labs from the mode selector in the input bar on Web, iOS, Android (coming soon to Mac and Windows apps).What’s the easiest way to get started?\\xa0 We are constantly updating examples on our Projects Gallery.\\xa0 Or, you can pick one thing on your to-do list and try it out.\\xa0How It WorksPerplexity Labs can help you complete a variety of work and personal projects. From creating a marketing campaign, to analyzing your business finances, to meal planning for the week, Labs is where you turn your best ideas into reality.Code GenerationLabs writes and executes code to handle tasks like structuring data, applying formulas, and creating charts, text documents, or spreadsheets.Assets TabAll files created during your workflow — from generated charts and images, to CSV and code files — are organized in the “Assets” tab. From there, you can easily view and download everything you need.Mini AppsLabs can develop and deploy simple interactive web apps directly within an “App” tab in your project. This enables the creation of basic dashboards, slideshows, and websites without external development tools.What can you do with Labs?ExamplesWhen to use a Lab vs. Deep ResearchWhile Deep Research remains the fastest way to obtain comprehensive answers to in-depth questions— typically within 3 or 4 minutes — Labs is designed to invest more time (10\\xa0 minutes or longer) and leverage additional tools, such as advanced file generation and mini-app creation. This expanded capability empowers you to develop a broader array of deliverables for your projects.Moving forward, we’re streamlining the name “Deep Research” to “Research,” reflecting its central role between Perplexity’s Search and Lab modes.Try Labs now by going to perplexity.com and selecting “Labs” mode in the search box.Introducing Perplexity LabsPerplexity Labs is a way to bring your projects to life faster than ever before. Available today for Pro subscribers.We built Perplexity Search for people who want answers. Later, we launched Deep Research, for users who want deeper, richer analysis.\\xa0Today we are launching Perplexity Labs.\\xa0Using Perplexity is like having a dedicated answer machine available to you 24/7. Using Labs\\xa0is like having a team. A Perplexity Lab is for anyone who wants to bring an entire idea to life.\\xa0Labs can craft everything from reports and spreadsheets to dashboards and simple web apps — all backed by extensive research and analysis. Often performing 10 minutes or more of self-supervised work, Perplexity Labs use a suite of tools like deep web browsing, code execution, and chart and image creation to turn your ideas and to-do’s into work that’s been done.Labs can accomplish in 10 minutes what would previously have taken days of work, tedious research, and coordination of many different skills. Most importantly, the magic behind Labs\\xa0is what Perplexity is best known for — accurate answers that help you make better decisions.\\xa0Since launching the world’s first answer engine two and half years ago, millions of people turn to Perplexity every day for a more accessible way to fuel their curiosity. Labs allow you to convert your curiosity into action.\\xa0Starting today, Pro subscribers can start working with Labs from the mode selector in the input bar on Web, iOS, Android (coming soon to Mac and Windows apps).What’s the easiest way to get started?\\xa0 We are constantly updating examples on our Projects Gallery.\\xa0 Or, you can pick one thing on your to-do list and try it out.\\xa0How It WorksPerplexity Labs can help you complete a variety of work and personal projects. From creating a marketing campaign, to analyzing your business finances, to meal planning for the week, Labs is where you turn your best ideas into reality.Code GenerationLabs writes and executes code to handle tasks like structuring data, applying formulas, and creating charts, text documents, or spreadsheets.Assets TabAll files created during your workflow — from generated charts and images, to CSV and code files — are organized in the “Assets” tab. From there, you can easily view and download everything you need.Mini AppsLabs can develop and deploy simple interactive web apps directly within an “App” tab in your project. This enables the creation of basic dashboards, slideshows, and websites without external development tools.What can you do with Labs?ExamplesWhen to use a Lab vs. Deep ResearchWhile Deep Research remains the fastest way to obtain comprehensive answers to in-depth questions— typically within 3 or 4 minutes — Labs is designed to invest more time (10\\xa0 minutes or longer) and leverage additional tools, such as advanced file generation and mini-app creation. This expanded capability empowers you to develop a broader array of deliverables for your projects.Moving forward, we’re streamlining the name “Deep Research” to “Research,” reflecting its central role between Perplexity’s Search and Lab modes.Try Labs now by going to perplexity.com and selecting “Labs” mode in the search box.Introducing Perplexity LabsPerplexity Labs is a way to bring your projects to life faster than ever before. Available today for Pro subscribers.We built Perplexity Search for people who want answers. Later, we launched Deep Research, for users who want deeper, richer analysis.\\xa0Today we are launching Perplexity Labs.\\xa0Using Perplexity is like having a dedicated answer machine available to you 24/7. Using Labs\\xa0is like having a team. A Perplexity Lab is for anyone who wants to bring an entire idea to life.\\xa0Labs can craft everything from reports and spreadsheets to dashboards and simple web apps — all backed by extensive research and analysis. Often performing 10 minutes or more of self-supervised work, Perplexity Labs use a suite of tools like deep web browsing, code execution, and chart and image creation to turn your ideas and to-do’s into work that’s been done.Labs can accomplish in 10 minutes what would previously have taken days of work, tedious research, and coordination of many different skills. Most importantly, the magic behind Labs\\xa0is what Perplexity is best known for — accurate answers that help you make better decisions.\\xa0Since launching the world’s first answer engine two and half years ago, millions of people turn to Perplexity every day for a more accessible way to fuel their curiosity. Labs allow you to convert your curiosity into action.\\xa0Starting today, Pro subscribers can start working with Labs from the mode selector in the input bar on Web, iOS, Android (coming soon to Mac and Windows apps).What’s the easiest way to get started?\\xa0 We are constantly updating examples on our Projects Gallery.\\xa0 Or, you can pick one thing on your to-do list and try it out.\\xa0How It WorksPerplexity Labs can help you complete a variety of work and personal projects. From creating a marketing campaign, to analyzing your business finances, to meal planning for the week, Labs is where you turn your best ideas into reality.Code GenerationLabs writes and executes code to handle tasks like structuring data, applying formulas, and creating charts, text documents, or spreadsheets.Assets TabAll files created during your workflow — from generated charts and images, to CSV and code files — are organized in the “Assets” tab. From there, you can easily view and download everything you need.Mini AppsLabs can develop and deploy simple interactive web apps directly within an “App” tab in your project. This enables the creation of basic dashboards, slideshows, and websites without external development tools.What can you do with Labs?ExamplesWhen to use a Lab vs. Deep ResearchWhile Deep Research remains the fastest way to obtain comprehensive answers to in-depth questions— typically within 3 or 4 minutes — Labs is designed to invest more time (10\\xa0 minutes or longer) and leverage additional tools, such as advanced file generation and mini-app creation. This expanded capability empowers you to develop a broader array of deliverables for your projects.Moving forward, we’re streamlining the name “Deep Research” to “Research,” reflecting its central role between Perplexity’s Search and Lab modes.Try Labs now by going to perplexity.com and selecting “Labs” mode in the search box.'),\n",
       " Document(metadata={'source': 'https://www.perplexity.ai/hub/blog/perplexity-x-lewis-hamilton'}, page_content='Today we’re announcing a global partnership between Perplexity and Lewis Hamilton - a collaboration that brings together the relentless pursuit of speed, precision, and curiosity. This partnership unites a world-class athlete and icon with a platform built for those who never stop asking, learning, and moving forward.Lewis Hamilton is not just a Seven-Time Formula One World Champion - he’s a symbol of focus, intention, and constant evolution. Lewis says, “Whether it’s in sport or life, you can never stop asking questions. The best never stop learning. Curiosity is fuel, and that’s why I like using Perplexity.”On and off the track, Lewis leads by example: always growing, always pushing, always questioning what’s possible. At Perplexity, we build for people like Lewis - those who seek clarity in a noisy world.Perplexity is trusted by millions to transform questions into momentum. We believe that knowledge isn’t just about finding answers, but about fueling discovery and action. Whether it’s split-second decision making at 360 kph or instant access to trusted knowledge, it’s important to move fast without sacrificing accuracy. Lewis embodies this philosophy: speed with understanding, precision in motion, curiosity as a driving force.This partnership marks an exciting new chapter for both Perplexity and Lewis Hamilton. Whether you’re pursuing a world championship or searching for answers to life’s toughest questions, success means more than just going fast - it’s about advancing with understanding and purpose.Today we’re announcing a global partnership between Perplexity and Lewis Hamilton - a collaboration that brings together the relentless pursuit of speed, precision, and curiosity. This partnership unites a world-class athlete and icon with a platform built for those who never stop asking, learning, and moving forward.Lewis Hamilton is not just a Seven-Time Formula One World Champion - he’s a symbol of focus, intention, and constant evolution. Lewis says, “Whether it’s in sport or life, you can never stop asking questions. The best never stop learning. Curiosity is fuel, and that’s why I like using Perplexity.”On and off the track, Lewis leads by example: always growing, always pushing, always questioning what’s possible. At Perplexity, we build for people like Lewis - those who seek clarity in a noisy world.Perplexity is trusted by millions to transform questions into momentum. We believe that knowledge isn’t just about finding answers, but about fueling discovery and action. Whether it’s split-second decision making at 360 kph or instant access to trusted knowledge, it’s important to move fast without sacrificing accuracy. Lewis embodies this philosophy: speed with understanding, precision in motion, curiosity as a driving force.This partnership marks an exciting new chapter for both Perplexity and Lewis Hamilton. Whether you’re pursuing a world championship or searching for answers to life’s toughest questions, success means more than just going fast - it’s about advancing with understanding and purpose.Today we’re announcing a global partnership between Perplexity and Lewis Hamilton - a collaboration that brings together the relentless pursuit of speed, precision, and curiosity. This partnership unites a world-class athlete and icon with a platform built for those who never stop asking, learning, and moving forward.Lewis Hamilton is not just a Seven-Time Formula One World Champion - he’s a symbol of focus, intention, and constant evolution. Lewis says, “Whether it’s in sport or life, you can never stop asking questions. The best never stop learning. Curiosity is fuel, and that’s why I like using Perplexity.”On and off the track, Lewis leads by example: always growing, always pushing, always questioning what’s possible. At Perplexity, we build for people like Lewis - those who seek clarity in a noisy world.Perplexity is trusted by millions to transform questions into momentum. We believe that knowledge isn’t just about finding answers, but about fueling discovery and action. Whether it’s split-second decision making at 360 kph or instant access to trusted knowledge, it’s important to move fast without sacrificing accuracy. Lewis embodies this philosophy: speed with understanding, precision in motion, curiosity as a driving force.This partnership marks an exciting new chapter for both Perplexity and Lewis Hamilton. Whether you’re pursuing a world championship or searching for answers to life’s toughest questions, success means more than just going fast - it’s about advancing with understanding and purpose.'),\n",
       " Document(metadata={'source': 'https://www.perplexity.ai/hub/blog/more-value-in-every-answer-new-benefits-for-every-level-of-perplexity-user'}, page_content=\"More Value In Every Answer: New Benefits For Every Level of Perplexity UserWe’re excited to announce the beginning of two big changes in the way Perplexity helps users find information.\\xa0Beginning this week, we are adding perks for free, Pro, and Enterprise users of Perplexity. These additions to Perplexity reflect the same focus we’ve always had–bringing accurate, trustworthy, and valuable answers and information to everyone with the power of AI–as well as our vision for the future of AI that assists us in meaningful and helpful ways.\\xa0These two updates reflect a simple idea: as AI becomes a core tool for decision-making, it should not only make information more accessible, it should make life better!Rewards That Work: Introducing Pro PerksBeginning today, Perplexity Pro subscribers will now have access to premium deals and offers as part of their Perplexity subscriptions, and all users–free, Pro, and Enterprise–will enjoy the benefits of an innovative new way to receive answer citations from some of the world’s most trustworthy data sources.Perplexity Pro Perks provides access to deals and offers from leading brands across travel, health and wellness, finance, and more. We’ve teamed up with brands in categories that are popular on Perplexity, and will build the offers seamlessly into the Perplexity experience, so that Pro users can choose to take advantage of these deals simply by being a Pro member.\\xa0We’re excited to announce the initial batch of Pro Perks, including brands like Visa, Avis®, Budget®, Caliber, Chase Home Lending, Cursor, Eight Sleep, Ezra, Farmers GroupSelect®*, Function, GoodRX, Headspace, LegalZoom, ŌURA, Selfbook, Thumbtack, TurboTax, and Viator. We will continually add to our Pro Perks page, which you can check anytime—or simply keep using Perplexity, and we’ll surface them when relevant.Better Data. Better Answers. For Any Question.Beginning this month Perplexity will also integrate premium content from industry-leading providers: Statista, PitchBook, and Wiley. This initiative will bring financial, health, statistical, and market data, previously restricted to paid enterprise contracts directly to users.These partnerships reflect our commitment to providing the best answers to every possible question. We believe a health query should surface the same citations a physician or nurse would have access to, which is why we’ve partnered with Wiley to search a selection of healthcare content and provide links to the original, trusted sources. Finance and strategy professionals already turn to Perplexity for trustworthy and reliable answers, on thousands of topics. Now, those answers can cite the other data sources they deeply trust, like Statista and PitchBook.These partnerships reflect our deeply shared values of trustworthy, accurate information that is helpful to whoever seeks it.\\xa0 We are citing this data in three ways, depending on how a user accesses Perplexity.\\xa0Free Users: Free users will have access to three searches per month, allowing them to experience the power of our premium data integrations. This upgrade reflects our commitment to making high-quality information accessible to everyone.Pro Users: Pro subscribers will enjoy five Premium searches per month, with each search leveraging our advanced AI models and premium data sources.Enterprise Pro Users: Our enterprise clients will benefit from ten Premium data searches while leveraging the Perplexity platform at work.\\xa0We're Just Getting StartedThe integration of premium data expands Perplexity's ability to provide expert-level answers and information across critical domains, making Perplexity more helpful to everyone who uses it and further increasing the value of a Perplexity subscription. Pro Perks takes that value a step further, bringing the value of AI directly to consumers in immediately beneficial ways.\\xa0\\xa0In each case, one thing remains paramount: trust. That’s why we’re teaming up with companies like Visa – which has a track record of enabling trusted, secure, and reliable payments for billions of consumers around the world. Perplexity’s goal is to provide accurate, trustworthy answers to every question.\\xa0Pro Perks gives our Pro subscribers access to a growing roster of benefits for all kinds of decision-making.\\xa0 You can find current details about Pro Perks at perplexity.ai/properks, and we are excited to add more offers and perks in the coming months. All of our relationships reflect the shared vision that the benefits of AI will extend far beyond the core AI interface. This is our first step on that shared journey, and we can’t wait to announce what’s next.\\xa0\\xa0*Underwritten by Farmers Prop. & Cas. Ins. Co., Economy Fire & Cas. Co., Economy Preferred Ins. Co., Farmers Cas. Ins. Co., Farmers Direct Prop. & Cas. Ins. Co., Farmers Group Prop. & Cas. Ins. Co., or Farmers Lloyds Ins. Co. of TX, all with offices in Warwick, RI. List of licenses at www.farmers.com/companies/state/. Coverage, rates, discounts, & policy features vary by state & product & are available in most states to the qualified.\\xa0More Value In Every Answer: New Benefits For Every Level of Perplexity UserWe’re excited to announce the beginning of two big changes in the way Perplexity helps users find information.\\xa0Beginning this week, we are adding perks for free, Pro, and Enterprise users of Perplexity. These additions to Perplexity reflect the same focus we’ve always had–bringing accurate, trustworthy, and valuable answers and information to everyone with the power of AI–as well as our vision for the future of AI that assists us in meaningful and helpful ways.\\xa0These two updates reflect a simple idea: as AI becomes a core tool for decision-making, it should not only make information more accessible, it should make life better!Rewards That Work: Introducing Pro PerksBeginning today, Perplexity Pro subscribers will now have access to premium deals and offers as part of their Perplexity subscriptions, and all users–free, Pro, and Enterprise–will enjoy the benefits of an innovative new way to receive answer citations from some of the world’s most trustworthy data sources.Perplexity Pro Perks provides access to deals and offers from leading brands across travel, health and wellness, finance, and more. We’ve teamed up with brands in categories that are popular on Perplexity, and will build the offers seamlessly into the Perplexity experience, so that Pro users can choose to take advantage of these deals simply by being a Pro member.\\xa0We’re excited to announce the initial batch of Pro Perks, including brands like Visa, Avis®, Budget®, Caliber, Chase Home Lending, Cursor, Eight Sleep, Ezra, Farmers GroupSelect®*, Function, GoodRX, Headspace, LegalZoom, ŌURA, Selfbook, Thumbtack, TurboTax, and Viator. We will continually add to our Pro Perks page, which you can check anytime—or simply keep using Perplexity, and we’ll surface them when relevant.Better Data. Better Answers. For Any Question.Beginning this month Perplexity will also integrate premium content from industry-leading providers: Statista, PitchBook, and Wiley. This initiative will bring financial, health, statistical, and market data, previously restricted to paid enterprise contracts directly to users.These partnerships reflect our commitment to providing the best answers to every possible question. We believe a health query should surface the same citations a physician or nurse would have access to, which is why we’ve partnered with Wiley to search a selection of healthcare content and provide links to the original, trusted sources. Finance and strategy professionals already turn to Perplexity for trustworthy and reliable answers, on thousands of topics. Now, those answers can cite the other data sources they deeply trust, like Statista and PitchBook.These partnerships reflect our deeply shared values of trustworthy, accurate information that is helpful to whoever seeks it.\\xa0 We are citing this data in three ways, depending on how a user accesses Perplexity.\\xa0Free Users: Free users will have access to three searches per month, allowing them to experience the power of our premium data integrations. This upgrade reflects our commitment to making high-quality information accessible to everyone.Pro Users: Pro subscribers will enjoy five Premium searches per month, with each search leveraging our advanced AI models and premium data sources.Enterprise Pro Users: Our enterprise clients will benefit from ten Premium data searches while leveraging the Perplexity platform at work.\\xa0We're Just Getting StartedThe integration of premium data expands Perplexity's ability to provide expert-level answers and information across critical domains, making Perplexity more helpful to everyone who uses it and further increasing the value of a Perplexity subscription. Pro Perks takes that value a step further, bringing the value of AI directly to consumers in immediately beneficial ways.\\xa0\\xa0In each case, one thing remains paramount: trust. That’s why we’re teaming up with companies like Visa – which has a track record of enabling trusted, secure, and reliable payments for billions of consumers around the world. Perplexity’s goal is to provide accurate, trustworthy answers to every question.\\xa0Pro Perks gives our Pro subscribers access to a growing roster of benefits for all kinds of decision-making.\\xa0 You can find current details about Pro Perks at perplexity.ai/properks, and we are excited to add more offers and perks in the coming months. All of our relationships reflect the shared vision that the benefits of AI will extend far beyond the core AI interface. This is our first step on that shared journey, and we can’t wait to announce what’s next.\\xa0\\xa0*Underwritten by Farmers Prop. & Cas. Ins. Co., Economy Fire & Cas. Co., Economy Preferred Ins. Co., Farmers Cas. Ins. Co., Farmers Direct Prop. & Cas. Ins. Co., Farmers Group Prop. & Cas. Ins. Co., or Farmers Lloyds Ins. Co. of TX, all with offices in Warwick, RI. List of licenses at www.farmers.com/companies/state/. Coverage, rates, discounts, & policy features vary by state & product & are available in most states to the qualified.\\xa0More Value In Every Answer: New Benefits For Every Level of Perplexity UserWe’re excited to announce the beginning of two big changes in the way Perplexity helps users find information.\\xa0Beginning this week, we are adding perks for free, Pro, and Enterprise users of Perplexity. These additions to Perplexity reflect the same focus we’ve always had–bringing accurate, trustworthy, and valuable answers and information to everyone with the power of AI–as well as our vision for the future of AI that assists us in meaningful and helpful ways.\\xa0These two updates reflect a simple idea: as AI becomes a core tool for decision-making, it should not only make information more accessible, it should make life better!Rewards That Work: Introducing Pro PerksBeginning today, Perplexity Pro subscribers will now have access to premium deals and offers as part of their Perplexity subscriptions, and all users–free, Pro, and Enterprise–will enjoy the benefits of an innovative new way to receive answer citations from some of the world’s most trustworthy data sources.Perplexity Pro Perks provides access to deals and offers from leading brands across travel, health and wellness, finance, and more. We’ve teamed up with brands in categories that are popular on Perplexity, and will build the offers seamlessly into the Perplexity experience, so that Pro users can choose to take advantage of these deals simply by being a Pro member.\\xa0We’re excited to announce the initial batch of Pro Perks, including brands like Visa, Avis®, Budget®, Caliber, Chase Home Lending, Cursor, Eight Sleep, Ezra, Farmers GroupSelect®*, Function, GoodRX, Headspace, LegalZoom, ŌURA, Selfbook, Thumbtack, TurboTax, and Viator. We will continually add to our Pro Perks page, which you can check anytime—or simply keep using Perplexity, and we’ll surface them when relevant.Better Data. Better Answers. For Any Question.Beginning this month Perplexity will also integrate premium content from industry-leading providers: Statista, PitchBook, and Wiley. This initiative will bring financial, health, statistical, and market data, previously restricted to paid enterprise contracts directly to users.These partnerships reflect our commitment to providing the best answers to every possible question. We believe a health query should surface the same citations a physician or nurse would have access to, which is why we’ve partnered with Wiley to search a selection of healthcare content and provide links to the original, trusted sources. Finance and strategy professionals already turn to Perplexity for trustworthy and reliable answers, on thousands of topics. Now, those answers can cite the other data sources they deeply trust, like Statista and PitchBook.These partnerships reflect our deeply shared values of trustworthy, accurate information that is helpful to whoever seeks it.\\xa0 We are citing this data in three ways, depending on how a user accesses Perplexity.\\xa0Free Users: Free users will have access to three searches per month, allowing them to experience the power of our premium data integrations. This upgrade reflects our commitment to making high-quality information accessible to everyone.Pro Users: Pro subscribers will enjoy five Premium searches per month, with each search leveraging our advanced AI models and premium data sources.Enterprise Pro Users: Our enterprise clients will benefit from ten Premium data searches while leveraging the Perplexity platform at work.\\xa0We're Just Getting StartedThe integration of premium data expands Perplexity's ability to provide expert-level answers and information across critical domains, making Perplexity more helpful to everyone who uses it and further increasing the value of a Perplexity subscription. Pro Perks takes that value a step further, bringing the value of AI directly to consumers in immediately beneficial ways.\\xa0\\xa0In each case, one thing remains paramount: trust. That’s why we’re teaming up with companies like Visa – which has a track record of enabling trusted, secure, and reliable payments for billions of consumers around the world. Perplexity’s goal is to provide accurate, trustworthy answers to every question.\\xa0Pro Perks gives our Pro subscribers access to a growing roster of benefits for all kinds of decision-making.\\xa0 You can find current details about Pro Perks at perplexity.ai/properks, and we are excited to add more offers and perks in the coming months. All of our relationships reflect the shared vision that the benefits of AI will extend far beyond the core AI interface. This is our first step on that shared journey, and we can’t wait to announce what’s next.\\xa0\\xa0*Underwritten by Farmers Prop. & Cas. Ins. Co., Economy Fire & Cas. Co., Economy Preferred Ins. Co., Farmers Cas. Ins. Co., Farmers Direct Prop. & Cas. Ins. Co., Farmers Group Prop. & Cas. Ins. Co., or Farmers Lloyds Ins. Co. of TX, all with offices in Warwick, RI. List of licenses at www.farmers.com/companies/state/. Coverage, rates, discounts, & policy features vary by state & product & are available in most states to the qualified.\\xa0\"),\n",
       " Document(metadata={'source': 'https://www.perplexity.ai/hub/blog/perplexity-and-sap-turbocharging-joule-with-real-time-answers-for-every-enterprise'}, page_content=\"Perplexity and SAP: Turbocharging Joule with Real-Time Answers for Every EnterpriseToday we’re excited to announce an important collaboration between Perplexity and SAP, unveiled today at SAP Sapphire 2025. Perplexity’s answer engine is being embedded directly into SAP Joule, signaling a bold new chapter in how companies leverage trusted intelligence.Perplexity wants to have accurate, trustworthy answers wherever people are making decisions. SAP empowers workers in businesses around the world to make trillions of dollars of decisions every year.\\xa0 We have a shared goal of ensuring that enterprises and knowledge workers can rely on precise, secure answers whenever business-critical insight matters most.What’s Happening?Soon, anyone using SAP Joule will get instant, AI-powered answers-right inside their SAP workflows. Whether you're tackling strategic projects or fielding ad-hoc data questions, Perplexity’s contextual search will bridge the gap between enterprise information and action.Together with new business answer capabilities from SAP using Perplexity and SAP’s Knowledge Graph, users can ask Joule questions and instantly receive structured answers like metrics, and visual answers, such as charts or graphs, grounded in real-time business data within SAP workflows. For example, a user can ask the tool how recent external events might impact their business and get multiple scenarios based on both current events and the company’s own business data.Why Does It Matter?Clarity from Complexity: Perplexity’s technology cuts through enterprise data noise, surfacing relevant, reliable answers on demand.Trust at Every Turn: We blend Perplexity’s search smarts with SAP’s secure business content, respecting privacy, data ownership, and enterprise standards.Solutions at Scale: This integration isn’t just a product feature-it’s a leap in how organizations find and use business-critical knowledge.Looking AheadFrom enhanced enterprise Q&A to deeper knowledge workflows, this collaboration helps workers make business intelligence even more actionable. With Enterprise Pro, our Sonar API, and Perplexity Pro, our vision is to help people get the best answers whenever and wherever they need them.Perplexity and SAP: Turbocharging Joule with Real-Time Answers for Every EnterpriseToday we’re excited to announce an important collaboration between Perplexity and SAP, unveiled today at SAP Sapphire 2025. Perplexity’s answer engine is being embedded directly into SAP Joule, signaling a bold new chapter in how companies leverage trusted intelligence.Perplexity wants to have accurate, trustworthy answers wherever people are making decisions. SAP empowers workers in businesses around the world to make trillions of dollars of decisions every year.\\xa0 We have a shared goal of ensuring that enterprises and knowledge workers can rely on precise, secure answers whenever business-critical insight matters most.What’s Happening?Soon, anyone using SAP Joule will get instant, AI-powered answers-right inside their SAP workflows. Whether you're tackling strategic projects or fielding ad-hoc data questions, Perplexity’s contextual search will bridge the gap between enterprise information and action.Together with new business answer capabilities from SAP using Perplexity and SAP’s Knowledge Graph, users can ask Joule questions and instantly receive structured answers like metrics, and visual answers, such as charts or graphs, grounded in real-time business data within SAP workflows. For example, a user can ask the tool how recent external events might impact their business and get multiple scenarios based on both current events and the company’s own business data.Why Does It Matter?Clarity from Complexity: Perplexity’s technology cuts through enterprise data noise, surfacing relevant, reliable answers on demand.Trust at Every Turn: We blend Perplexity’s search smarts with SAP’s secure business content, respecting privacy, data ownership, and enterprise standards.Solutions at Scale: This integration isn’t just a product feature-it’s a leap in how organizations find and use business-critical knowledge.Looking AheadFrom enhanced enterprise Q&A to deeper knowledge workflows, this collaboration helps workers make business intelligence even more actionable. With Enterprise Pro, our Sonar API, and Perplexity Pro, our vision is to help people get the best answers whenever and wherever they need them.Perplexity and SAP: Turbocharging Joule with Real-Time Answers for Every EnterpriseToday we’re excited to announce an important collaboration between Perplexity and SAP, unveiled today at SAP Sapphire 2025. Perplexity’s answer engine is being embedded directly into SAP Joule, signaling a bold new chapter in how companies leverage trusted intelligence.Perplexity wants to have accurate, trustworthy answers wherever people are making decisions. SAP empowers workers in businesses around the world to make trillions of dollars of decisions every year.\\xa0 We have a shared goal of ensuring that enterprises and knowledge workers can rely on precise, secure answers whenever business-critical insight matters most.What’s Happening?Soon, anyone using SAP Joule will get instant, AI-powered answers-right inside their SAP workflows. Whether you're tackling strategic projects or fielding ad-hoc data questions, Perplexity’s contextual search will bridge the gap between enterprise information and action.Together with new business answer capabilities from SAP using Perplexity and SAP’s Knowledge Graph, users can ask Joule questions and instantly receive structured answers like metrics, and visual answers, such as charts or graphs, grounded in real-time business data within SAP workflows. For example, a user can ask the tool how recent external events might impact their business and get multiple scenarios based on both current events and the company’s own business data.Why Does It Matter?Clarity from Complexity: Perplexity’s technology cuts through enterprise data noise, surfacing relevant, reliable answers on demand.Trust at Every Turn: We blend Perplexity’s search smarts with SAP’s secure business content, respecting privacy, data ownership, and enterprise standards.Solutions at Scale: This integration isn’t just a product feature-it’s a leap in how organizations find and use business-critical knowledge.Looking AheadFrom enhanced enterprise Q&A to deeper knowledge workflows, this collaboration helps workers make business intelligence even more actionable. With Enterprise Pro, our Sonar API, and Perplexity Pro, our vision is to help people get the best answers whenever and wherever they need them.\"),\n",
       " Document(metadata={'source': 'https://www.perplexity.ai/hub/blog/perplexity-partners-with-wiley-to-power-educational-ai-search'}, page_content=\"Today, we're announcing our partnership with Wiley, one of the world’s largest publishers and trusted leader in research and learning to combine our answer engine with their extensive library of scientific, technical and medical learning materials. This collaboration will enable instructors and students to access Wiley learning materials directly through Perplexity, creating a more engaging and efficient learning experience.  For educational institutions, this partnership represents a significant step forward in curriculum delivery, providing direct access to purchased Wiley academic materials alongside reliable web sources. For students, it means less time searching and more time learning.\\xa0Students can access assigned Wiley curriculum materials through their institution's Enterprise Pro subscription, eliminating the need to switch between platforms. When studying, they can ask questions about textbook content and receive explanations and get comparative real-time examples from the live web.\\xa0\\xa0Benefits for Students and Educators\\xa0This partnership embraces the way GenZ likes to learn. Students can engage with Wiley’s learning materials, including etextbooks, in a number of ways:\\xa0Generate comprehensive study guides from complex material.\\xa0Receive explanations tailored to their level of understanding.\\xa0Connect textbook concepts with real-world applications.\\xa0Focus on learning rather than searching.\\xa0\\xa0For educators, this partnership offers powerful tools to enhance teaching effectiveness:\\xa0\\xa0Create customized learning materials.\\xa0Quickly develop lesson plans that incorporate textbook material.\\xa0Adjust curriculum to real-time events and examples.\\xa0Save time on research and content preparation.\\xa0\\xa0\\xa0For institutions, this solution provides a competitive advantage in educational technology while improving resource utilization and student outcomes.\\xa0Developing Critical AI Literacy\\xa0As educators increasingly tell us, teaching students to use AI effectively and responsibly is becoming as important as the subject matter itself. This partnership supports that mission by providing a structured, educational context for AI interaction that emphasizes accuracy and proper attribution.  By bringing Perplexity's capabilities to Wiley's trusted academic content, we're creating an environment where students develop critical AI literacy alongside subject matter expertise. Our commitment to trusted citations and factual accuracy helps students learn to evaluate information quality, trace knowledge to its origins, and develop the critical thinking skills vital for future success.\\xa0Get Started\\xa0If you’re interested in bringing trusted AI to your campus with Perplexity’s integration with Wiley, admins can reach out to their Wiley representative to authorize the connection to Perplexity. Once approved, admins of Perplexity Enterprise Pro subscriptions can connect their Wiley account to enable students to access textbooks and curriculum in Perplexity.Learn more about how to get started on the Wiley Integration page.  Today, we're announcing our partnership with Wiley, one of the world’s largest publishers and trusted leader in research and learning to combine our answer engine with their extensive library of scientific, technical and medical learning materials. This collaboration will enable instructors and students to access Wiley learning materials directly through Perplexity, creating a more engaging and efficient learning experience.  For educational institutions, this partnership represents a significant step forward in curriculum delivery, providing direct access to purchased Wiley academic materials alongside reliable web sources. For students, it means less time searching and more time learning.\\xa0Students can access assigned Wiley curriculum materials through their institution's Enterprise Pro subscription, eliminating the need to switch between platforms. When studying, they can ask questions about textbook content and receive explanations and get comparative real-time examples from the live web.\\xa0\\xa0Benefits for Students and Educators\\xa0This partnership embraces the way GenZ likes to learn. Students can engage with Wiley’s learning materials, including etextbooks, in a number of ways:\\xa0Generate comprehensive study guides from complex material.\\xa0Receive explanations tailored to their level of understanding.\\xa0Connect textbook concepts with real-world applications.\\xa0Focus on learning rather than searching.\\xa0\\xa0For educators, this partnership offers powerful tools to enhance teaching effectiveness:\\xa0\\xa0Create customized learning materials.\\xa0Quickly develop lesson plans that incorporate textbook material.\\xa0Adjust curriculum to real-time events and examples.\\xa0Save time on research and content preparation.\\xa0\\xa0\\xa0For institutions, this solution provides a competitive advantage in educational technology while improving resource utilization and student outcomes.\\xa0Developing Critical AI Literacy\\xa0As educators increasingly tell us, teaching students to use AI effectively and responsibly is becoming as important as the subject matter itself. This partnership supports that mission by providing a structured, educational context for AI interaction that emphasizes accuracy and proper attribution.  By bringing Perplexity's capabilities to Wiley's trusted academic content, we're creating an environment where students develop critical AI literacy alongside subject matter expertise. Our commitment to trusted citations and factual accuracy helps students learn to evaluate information quality, trace knowledge to its origins, and develop the critical thinking skills vital for future success.\\xa0Get Started\\xa0If you’re interested in bringing trusted AI to your campus with Perplexity’s integration with Wiley, admins can reach out to their Wiley representative to authorize the connection to Perplexity. Once approved, admins of Perplexity Enterprise Pro subscriptions can connect their Wiley account to enable students to access textbooks and curriculum in Perplexity.Learn more about how to get started on the Wiley Integration page.  Today, we're announcing our partnership with Wiley, one of the world’s largest publishers and trusted leader in research and learning to combine our answer engine with their extensive library of scientific, technical and medical learning materials. This collaboration will enable instructors and students to access Wiley learning materials directly through Perplexity, creating a more engaging and efficient learning experience.  For educational institutions, this partnership represents a significant step forward in curriculum delivery, providing direct access to purchased Wiley academic materials alongside reliable web sources. For students, it means less time searching and more time learning.\\xa0Students can access assigned Wiley curriculum materials through their institution's Enterprise Pro subscription, eliminating the need to switch between platforms. When studying, they can ask questions about textbook content and receive explanations and get comparative real-time examples from the live web.\\xa0\\xa0Benefits for Students and Educators\\xa0This partnership embraces the way GenZ likes to learn. Students can engage with Wiley’s learning materials, including etextbooks, in a number of ways:\\xa0Generate comprehensive study guides from complex material.\\xa0Receive explanations tailored to their level of understanding.\\xa0Connect textbook concepts with real-world applications.\\xa0Focus on learning rather than searching.\\xa0\\xa0For educators, this partnership offers powerful tools to enhance teaching effectiveness:\\xa0\\xa0Create customized learning materials.\\xa0Quickly develop lesson plans that incorporate textbook material.\\xa0Adjust curriculum to real-time events and examples.\\xa0Save time on research and content preparation.\\xa0\\xa0\\xa0For institutions, this solution provides a competitive advantage in educational technology while improving resource utilization and student outcomes.\\xa0Developing Critical AI Literacy\\xa0As educators increasingly tell us, teaching students to use AI effectively and responsibly is becoming as important as the subject matter itself. This partnership supports that mission by providing a structured, educational context for AI interaction that emphasizes accuracy and proper attribution.  By bringing Perplexity's capabilities to Wiley's trusted academic content, we're creating an environment where students develop critical AI literacy alongside subject matter expertise. Our commitment to trusted citations and factual accuracy helps students learn to evaluate information quality, trace knowledge to its origins, and develop the critical thinking skills vital for future success.\\xa0Get Started\\xa0If you’re interested in bringing trusted AI to your campus with Perplexity’s integration with Wiley, admins can reach out to their Wiley representative to authorize the connection to Perplexity. Once approved, admins of Perplexity Enterprise Pro subscriptions can connect their Wiley account to enable students to access textbooks and curriculum in Perplexity.Learn more about how to get started on the Wiley Integration page.  \"),\n",
       " Document(metadata={'source': 'https://www.perplexity.ai/hub/blog/announcing-our-global-partnership-with-motorola'}, page_content='We\\'re excited to announce our global partnership with Motorola. Starting with the new generation of Motorola devices, Perplexity will be pre-installed on millions of smartphones worldwide, giving Motorola users direct access to our answer engine and assistant.Integration Across Motorola DevicesWe\\'ve worked closely with Motorola to ensure Perplexity is optimized for their latest devices, including the innovative Razr series. The Perplexity app will come pre-installed on all new Motorola devices, making our search capabilities immediately available. But it’s not just about saving you a trip to the Play Store. We\\'ve created custom optimizations for Motorola\\'s hardware and software, including:Functionality on Razr devices\\' external display when folded shutPerplexity Assistant that extends beyond just information retrieval, allowing users to send emails, set smart reminders, play media, request rides, and book restaurant reservationsDirect access through Moto AI by typing \"Ask Perplexity\"Content exploration leveraging Perplexity’s Related Questions from the screen with \"Next Move\"This is one of our first and most comprehensive integrations with a mobile phone brand, designed to provide a seamless search and assistant experience directly within Motorola\\'s ecosystem.Perplexity Pro for Motorola UsersAll users of new Motorola phones—including the Razr and Edge 60 devices—will receive 3 months of Perplexity Pro at no cost. This provides access to:Deep Research for comprehensive analysis on complex topicsChoice of advanced AI models including Sonar, Claude 3.5 Sonnet, and GPT-4oPro Shopping featuresUnlimited file uploads and Pro/Reasoning searchesThis collaboration with Motorola significantly expands Perplexity\\'s reach. By integrating with one of the world\\'s leading smartphone manufacturers, we\\'re bringing our search and assistant capabilities to millions of new users. It also fundamentally changes the relationship between you and your device. Your phone is now a personal assistant, answer machine and research analyst all in one, available to you on demand, 24/7. The seamless integration of information synthesis and actions is the future of mobile technology and we are happy to get it into the hands of millions of users through our Motorola partnership.\\xa0\\xa0We\\'re excited to announce our global partnership with Motorola. Starting with the new generation of Motorola devices, Perplexity will be pre-installed on millions of smartphones worldwide, giving Motorola users direct access to our answer engine and assistant.Integration Across Motorola DevicesWe\\'ve worked closely with Motorola to ensure Perplexity is optimized for their latest devices, including the innovative Razr series. The Perplexity app will come pre-installed on all new Motorola devices, making our search capabilities immediately available. But it’s not just about saving you a trip to the Play Store. We\\'ve created custom optimizations for Motorola\\'s hardware and software, including:Functionality on Razr devices\\' external display when folded shutPerplexity Assistant that extends beyond just information retrieval, allowing users to send emails, set smart reminders, play media, request rides, and book restaurant reservationsDirect access through Moto AI by typing \"Ask Perplexity\"Content exploration leveraging Perplexity’s Related Questions from the screen with \"Next Move\"This is one of our first and most comprehensive integrations with a mobile phone brand, designed to provide a seamless search and assistant experience directly within Motorola\\'s ecosystem.Perplexity Pro for Motorola UsersAll users of new Motorola phones—including the Razr and Edge 60 devices—will receive 3 months of Perplexity Pro at no cost. This provides access to:Deep Research for comprehensive analysis on complex topicsChoice of advanced AI models including Sonar, Claude 3.5 Sonnet, and GPT-4oPro Shopping featuresUnlimited file uploads and Pro/Reasoning searchesThis collaboration with Motorola significantly expands Perplexity\\'s reach. By integrating with one of the world\\'s leading smartphone manufacturers, we\\'re bringing our search and assistant capabilities to millions of new users. It also fundamentally changes the relationship between you and your device. Your phone is now a personal assistant, answer machine and research analyst all in one, available to you on demand, 24/7. The seamless integration of information synthesis and actions is the future of mobile technology and we are happy to get it into the hands of millions of users through our Motorola partnership.\\xa0\\xa0We\\'re excited to announce our global partnership with Motorola. Starting with the new generation of Motorola devices, Perplexity will be pre-installed on millions of smartphones worldwide, giving Motorola users direct access to our answer engine and assistant.Integration Across Motorola DevicesWe\\'ve worked closely with Motorola to ensure Perplexity is optimized for their latest devices, including the innovative Razr series. The Perplexity app will come pre-installed on all new Motorola devices, making our search capabilities immediately available. But it’s not just about saving you a trip to the Play Store. We\\'ve created custom optimizations for Motorola\\'s hardware and software, including:Functionality on Razr devices\\' external display when folded shutPerplexity Assistant that extends beyond just information retrieval, allowing users to send emails, set smart reminders, play media, request rides, and book restaurant reservationsDirect access through Moto AI by typing \"Ask Perplexity\"Content exploration leveraging Perplexity’s Related Questions from the screen with \"Next Move\"This is one of our first and most comprehensive integrations with a mobile phone brand, designed to provide a seamless search and assistant experience directly within Motorola\\'s ecosystem.Perplexity Pro for Motorola UsersAll users of new Motorola phones—including the Razr and Edge 60 devices—will receive 3 months of Perplexity Pro at no cost. This provides access to:Deep Research for comprehensive analysis on complex topicsChoice of advanced AI models including Sonar, Claude 3.5 Sonnet, and GPT-4oPro Shopping featuresUnlimited file uploads and Pro/Reasoning searchesThis collaboration with Motorola significantly expands Perplexity\\'s reach. By integrating with one of the world\\'s leading smartphone manufacturers, we\\'re bringing our search and assistant capabilities to millions of new users. It also fundamentally changes the relationship between you and your device. Your phone is now a personal assistant, answer machine and research analyst all in one, available to you on demand, 24/7. The seamless integration of information synthesis and actions is the future of mobile technology and we are happy to get it into the hands of millions of users through our Motorola partnership.\\xa0\\xa0'),\n",
       " Document(metadata={'source': 'https://www.perplexity.ai/hub/blog/choice-is-the-remedy'}, page_content='Choice is the RemedyEvery smartphone powers on with pre-selected apps you didn\\'t choose: your browser, your search engine, your voice assistant, and other apps.\\xa0You don\\'t have a choice. For Google, that\\'s a strategy.Last August, the US Government ruled that Google illegally exercised monopoly powers by controlling which apps are on your phone, and worse, which apps are NOT on your phone.\\xa0\\xa0Google controlled this through elaborate contracts with phone makers and carriers. The terms of the contracts threatened OEMs and carriers with wide ranging penalties for not satisfying vague or arbitrary reviews by Google.This week, Google will be back in court with the Department of Justice to determine what the remedy for this should be.Perplexity has been asked by both the DOJ and Google to provide input on this case. That’s rare. It tells you this moment matters—because what happens here affects the future of innovation, not just in search, but across the digital economy.When we think about a search product that\\'s 10X better than 10 blue links, we also think about being a company that works better with OEMs, carriers, and partners of all kinds.\\xa0That\\'s because the only way we (or anyone else) can compete after all the hard work of building a superior product, is to be chosen.\\xa0Consumers must be aware choices exist, and they must have the option to identify and select what\\'s best for them.\\xa0Openness Is Working—But It’s Under StrainIt\\'s important to remember how beneficial Google has been for consumers and for America.\\xa0The truth is, open systems like Android and Chromium have powered huge advances in search, mobile, and AI over the last decade. The value others have created on those platforms far exceeds their own value.\\xa0The heart of this case is that a platform is no longer \"open\" when it\\'s accompanied by legal obligations to promote Google\\'s products.\\xa0\\xa0Android is a clear example. The operating system is open-source, yet Google\\'s rules and revenue agreements layered on top are what made Google a monopoly.If a phone maker wants to include any of Google\\'s apps like Google Maps or the Play Store, they\\'re required to include all of them. They also have to preload Google Search and Google Assistant as required defaults and limit alternatives for their users. Some carriers are even subject to a lower revenue share if Google just doesn\\'t like what they eventually ship.\\xa0When these rules for OEMs and carriers first launched in 2009, Google required them to include 12 of Google\\'s apps by default, ballooning to 30 by 2014. Today, six of them must be \"undeletable.\"\\xa0This is especially bad because many of Google\\'s apps are no longer the best at what they do (like search or voice assistance), or never were the best (like music or TV).\\xa0 In other words, mobile companies are forced to give their customers a subpar experience that Google demands to preserve its monopoly.\\xa0\\xa0For consumers, even when they have a choice it\\'s still a frustrating experience. Google\\'s pet soundbite in interviews and testimony has been \"the competition is just 1 click away.\"\\xa0 The reality is that the option to change your default search engine on an Android device is hidden in settings 4-5 clicks away from the home screen. Most people don’t even know it\\'s possible.\\xa0That’s not what choice looks like.\\xa0The Remedy Isn’t Breakup. Just Let People Choose.The issue in this case is that Google\\'s vast financial resources let it stay dominant by simply paying to force a subpar experience on consumers–not by building better products.\\xa0In the proceedings this week, three remedies are under consideration. One forces Google to sell Chrome. Another forces Google to license its data to other companies. Neither of these address the root issue: consumers deserve choice, while OEMs and carriers deserve freedom to offer it.\\xa0The third remedy separates Android use from the requirements to include all Google\\'s apps and eliminates revenue penalties for offering choices.\\xa0This solution is the simplest. Let people choose. Let phone makers and carriers offer their customers what they want–choice–without fearing financial penalties or access restrictions.Without this remedy, the risk is obvious: Google can keep outspending everyone else to stay the default—even if someone builds something better. Consumers deserve the best products, not just the ones that pay the most for placement. This is the only remedy that ensures consumer choice can determine the winners.We’re Not a Competitor. We’re a Choice.At Perplexity, we don’t see ourselves as a competitor to Google. We’re building something different. We\\'re trying to give people another choice: search that answers, assistants that work. AI that\\'s intelligent, accurate, and trustworthy.Some consumers will choose both Google and Perplexity. Some will choose one or the other. That’s what choice looks like in a healthy ecosystem.\\xa0And for OEMs and carriers: you shouldn’t have to pick a side.\\xa0Leaders and innovators at mobile OEMs and carriers should feel confident and safe working with us and with Google. Our apps can live on the same devices. We believe high-quality products should be allowed to earn a place on any screen.\\xa0The future of search is clear, and it isn\\'t links for Google to sell traffic. It\\'s AI that answers questions, completes tasks, and interacts with applications. Simply put, the future of search is much better for consumers.Google knows this. So they\\'ve protected the monopoly power of an inferior product by hiding better ones from their own users–and forcing OEM and carrier partners to sandbag their own customers. Otherwise, consumers would choose the future.To be clear, the risk for America isn’t that Google is too dominant. It’s when any company uses their dominance to limit consumer choice, especially when better options already exist.\\xa0The remedy to this problem is to prevent restrictive contracts that limit OEMs and carriers from giving consumers real choice. If regulators overreact in this remedy phase the alternative could be worse.\\xa0Chrome has rightly earned its dominant position in the market because it has been (emphasis on has-been) a superior product. For consumers, that made it a welcome choice.\\xa0\\xa0For developers, innovators, and any American with a vision, that made Google a welcome inspiration. To innovate. To create a new choice and someday earn the same rewards of success.\\xa0\\xa0We think Perplexity is doing that.\\xa0And if someone–even Google–builds something even better than us? That\\'s the whole point. It would be an excellent choice.Choice is the RemedyEvery smartphone powers on with pre-selected apps you didn\\'t choose: your browser, your search engine, your voice assistant, and other apps.\\xa0You don\\'t have a choice. For Google, that\\'s a strategy.Last August, the US Government ruled that Google illegally exercised monopoly powers by controlling which apps are on your phone, and worse, which apps are NOT on your phone.\\xa0\\xa0Google controlled this through elaborate contracts with phone makers and carriers. The terms of the contracts threatened OEMs and carriers with wide ranging penalties for not satisfying vague or arbitrary reviews by Google.This week, Google will be back in court with the Department of Justice to determine what the remedy for this should be.Perplexity has been asked by both the DOJ and Google to provide input on this case. That’s rare. It tells you this moment matters—because what happens here affects the future of innovation, not just in search, but across the digital economy.When we think about a search product that\\'s 10X better than 10 blue links, we also think about being a company that works better with OEMs, carriers, and partners of all kinds.\\xa0That\\'s because the only way we (or anyone else) can compete after all the hard work of building a superior product, is to be chosen.\\xa0Consumers must be aware choices exist, and they must have the option to identify and select what\\'s best for them.\\xa0Openness Is Working—But It’s Under StrainIt\\'s important to remember how beneficial Google has been for consumers and for America.\\xa0The truth is, open systems like Android and Chromium have powered huge advances in search, mobile, and AI over the last decade. The value others have created on those platforms far exceeds their own value.\\xa0The heart of this case is that a platform is no longer \"open\" when it\\'s accompanied by legal obligations to promote Google\\'s products.\\xa0\\xa0Android is a clear example. The operating system is open-source, yet Google\\'s rules and revenue agreements layered on top are what made Google a monopoly.If a phone maker wants to include any of Google\\'s apps like Google Maps or the Play Store, they\\'re required to include all of them. They also have to preload Google Search and Google Assistant as required defaults and limit alternatives for their users. Some carriers are even subject to a lower revenue share if Google just doesn\\'t like what they eventually ship.\\xa0When these rules for OEMs and carriers first launched in 2009, Google required them to include 12 of Google\\'s apps by default, ballooning to 30 by 2014. Today, six of them must be \"undeletable.\"\\xa0This is especially bad because many of Google\\'s apps are no longer the best at what they do (like search or voice assistance), or never were the best (like music or TV).\\xa0 In other words, mobile companies are forced to give their customers a subpar experience that Google demands to preserve its monopoly.\\xa0\\xa0For consumers, even when they have a choice it\\'s still a frustrating experience. Google\\'s pet soundbite in interviews and testimony has been \"the competition is just 1 click away.\"\\xa0 The reality is that the option to change your default search engine on an Android device is hidden in settings 4-5 clicks away from the home screen. Most people don’t even know it\\'s possible.\\xa0That’s not what choice looks like.\\xa0The Remedy Isn’t Breakup. Just Let People Choose.The issue in this case is that Google\\'s vast financial resources let it stay dominant by simply paying to force a subpar experience on consumers–not by building better products.\\xa0In the proceedings this week, three remedies are under consideration. One forces Google to sell Chrome. Another forces Google to license its data to other companies. Neither of these address the root issue: consumers deserve choice, while OEMs and carriers deserve freedom to offer it.\\xa0The third remedy separates Android use from the requirements to include all Google\\'s apps and eliminates revenue penalties for offering choices.\\xa0This solution is the simplest. Let people choose. Let phone makers and carriers offer their customers what they want–choice–without fearing financial penalties or access restrictions.Without this remedy, the risk is obvious: Google can keep outspending everyone else to stay the default—even if someone builds something better. Consumers deserve the best products, not just the ones that pay the most for placement. This is the only remedy that ensures consumer choice can determine the winners.We’re Not a Competitor. We’re a Choice.At Perplexity, we don’t see ourselves as a competitor to Google. We’re building something different. We\\'re trying to give people another choice: search that answers, assistants that work. AI that\\'s intelligent, accurate, and trustworthy.Some consumers will choose both Google and Perplexity. Some will choose one or the other. That’s what choice looks like in a healthy ecosystem.\\xa0And for OEMs and carriers: you shouldn’t have to pick a side.\\xa0Leaders and innovators at mobile OEMs and carriers should feel confident and safe working with us and with Google. Our apps can live on the same devices. We believe high-quality products should be allowed to earn a place on any screen.\\xa0The future of search is clear, and it isn\\'t links for Google to sell traffic. It\\'s AI that answers questions, completes tasks, and interacts with applications. Simply put, the future of search is much better for consumers.Google knows this. So they\\'ve protected the monopoly power of an inferior product by hiding better ones from their own users–and forcing OEM and carrier partners to sandbag their own customers. Otherwise, consumers would choose the future.To be clear, the risk for America isn’t that Google is too dominant. It’s when any company uses their dominance to limit consumer choice, especially when better options already exist.\\xa0The remedy to this problem is to prevent restrictive contracts that limit OEMs and carriers from giving consumers real choice. If regulators overreact in this remedy phase the alternative could be worse.\\xa0Chrome has rightly earned its dominant position in the market because it has been (emphasis on has-been) a superior product. For consumers, that made it a welcome choice.\\xa0\\xa0For developers, innovators, and any American with a vision, that made Google a welcome inspiration. To innovate. To create a new choice and someday earn the same rewards of success.\\xa0\\xa0We think Perplexity is doing that.\\xa0And if someone–even Google–builds something even better than us? That\\'s the whole point. It would be an excellent choice.Choice is the RemedyEvery smartphone powers on with pre-selected apps you didn\\'t choose: your browser, your search engine, your voice assistant, and other apps.\\xa0You don\\'t have a choice. For Google, that\\'s a strategy.Last August, the US Government ruled that Google illegally exercised monopoly powers by controlling which apps are on your phone, and worse, which apps are NOT on your phone.\\xa0\\xa0Google controlled this through elaborate contracts with phone makers and carriers. The terms of the contracts threatened OEMs and carriers with wide ranging penalties for not satisfying vague or arbitrary reviews by Google.This week, Google will be back in court with the Department of Justice to determine what the remedy for this should be.Perplexity has been asked by both the DOJ and Google to provide input on this case. That’s rare. It tells you this moment matters—because what happens here affects the future of innovation, not just in search, but across the digital economy.When we think about a search product that\\'s 10X better than 10 blue links, we also think about being a company that works better with OEMs, carriers, and partners of all kinds.\\xa0That\\'s because the only way we (or anyone else) can compete after all the hard work of building a superior product, is to be chosen.\\xa0Consumers must be aware choices exist, and they must have the option to identify and select what\\'s best for them.\\xa0Openness Is Working—But It’s Under StrainIt\\'s important to remember how beneficial Google has been for consumers and for America.\\xa0The truth is, open systems like Android and Chromium have powered huge advances in search, mobile, and AI over the last decade. The value others have created on those platforms far exceeds their own value.\\xa0The heart of this case is that a platform is no longer \"open\" when it\\'s accompanied by legal obligations to promote Google\\'s products.\\xa0\\xa0Android is a clear example. The operating system is open-source, yet Google\\'s rules and revenue agreements layered on top are what made Google a monopoly.If a phone maker wants to include any of Google\\'s apps like Google Maps or the Play Store, they\\'re required to include all of them. They also have to preload Google Search and Google Assistant as required defaults and limit alternatives for their users. Some carriers are even subject to a lower revenue share if Google just doesn\\'t like what they eventually ship.\\xa0When these rules for OEMs and carriers first launched in 2009, Google required them to include 12 of Google\\'s apps by default, ballooning to 30 by 2014. Today, six of them must be \"undeletable.\"\\xa0This is especially bad because many of Google\\'s apps are no longer the best at what they do (like search or voice assistance), or never were the best (like music or TV).\\xa0 In other words, mobile companies are forced to give their customers a subpar experience that Google demands to preserve its monopoly.\\xa0\\xa0For consumers, even when they have a choice it\\'s still a frustrating experience. Google\\'s pet soundbite in interviews and testimony has been \"the competition is just 1 click away.\"\\xa0 The reality is that the option to change your default search engine on an Android device is hidden in settings 4-5 clicks away from the home screen. Most people don’t even know it\\'s possible.\\xa0That’s not what choice looks like.\\xa0The Remedy Isn’t Breakup. Just Let People Choose.The issue in this case is that Google\\'s vast financial resources let it stay dominant by simply paying to force a subpar experience on consumers–not by building better products.\\xa0In the proceedings this week, three remedies are under consideration. One forces Google to sell Chrome. Another forces Google to license its data to other companies. Neither of these address the root issue: consumers deserve choice, while OEMs and carriers deserve freedom to offer it.\\xa0The third remedy separates Android use from the requirements to include all Google\\'s apps and eliminates revenue penalties for offering choices.\\xa0This solution is the simplest. Let people choose. Let phone makers and carriers offer their customers what they want–choice–without fearing financial penalties or access restrictions.Without this remedy, the risk is obvious: Google can keep outspending everyone else to stay the default—even if someone builds something better. Consumers deserve the best products, not just the ones that pay the most for placement. This is the only remedy that ensures consumer choice can determine the winners.We’re Not a Competitor. We’re a Choice.At Perplexity, we don’t see ourselves as a competitor to Google. We’re building something different. We\\'re trying to give people another choice: search that answers, assistants that work. AI that\\'s intelligent, accurate, and trustworthy.Some consumers will choose both Google and Perplexity. Some will choose one or the other. That’s what choice looks like in a healthy ecosystem.\\xa0And for OEMs and carriers: you shouldn’t have to pick a side.\\xa0Leaders and innovators at mobile OEMs and carriers should feel confident and safe working with us and with Google. Our apps can live on the same devices. We believe high-quality products should be allowed to earn a place on any screen.\\xa0The future of search is clear, and it isn\\'t links for Google to sell traffic. It\\'s AI that answers questions, completes tasks, and interacts with applications. Simply put, the future of search is much better for consumers.Google knows this. So they\\'ve protected the monopoly power of an inferior product by hiding better ones from their own users–and forcing OEM and carrier partners to sandbag their own customers. Otherwise, consumers would choose the future.To be clear, the risk for America isn’t that Google is too dominant. It’s when any company uses their dominance to limit consumer choice, especially when better options already exist.\\xa0The remedy to this problem is to prevent restrictive contracts that limit OEMs and carriers from giving consumers real choice. If regulators overreact in this remedy phase the alternative could be worse.\\xa0Chrome has rightly earned its dominant position in the market because it has been (emphasis on has-been) a superior product. For consumers, that made it a welcome choice.\\xa0\\xa0For developers, innovators, and any American with a vision, that made Google a welcome inspiration. To innovate. To create a new choice and someday earn the same rewards of success.\\xa0\\xa0We think Perplexity is doing that.\\xa0And if someone–even Google–builds something even better than us? That\\'s the whole point. It would be an excellent choice.'),\n",
       " Document(metadata={'source': 'https://www.perplexity.ai/hub/blog/how-perplexity-enterprise-pro-keeps-your-data-secure'}, page_content=\"At Perplexity, we take our customer’s data and security very seriously. Which is why we're proud to announce Perplexity is now SOC 2 Type II compliant. This independently validates our security practices and confirms that our platform meets the highest data protection standards. Alongside our adherence to GDPR and PCI DSS frameworks, Perplexity provides our enterprise customers with verified assurance that their sensitive information is handled with the highest standards of care and protection.With advanced privacy protections, admin controls, and compliance-ready infrastructure, Enterprise Pro provides your team with a powerful answer engine and deep research tools for the web and internal files while keeping your data private and secure.\\xa0Security Hub Gives Admins Precise Control Over Your Orgs Security\\xa0Enterprise Pro offers access to our Security Hub: a centralized admin command center that gives organizations complete control over how Perplexity is used across teams.\\xa0User ManagementAdmins use the Security Hub to give every team member the appropriate level of access within Perplexity. They can set permissions for:\\xa0File Uploads and Downloads: Choose who can upload or download documents AI-generated content to prevent unauthorized sharing or data leakage.Shared Content: Manage how team members create, share, and collaborate on Threads, Pages, and Spaces to keep internal knowledge organized and secure.Data Integrations and Connectors: Set permissions for who can connect to external, integrated data sources and cloud storage services like Google Drive, Microsoft OneDrive, and Sharepoint. Perplexity Connectors enable these data integrations so teams can connect external data sources directly to our AI engine. While Connectors boost productivity by making private data accessible, the Security Hub's permissions ensure only authorized users have that power.AI Model Enablement: Decide which large language models your team can use to align with internal security or compliance policies.Internal Knowledge Bases: Manage access to internal knowledge bases and file repositories, ensuring only approved users can browse connected files, documents, or datasets within your workspace.To close the User Management loop, Enterprise Pro combines Single Sign-On (SSO) with Multi-Factor Authentication (MFA) and short-lived session credentials. Data Privacy and RetentionBeyond User Management, Perplexity Enterprise Pro ensures your data is secure and private by keeping it out of reach of attackers and even third parties.\\xa0Third-Party Agreements: We maintain formal agreements with all third-party model providers to ensure they can’t access or use your data for training purposes. We review these agreements annually to stay compliant with evolving standards and requirements. The best part? Enterprise Pro (unlike every other model provider) does not use your data to improve or train its own systems.Connector Security: Whether you're connecting to a public data source like Crunchbase or syncing files through FactSet, all connectors fall under our platform’s strict privacy and security policies.Safe Use of DeepSeek R1: Rest assured that Enterprise Pro’s integration with DeepSeek R1 only uses the open weights version, which runs directly on Perplexity servers in the U.S. This means your queries stay within the Perplexity ecosystem and are never exposed to DeepSeek or any external infrastructure.Automatic Data Deletion: Perplexity automatically deletes files attached to threads after seven days. Only files uploaded to Spaces do not automatically expire, and these files (just like all data that we handle) are protected through enterprise-grade encryption both at rest and in transit. You can also enforce incognito mode to disable search history.\\xa0Audit logs: Enterprise Pro admins can receive real-time activity alerts such as file uploads and downloads into your workspace. This way, admins can immediately have increased visibility.Transparency: Review which service providers we work with and how your data flows through the platform at any time on our Trust Center. This gives your team full visibility into third-party relationships and ensures accountability at every step.A Robust Security Program That Operates 24/7Perplexity’s commitment to your security doesn’t stop there. Our security team also monitors for potential threats around the clock and responds in real time using automated workflows with detailed playbooks. Here's what they use to do it:Cloud security infrastructure: Perplexity Enterprise Pro runs on AWS, a secure cloud infrastructure with built-in physical security, network protection, and industry-leading compliance certifications. To strengthen our cloud security posture further, we also use Wiz to proactively identify risks across our environment.Vulnerability Disclosure Program (VDP): We created this to impose a safe, structured process for external security researchers and ethical hackers to report vulnerabilities. It covers all public-facing endpoints and rewards those who help us identify and fix potential gaps – like misconfigured access controls or exposed APIs – before they can be exploited.Short-lived authentication and just-in-time access: No engineer has access to production data by default. Getting access to production data is only granted in business critical cases, requires security approval, and is heavily monitored.\\xa0Bug Bounty Program: We reward vetted security researchers for reporting critical vulnerabilities across our ecosystem. Run through a private, invite-only program with Bugcrowd; it covers Perplexity’s web applications, APIs, and cloud services as another layer of proactive protection to keep your data safe.\\xa0Perplexity Enterprise Pro’s Secure, AI-Powered Answer EngineWith Perplexity Enterprise Pro being compliant with SOC2 Type II, organizations mitigate the threat of data leaks, black-box risks, and security compromises. You can have confidence in our AI answer engine with enterprise-grade controls, transparent data privacy policies, and full confidence in how your data is handled, so your team can get work done quickly without sacrificing security. You can view our report here.\\xa0Here’s how Perplexity Enterprise Pro stacks up against Perplexity offering:More than 7,000 Enterprise Pro customers include NVIDIA, Databricks, Stripe, and the U.S. Anti-Doping Agency (USADA) trust Perplexity. For USADA, data privacy isn’t a preference; it’s policy. “We implemented our first AI policy stating that no athlete data is ever allowed to be used on an AI platform,” said John Bobo, Chief Operating & Innovation Officer. “What really attracted me to the Perplexity Enterprise offering was the advanced security features.”Start searching securely and strategically with Perplexity Enterprise Pro today.At Perplexity, we take our customer’s data and security very seriously. Which is why we're proud to announce Perplexity is now SOC 2 Type II compliant. This independently validates our security practices and confirms that our platform meets the highest data protection standards. Alongside our adherence to GDPR and PCI DSS frameworks, Perplexity provides our enterprise customers with verified assurance that their sensitive information is handled with the highest standards of care and protection.With advanced privacy protections, admin controls, and compliance-ready infrastructure, Enterprise Pro provides your team with a powerful answer engine and deep research tools for the web and internal files while keeping your data private and secure.\\xa0Security Hub Gives Admins Precise Control Over Your Orgs Security\\xa0Enterprise Pro offers access to our Security Hub: a centralized admin command center that gives organizations complete control over how Perplexity is used across teams.\\xa0User ManagementAdmins use the Security Hub to give every team member the appropriate level of access within Perplexity. They can set permissions for:\\xa0File Uploads and Downloads: Choose who can upload or download documents AI-generated content to prevent unauthorized sharing or data leakage.Shared Content: Manage how team members create, share, and collaborate on Threads, Pages, and Spaces to keep internal knowledge organized and secure.Data Integrations and Connectors: Set permissions for who can connect to external, integrated data sources and cloud storage services like Google Drive, Microsoft OneDrive, and Sharepoint. Perplexity Connectors enable these data integrations so teams can connect external data sources directly to our AI engine. While Connectors boost productivity by making private data accessible, the Security Hub's permissions ensure only authorized users have that power.AI Model Enablement: Decide which large language models your team can use to align with internal security or compliance policies.Internal Knowledge Bases: Manage access to internal knowledge bases and file repositories, ensuring only approved users can browse connected files, documents, or datasets within your workspace.To close the User Management loop, Enterprise Pro combines Single Sign-On (SSO) with Multi-Factor Authentication (MFA) and short-lived session credentials. Data Privacy and RetentionBeyond User Management, Perplexity Enterprise Pro ensures your data is secure and private by keeping it out of reach of attackers and even third parties.\\xa0Third-Party Agreements: We maintain formal agreements with all third-party model providers to ensure they can’t access or use your data for training purposes. We review these agreements annually to stay compliant with evolving standards and requirements. The best part? Enterprise Pro (unlike every other model provider) does not use your data to improve or train its own systems.Connector Security: Whether you're connecting to a public data source like Crunchbase or syncing files through FactSet, all connectors fall under our platform’s strict privacy and security policies.Safe Use of DeepSeek R1: Rest assured that Enterprise Pro’s integration with DeepSeek R1 only uses the open weights version, which runs directly on Perplexity servers in the U.S. This means your queries stay within the Perplexity ecosystem and are never exposed to DeepSeek or any external infrastructure.Automatic Data Deletion: Perplexity automatically deletes files attached to threads after seven days. Only files uploaded to Spaces do not automatically expire, and these files (just like all data that we handle) are protected through enterprise-grade encryption both at rest and in transit. You can also enforce incognito mode to disable search history.\\xa0Audit logs: Enterprise Pro admins can receive real-time activity alerts such as file uploads and downloads into your workspace. This way, admins can immediately have increased visibility.Transparency: Review which service providers we work with and how your data flows through the platform at any time on our Trust Center. This gives your team full visibility into third-party relationships and ensures accountability at every step.A Robust Security Program That Operates 24/7Perplexity’s commitment to your security doesn’t stop there. Our security team also monitors for potential threats around the clock and responds in real time using automated workflows with detailed playbooks. Here's what they use to do it:Cloud security infrastructure: Perplexity Enterprise Pro runs on AWS, a secure cloud infrastructure with built-in physical security, network protection, and industry-leading compliance certifications. To strengthen our cloud security posture further, we also use Wiz to proactively identify risks across our environment.Vulnerability Disclosure Program (VDP): We created this to impose a safe, structured process for external security researchers and ethical hackers to report vulnerabilities. It covers all public-facing endpoints and rewards those who help us identify and fix potential gaps – like misconfigured access controls or exposed APIs – before they can be exploited.Short-lived authentication and just-in-time access: No engineer has access to production data by default. Getting access to production data is only granted in business critical cases, requires security approval, and is heavily monitored.\\xa0Bug Bounty Program: We reward vetted security researchers for reporting critical vulnerabilities across our ecosystem. Run through a private, invite-only program with Bugcrowd; it covers Perplexity’s web applications, APIs, and cloud services as another layer of proactive protection to keep your data safe.\\xa0Perplexity Enterprise Pro’s Secure, AI-Powered Answer EngineWith Perplexity Enterprise Pro being compliant with SOC2 Type II, organizations mitigate the threat of data leaks, black-box risks, and security compromises. You can have confidence in our AI answer engine with enterprise-grade controls, transparent data privacy policies, and full confidence in how your data is handled, so your team can get work done quickly without sacrificing security. You can view our report here.\\xa0Here’s how Perplexity Enterprise Pro stacks up against Perplexity offering:More than 7,000 Enterprise Pro customers include NVIDIA, Databricks, Stripe, and the U.S. Anti-Doping Agency (USADA) trust Perplexity. For USADA, data privacy isn’t a preference; it’s policy. “We implemented our first AI policy stating that no athlete data is ever allowed to be used on an AI platform,” said John Bobo, Chief Operating & Innovation Officer. “What really attracted me to the Perplexity Enterprise offering was the advanced security features.”Start searching securely and strategically with Perplexity Enterprise Pro today.At Perplexity, we take our customer’s data and security very seriously. Which is why we're proud to announce Perplexity is now SOC 2 Type II compliant. This independently validates our security practices and confirms that our platform meets the highest data protection standards. Alongside our adherence to GDPR and PCI DSS frameworks, Perplexity provides our enterprise customers with verified assurance that their sensitive information is handled with the highest standards of care and protection.With advanced privacy protections, admin controls, and compliance-ready infrastructure, Enterprise Pro provides your team with a powerful answer engine and deep research tools for the web and internal files while keeping your data private and secure.\\xa0Security Hub Gives Admins Precise Control Over Your Orgs Security\\xa0Enterprise Pro offers access to our Security Hub: a centralized admin command center that gives organizations complete control over how Perplexity is used across teams.\\xa0User ManagementAdmins use the Security Hub to give every team member the appropriate level of access within Perplexity. They can set permissions for:\\xa0File Uploads and Downloads: Choose who can upload or download documents AI-generated content to prevent unauthorized sharing or data leakage.Shared Content: Manage how team members create, share, and collaborate on Threads, Pages, and Spaces to keep internal knowledge organized and secure.Data Integrations and Connectors: Set permissions for who can connect to external, integrated data sources and cloud storage services like Google Drive, Microsoft OneDrive, and Sharepoint. Perplexity Connectors enable these data integrations so teams can connect external data sources directly to our AI engine. While Connectors boost productivity by making private data accessible, the Security Hub's permissions ensure only authorized users have that power.AI Model Enablement: Decide which large language models your team can use to align with internal security or compliance policies.Internal Knowledge Bases: Manage access to internal knowledge bases and file repositories, ensuring only approved users can browse connected files, documents, or datasets within your workspace.To close the User Management loop, Enterprise Pro combines Single Sign-On (SSO) with Multi-Factor Authentication (MFA) and short-lived session credentials. Data Privacy and RetentionBeyond User Management, Perplexity Enterprise Pro ensures your data is secure and private by keeping it out of reach of attackers and even third parties.\\xa0Third-Party Agreements: We maintain formal agreements with all third-party model providers to ensure they can’t access or use your data for training purposes. We review these agreements annually to stay compliant with evolving standards and requirements. The best part? Enterprise Pro (unlike every other model provider) does not use your data to improve or train its own systems.Connector Security: Whether you're connecting to a public data source like Crunchbase or syncing files through FactSet, all connectors fall under our platform’s strict privacy and security policies.Safe Use of DeepSeek R1: Rest assured that Enterprise Pro’s integration with DeepSeek R1 only uses the open weights version, which runs directly on Perplexity servers in the U.S. This means your queries stay within the Perplexity ecosystem and are never exposed to DeepSeek or any external infrastructure.Automatic Data Deletion: Perplexity automatically deletes files attached to threads after seven days. Only files uploaded to Spaces do not automatically expire, and these files (just like all data that we handle) are protected through enterprise-grade encryption both at rest and in transit. You can also enforce incognito mode to disable search history.\\xa0Audit logs: Enterprise Pro admins can receive real-time activity alerts such as file uploads and downloads into your workspace. This way, admins can immediately have increased visibility.Transparency: Review which service providers we work with and how your data flows through the platform at any time on our Trust Center. This gives your team full visibility into third-party relationships and ensures accountability at every step.A Robust Security Program That Operates 24/7Perplexity’s commitment to your security doesn’t stop there. Our security team also monitors for potential threats around the clock and responds in real time using automated workflows with detailed playbooks. Here's what they use to do it:Cloud security infrastructure: Perplexity Enterprise Pro runs on AWS, a secure cloud infrastructure with built-in physical security, network protection, and industry-leading compliance certifications. To strengthen our cloud security posture further, we also use Wiz to proactively identify risks across our environment.Vulnerability Disclosure Program (VDP): We created this to impose a safe, structured process for external security researchers and ethical hackers to report vulnerabilities. It covers all public-facing endpoints and rewards those who help us identify and fix potential gaps – like misconfigured access controls or exposed APIs – before they can be exploited.Short-lived authentication and just-in-time access: No engineer has access to production data by default. Getting access to production data is only granted in business critical cases, requires security approval, and is heavily monitored.\\xa0Bug Bounty Program: We reward vetted security researchers for reporting critical vulnerabilities across our ecosystem. Run through a private, invite-only program with Bugcrowd; it covers Perplexity’s web applications, APIs, and cloud services as another layer of proactive protection to keep your data safe.\\xa0Perplexity Enterprise Pro’s Secure, AI-Powered Answer EngineWith Perplexity Enterprise Pro being compliant with SOC2 Type II, organizations mitigate the threat of data leaks, black-box risks, and security compromises. You can have confidence in our AI answer engine with enterprise-grade controls, transparent data privacy policies, and full confidence in how your data is handled, so your team can get work done quickly without sacrificing security. You can view our report here.\\xa0Here’s how Perplexity Enterprise Pro stacks up against Perplexity offering:More than 7,000 Enterprise Pro customers include NVIDIA, Databricks, Stripe, and the U.S. Anti-Doping Agency (USADA) trust Perplexity. For USADA, data privacy isn’t a preference; it’s policy. “We implemented our first AI policy stating that no athlete data is ever allowed to be used on an AI platform,” said John Bobo, Chief Operating & Innovation Officer. “What really attracted me to the Perplexity Enterprise offering was the advanced security features.”Start searching securely and strategically with Perplexity Enterprise Pro today.\"),\n",
       " Document(metadata={'source': 'https://www.perplexity.ai/hub/blog/perplexity-sonar-dominates-new-search-arena-evolution'}, page_content=\"Perplexity Sonar Dominates New Search Arena EvaluationTL;DRCo #1 Rank:\\xa0Sonar-Reasoning-Pro-High achieved an Arena Score of 1136 (±21/−19), statistically tied for first place with Google's Gemini-2.5-Pro-Grounding (1142 +14/-17). In direct head-to-head battles, Sonar-Reasoning-Pro-High beat Gemini-2.5-Pro-Grounding 53% of the time.Sonar Dominance:\\xa0Perplexity models secured ranks 1 through 4, significantly outperforming other evaluated models from Google and OpenAI.Reasoning Advantage:\\xa0Models incorporating reasoning capabilities (sonar-reasoning-pro and sonar-reasoning) ranked higher, aligning with the general user preference observed for reasoning models (top 3 on the leaderboard).Depth of Search: Sonar models perform deeper search and consider more sources, on average citing 2-3x more sources than comparable Gemini models.______LM Arena just released their new Search Arena leaderboard comparing search-augmented LLM systems based on human preference. Perplexity's Sonar-Reasoning-Pro model has tied for first place with Gemini-2.5-Pro-Grounding with the rest of the Sonar models outperforming Gemini-2.0-Flash-Grounding and all of OpenAI’s web search models.Search Arena BenchmarkingUnlike SimpleQA's focus on narrow factual accuracy, LM Arena evaluates how models perform on real user queries across coding, writing, research, and recommendations. With Search Arena, evaluation focuses on current events and includes longer, more complex prompts, collecting over 10,000 human preference votes across 11 models. Between March 18 and April 13, 2025, Search Arena asked users to prompt and select which model response better satisfied their information needs.Sonar Model Performance ResultsPerplexity's Sonar models outperformed many of the top state of the art models including Gemini 2.0 Flash and GPT 4o Search. Our Sonar-Reasoning-Pro model achieved a score of 1136, statistically tied with Gemini-2.5-Pro-Grounding (1142) at the top position.In direct head-to-head battles, Sonar-Reasoning-Pro-High beat Gemini-2.5-Pro-Grounding 53% of the time.Search Arena's evaluation revealed three factors strongly correlating with human preference:Longer responses (coeff 0.255, p<0.05)Higher citation counts (coeff 0.234, p<0.05)Citations from community web sourcesThe leaderboard showed clear user preference for reasoning-enhanced models, with Sonar-Reasoning-Pro and Sonar-Reasoning taking two of the top three positions. Control experiments reinforced these findings, showing that controlling for citations caused model rankings to converge, suggesting search depth is a significant performance differentiator.Perplexity's Sonar models had substantially higher search depth, with ppl-sonar-pro-high citing 2-3x more sources than equivalent Gemini models.What This Means for UsersFor Perplexity users, these results confirm that Sonar models provide best-in-class accuracy, comprehensive source attribution, and high-quality responses across a wide range of topics.Perplexity Pro users can continue to benefit from these top-performing models by setting Sonar as their default model in settings. API users can access these capabilities through our Sonar API offerings with flexible search modes to balance performance and cost efficiencyWhile we're proud of this achievement, we remain focused on continuous improvement. The Search Arena evaluation provides valuable insights into user preferences that will inform our ongoing development efforts.Introduction to SonarJoin Perplexity co-founder & CTO, Denis Yarats, for an overview of our API on April 24 at 11am PT. Denis will provide an overview of Perplexity's APIs, share benchmark results, and API use cases.Register here.Get started with Sonar.Perplexity Sonar Dominates New Search Arena EvaluationTL;DRCo #1 Rank:\\xa0Sonar-Reasoning-Pro-High achieved an Arena Score of 1136 (±21/−19), statistically tied for first place with Google's Gemini-2.5-Pro-Grounding (1142 +14/-17). In direct head-to-head battles, Sonar-Reasoning-Pro-High beat Gemini-2.5-Pro-Grounding 53% of the time.Sonar Dominance:\\xa0Perplexity models secured ranks 1 through 4, significantly outperforming other evaluated models from Google and OpenAI.Reasoning Advantage:\\xa0Models incorporating reasoning capabilities (sonar-reasoning-pro and sonar-reasoning) ranked higher, aligning with the general user preference observed for reasoning models (top 3 on the leaderboard).Depth of Search: Sonar models perform deeper search and consider more sources, on average citing 2-3x more sources than comparable Gemini models.______LM Arena just released their new Search Arena leaderboard comparing search-augmented LLM systems based on human preference. Perplexity's Sonar-Reasoning-Pro model has tied for first place with Gemini-2.5-Pro-Grounding with the rest of the Sonar models outperforming Gemini-2.0-Flash-Grounding and all of OpenAI’s web search models.Search Arena BenchmarkingUnlike SimpleQA's focus on narrow factual accuracy, LM Arena evaluates how models perform on real user queries across coding, writing, research, and recommendations. With Search Arena, evaluation focuses on current events and includes longer, more complex prompts, collecting over 10,000 human preference votes across 11 models. Between March 18 and April 13, 2025, Search Arena asked users to prompt and select which model response better satisfied their information needs.Sonar Model Performance ResultsPerplexity's Sonar models outperformed many of the top state of the art models including Gemini 2.0 Flash and GPT 4o Search. Our Sonar-Reasoning-Pro model achieved a score of 1136, statistically tied with Gemini-2.5-Pro-Grounding (1142) at the top position.In direct head-to-head battles, Sonar-Reasoning-Pro-High beat Gemini-2.5-Pro-Grounding 53% of the time.Search Arena's evaluation revealed three factors strongly correlating with human preference:Longer responses (coeff 0.255, p<0.05)Higher citation counts (coeff 0.234, p<0.05)Citations from community web sourcesThe leaderboard showed clear user preference for reasoning-enhanced models, with Sonar-Reasoning-Pro and Sonar-Reasoning taking two of the top three positions. Control experiments reinforced these findings, showing that controlling for citations caused model rankings to converge, suggesting search depth is a significant performance differentiator.Perplexity's Sonar models had substantially higher search depth, with ppl-sonar-pro-high citing 2-3x more sources than equivalent Gemini models.What This Means for UsersFor Perplexity users, these results confirm that Sonar models provide best-in-class accuracy, comprehensive source attribution, and high-quality responses across a wide range of topics.Perplexity Pro users can continue to benefit from these top-performing models by setting Sonar as their default model in settings. API users can access these capabilities through our Sonar API offerings with flexible search modes to balance performance and cost efficiencyWhile we're proud of this achievement, we remain focused on continuous improvement. The Search Arena evaluation provides valuable insights into user preferences that will inform our ongoing development efforts.Introduction to SonarJoin Perplexity co-founder & CTO, Denis Yarats, for an overview of our API on April 24 at 11am PT. Denis will provide an overview of Perplexity's APIs, share benchmark results, and API use cases.Register here.Get started with Sonar.Perplexity Sonar Dominates New Search Arena EvaluationTL;DRCo #1 Rank:\\xa0Sonar-Reasoning-Pro-High achieved an Arena Score of 1136 (±21/−19), statistically tied for first place with Google's Gemini-2.5-Pro-Grounding (1142 +14/-17). In direct head-to-head battles, Sonar-Reasoning-Pro-High beat Gemini-2.5-Pro-Grounding 53% of the time.Sonar Dominance:\\xa0Perplexity models secured ranks 1 through 4, significantly outperforming other evaluated models from Google and OpenAI.Reasoning Advantage:\\xa0Models incorporating reasoning capabilities (sonar-reasoning-pro and sonar-reasoning) ranked higher, aligning with the general user preference observed for reasoning models (top 3 on the leaderboard).Depth of Search: Sonar models perform deeper search and consider more sources, on average citing 2-3x more sources than comparable Gemini models.______LM Arena just released their new Search Arena leaderboard comparing search-augmented LLM systems based on human preference. Perplexity's Sonar-Reasoning-Pro model has tied for first place with Gemini-2.5-Pro-Grounding with the rest of the Sonar models outperforming Gemini-2.0-Flash-Grounding and all of OpenAI’s web search models.Search Arena BenchmarkingUnlike SimpleQA's focus on narrow factual accuracy, LM Arena evaluates how models perform on real user queries across coding, writing, research, and recommendations. With Search Arena, evaluation focuses on current events and includes longer, more complex prompts, collecting over 10,000 human preference votes across 11 models. Between March 18 and April 13, 2025, Search Arena asked users to prompt and select which model response better satisfied their information needs.Sonar Model Performance ResultsPerplexity's Sonar models outperformed many of the top state of the art models including Gemini 2.0 Flash and GPT 4o Search. Our Sonar-Reasoning-Pro model achieved a score of 1136, statistically tied with Gemini-2.5-Pro-Grounding (1142) at the top position.In direct head-to-head battles, Sonar-Reasoning-Pro-High beat Gemini-2.5-Pro-Grounding 53% of the time.Search Arena's evaluation revealed three factors strongly correlating with human preference:Longer responses (coeff 0.255, p<0.05)Higher citation counts (coeff 0.234, p<0.05)Citations from community web sourcesThe leaderboard showed clear user preference for reasoning-enhanced models, with Sonar-Reasoning-Pro and Sonar-Reasoning taking two of the top three positions. Control experiments reinforced these findings, showing that controlling for citations caused model rankings to converge, suggesting search depth is a significant performance differentiator.Perplexity's Sonar models had substantially higher search depth, with ppl-sonar-pro-high citing 2-3x more sources than equivalent Gemini models.What This Means for UsersFor Perplexity users, these results confirm that Sonar models provide best-in-class accuracy, comprehensive source attribution, and high-quality responses across a wide range of topics.Perplexity Pro users can continue to benefit from these top-performing models by setting Sonar as their default model in settings. API users can access these capabilities through our Sonar API offerings with flexible search modes to balance performance and cost efficiencyWhile we're proud of this achievement, we remain focused on continuous improvement. The Search Arena evaluation provides valuable insights into user preferences that will inform our ongoing development efforts.Introduction to SonarJoin Perplexity co-founder & CTO, Denis Yarats, for an overview of our API on April 24 at 11am PT. Denis will provide an overview of Perplexity's APIs, share benchmark results, and API use cases.Register here.Get started with Sonar.\"),\n",
       " Document(metadata={'source': 'https://www.perplexity.ai/hub/blog/efficient-and-portable-mixture-of-experts-communication'}, page_content='Efficient and Portable Mixture-of-Experts CommunicationAn overview of portable Mixture-of-Experts (MoE) communication, focusing on optimizing GPU parallelism and reducing latency in large-scale AI modelsWe present a high-performance, portable, open-source library for Mixture-of-Experts (MoE) communication that achieves 10x faster performance compared to standard All-to-All communication primitives.Our implementation features several key technical innovations that deliver superior MoE communication efficiency:GPU-initiated communication (IBGDA): Supports direct GPU-to-NIC communication, significantly reducing latency by bypassing CPU involvementCommunication and computation overlap: Split kernel architecture with separate send and receive stages enables computation to proceed concurrently with network transfersFastest single-node performance: 2.5x lower latency than the previously fastest implementation on single-node configurationsEfficient and portable multi-node performance: Our implementation achieves speeds up to 10x faster than standard all-to-all communication. Although approximately 2x slower than highly specialized implementations, our approach offers better portability across NVSHMEM versions and network environments (NVLink, CX-7, and EFA)The library is fully open-source and available at https://github.com/ppl-ai/pplx-kernels.In this article, we explore the challenges of expert parallelism in large-scale MoE models and describe our approach to efficient token dispatch and combination across distributed GPU environments.IntroductionMixture-of-Experts (MoE) models, such as DeepSeek R1 and Mixtral 8x7B, improve upon dense models by limiting the number of weights that are activated for each token. Instead of achieving high parameter counts by increasing the number of layers or increasing the size of the linear layers in the Multi-Layer Perceptron (MLP) of each decoder layer, MoE models replace the traditional MLP with multiple experts and a router. For example, out of the 671B parameters of DeepSeek R1, only 37B are multiplied with a given token during inference. This greatly reduces the amount of computation required to decode a token and improves latency compared to a dense model.MoE models present some additional challenges for inference, compared to dense models. While the experts themselves are small MLP layers, each decoder layer includes a router that decides which experts a token is dispatched to, with each token being dispatched to multiple experts. The router is typically a small linear layer producing a probability distribution. Usually the experts with the top-K scores are picked and the final activation is computed as a weighted average, summing the expert outputs multiplied by a weight derived from the probability distribution of the router.To minimize latency in distributed MoE systems, parallelism can be exploited across multiple devices, but this introduces communication challenges. Models such as Mixtral 8x7B or Llama-70B fit within 8 devices across a single node, benefiting from fast and low-latency NVLink interconnections (up to 900Gbps). However, larger models require sharding experts across multiple nodes using InfiniBand (peaking at 400Gbps), which introduces additional latency challenges.In this article, we explore the problem of expert parallelism, describing our implementation of GPU kernels that efficiently dispatch tokens to devices holding the assigned experts and collect them back by computing the weighted sum of expert outputs. While aggressively optimized implementations targeting specific network hardware exist, we present a highly portable implementation that relies on a minimal set of NVSHMEM primitives yet delivers performance 10x faster than standard all-to-all communication. Our implementation achieves state-of-the-art performance on single-node configurations while maintaining excellent portability across various network environments.Sharding and ParallelismFor efficient inference, the weights of a model must be held in device memory, while also leaving sufficient space for activations, KV caches and other buffers required by the forward pass through the model. The most capable models exceed the capacity of even the most capable GPUs, thus inference must be spread across multiple devices, which can collectively store the weights of the model. Based on the sharding schemes of weights, different communication and computation schemes must be used to offload computation and synchronize devices.Expert Parallelism (EP), illustrated in the first figure, only parallelizes the expert computation. Different experts are assigned to different devices, which hold their weights. After routing, tokens are sent to the corresponding device, with the results being gathered and accumulated afterwards. The complexity of routing depends on the degree of parallelism in the other parts of the model: replicating other layers could eliminate the need for routing altogether, as each rank can select the tokens from a locally replicated routing table. However, if only one of the rank run routing, a broadcast is required to dispatch tokens, indices and weights to their respective experts. Finally, an all-gather or an all-to-all broadcast synchronizes the output tokens with whichever rank continues the execution of the model. Such an implementation is relatively simple as torch already exposes the required primitives, albeit some benefits could be gained by fusing accumulation with the gather operation collecting the tokens from the experts.Expert-only parallelism does not scale ideally, as nodes in a cluster might be idle while the model is running non-expert layers, such as attention, norm and sampling. However, the computation of these layers, primarily attention, can also benefit from Tensor Parallelism (TP). Most models rely on multi-head attention, meaning that the attention heads and their corresponding Q, K and V projections can also be sharded across devices, replicating or gathering the slices between various layers. If attention is spread across all devices, an all-gather can synchronize the activations, allowing routing to be replicated, requiring synchronization primitives similar to the expert parallelism case for an efficient implementation. However, there are limits to parallelism at this level, as reducing the number of attention heads below a certain threshold will yield diminishing returns.To best utilize all devices and support a very high degree of expert parallelism, of up to 128 or 256 GPUs, Data Parallelism (DP) is required. Under this scheme, the devices a model is split across are grouped to handle requests concurrently, computing attention and maintaining KV caches sharded across their local group. Multiple instances of these parallel groups collaborate on expert evaluation, with each hosting a different subset of the expert weights. Based on the number of attention heads, a group may typically span up to the size of an entire node, as intra-node communication is faster. For example, in the figure above, one DP rank independently services two requests, A and B, handling attention, norm and any other bookkeeping for the requests. The other DP rank processes a distinct request C. However, the first node hosts half of the experts, while the other node the other half, thus after routing tokens from A and B might be sent to the second node and vice-versa. This leads to a sparse communication problem: each device might send a different number of tokens to any other destination rank. Existing primitives from torch, primarily all_to_all, are not particularly well suited, as they might require some form of padding or GPU-to-CPU synchronization and metadata broadcast. To implement communication effectively, custom kernels are required to process the routing information and initiate communication from the devices without CPU intervention. After routing, a dispatch kernel must send tokens to the ranks they were routed to, while on the combine side the activations belonging to the requests in the current DP group must be collected. Additionally, work must be balanced within a DP group which may have multiple devices, in order to minimize broadcast and correctly replicate the tokens on all ranks for the subsequent layers.NVSHMEMNVSHMEM is an NVIDIA-specific OpenSHMEM implementation, providing portable inter-device communication facilities that abstract away the complexity of the underlying hardware. The API can express device-to-device reads and writes, which are mapped to the primitives of individual transport layers. Both NVLink and RDMA are supported, granting a degree of portability and allowing NVSHMEM kernels to operate within a node or across a cluster of multiple nodes. In our kernels, we use remote memory writes and signaling operations in order to transfer data and synchronize the participating devices.NVSHMEM operations are built around the concept of symmetric memory: they operate on buffers which have been allocated on all the devices participating in inter-device communication. A local device can use an address derived within its own local buffer alongside the index of the target rank to specify the destination of an operation. The figure below illustrates this concept: both GPUs allocate symmetric buffers of the same size, retaining src pointers to them. The first rank wants to send 3 integers to the second one, placing them at the start of the buffer: nvshmem_int_put_nbi derives the start address from the local buffer, specifying the target device. The second rank derives an offset from its own buffer, sending one element to the first device, offsetting by one. While destination addresses must always be symmetric buffers allocated using nvshmem_alloc, source buffers can be arbitrary regions of device memory, provided they are pre-registered with NVSHMEM.While NVSHMEM provides a wide range of primitives, our kernels rely on only 3 functions, building all synchronization and fencing upon them.nvshmemx_putmem_signal_nbi_warp: Transfers a block of data from one device to another, while also setting a flag on the remote device. The operation either sets (NVSHMEM_SIGNAL_SET) or increments (NVSHMEM_SIGNAL_ADD) a 64-bit location. The flag is updated after the entire block of memory is transferred. If the remote device observed a change in the flag, it can safely access the buffer in its own memory. This function is useful for coalescing data transfer and synchronization.nvshmemx_signal_op operates on a single memory location, typically a 64-bit flag, atomically setting or incrementing it. It is useful in sending over metadata and synchronizing devices.nvshmem_uint64_wait_until is used on the receiving end of a signal, to poll a flag until the remote updates it.Ensuring any form of ordering between operations through nvshmem_quiet or nvshmem_fence is expensive, thus our implementation of the kernels avoids the use of any other NVSHMEM primitives and minimizes any dependencies other than individual send-receive pairs. When sending data, we always use the non-blocking version of functions, without waiting for the data to be even sent out of the local rank. Other, implicit synchronization across the group of kernels ensures that these buffers are not destructively overwritten while the NICs need access to them.NVSHMEM uses GPUDirect RDMA so that the NIC can directly read and write GPU memory for data exchange without involving CPUs. Furthermore, on ConnectX NICs, NVSHEMEM supports GPU-initiated communication (also known as Infiniband GPUDirect Async, or IBGDA), which allows GPU to post RDMA operations directly to NIC and to poll completion queue of NIC directly. On platforms that does not support GPU-initiated communication, NVSHMEM spawns a “proxy” thread on host CPU to initiate communication and poll completion on behalf of the GPU. NVSHMEM program is portable regardless of whether GPU-initiated communication is supported or not. However, GPU-initiated communication significantly cuts latency because it completely bypasses the detour to CPU.Portable KernelsWe implement MoE communication through a pair of dispatch and combine kernels. The dispatch kernels are responsible for reading tokens and routing information on each rank, dispatching them to the appropriate experts. The combine kernels collect the activations produced by the experts and send them back to their source ranks, while also computing the weighted average from the selected experts based on the weights computed by the router. The kernels are further split into a send and receive component, in order to allow data transfers to be overlapped with computation. The send kernels are all non-blocking and non-synchronizing: they simply dispatch all the writes to the remotes. On the other end, the combine kernels only read from memory, waiting until all required data has been transferred. After dispatching work to the NICs, while the data is transferred asynchronously over the wire, the GPUs can do other useful work locally, such as applying shared experts or computing attention.Each pair of kernels has its own symmetric memory buffers, which each rank allocating private storage to receive data from all other ranks. The maximal number of tokens that can be dispatched from a DP group is fixed across the ranks, which also sets the upper bound each rank can receive for each local expert from each DP rank. This allows sender ranks to derive a unique address on the destination rank to write to, without requiring any synchronization among them. After data is received, the kernels shuffle the tokens around to lay them out in memory in the optimal format required by the receiving kernels. While the buffers have a sizable dimension, they are re-used across all sequential layers of a model.The only form of global synchronization, as illustrated in the figure above, is implemented in the combine-receive kernel. Once data is dispatched from send, a rank returns and continues execution when it receives all the tokens it requires from the other ranks and enters the combine send kernels as soon as they are processed. The barrier in combine-receive ensures that no rank can run ahead and start dispatch-send while any other rank is still waiting to receive data, potentially causing destructive overlapping. Synchronization is done in the combine kernel for simplicity, as an optimization the same barrier could be implementing between the dispatch receive and the combine send kernels, further reducing latency by allowing the NICs more time to settle the transfers while the GPU is running other kernels.Both kernels are split across all available SMs of the devices: while the dispatch send and combine receive kernels must parallelize across a per-rank maximum token count (max_m), the dispatch receive and combine send kernels must handle that many tokens from each rank and local expert, for a maximum of max_m * (num_experts // EP) * (EP // TP).DispatchThe dispatch kernel is responsible for dispatching tokens and block scaling factors to peers which host the experts the tokens were routed to. The sender side relies on warp specialization to parallelize two tasks: aggregating routing information to determine how many tokens are sent from each rank and packing the tokens into messages to be dispatched to the remote. The receiver side first waits for all the token counts to be received, then it reads and unpacks the payloads from shared memory into the tensors that will be passed on to the downstream Grouped GEMM kernels. It also stores information into buffers shared with the combine kernels, to indicate where each token should be sent back. This information is required as the receive kernel shuffles tokens around in contiguous buffers in a non-deterministic order.In the sender part, a single warp of 32 threads on each block is responsible for reading the routing information and counting the number of tokens each destination expert is expected to receive from this rank, sending the count plus one using nvshmem_signal_op. The count is incremented by one, as the transition from zero to non-zero on the remote end signifies the receipt of the counts from a source rank. In parallel, the remaining warps co-operate to copy tokens into symmetric memory across all blocks in parallel, packing activations, scaling factors and their index on the local rank into a contiguous chunk. The index is required by the combine sender to determine the address where the token will be written to. Next, after ensuring all the data has been copied through a barrier across the warp groups, the warps yet again operate independently, each sending the same buffer to a different expert in paralle. The tokens are sent using nvshmemx_putmem_signal_nbi_warp, which also atomically increases the count of sent tokens from the local rank on the remote device. Within a DP group, since each rank ows a replica of the token to be sent, dispatch is balanced evenly, with each device sending out a subset of the tokens.On the receive end, all kernels first synchronize with the sender ranks by waiting for the total token counts to be sent over. Afterwards, they all wait for the atomically incremented sent token counts to settle to the total counts, indicating that all the payloads from the source ranks have also been sent over, thanks to the semantics of the putmem call. The kernels poll on the counts using nvshmem_uint64_wait_until, parallelizing the operation across all blocks and threads. Subsequently, a cross-block barrier ensures that no block reads the buffers unless all data has been correctly received. Spread across blocks and synchronized via an atomically incremented counter, the tokens are copied from symmetric memory into regular contiguous buffers which become the inputs to the downstream kernels implementing the experts. The source rank, expert index and token index are stored separately, exactly pinpointing the location where the combine kernel has to send the activations. Even though tokens from within a DP group are sent from different devices, they are all grouped together to be passed on to the corresponding expert.CombineThe combine kernels are yet again split into send and receiver halves: the senders copy the un-quantized 16-bit activations over to the remote devices, with the receive kernels waiting for the data to be sent over and computing the weighted average of all expert contributions locally. Additionally, they also act as a barrier, to synchronize the dispatch-combine sequence: each rank sets a flag on each peer on entry to the send kernels, with the receive kernels not being allowed to return unless they observe the flag being set. The latency of synchronization is minimal, as it overlaps with the actual communication and computation.On the sender side, the kernels traverse the list of tokens assigned to all local experts in parallel, writing them to a buffer on the destination rank. The target rank, expert index and token index are read from the per-token buffers populated by the scater kernels upon the receipt of the tokens. Each sender has its own private memory region per expert to write to, as indicated in the figure above, avoiding the need to synchronize. Similarly to dispatch, combine atomically increments per token counters on the destination rank to indicate the receipt of the data: when the counter matches the number of experts a token was dispatched to, the token contents can be accessed.In the receive kernel, the list of tokens is traversed in parallel across multiple blocks, waiting for their contents to arrive by polling the flag set by the signalling operation. Upon arrival, the payloads are read from the private buffers, with the routing table indicating which buffer to read from and what weight to assign to each expert. The results are then written to externally allocated tensors, with the kernel finishing execution once all devices passed the barrier. Across a DP group, all ranks receive a copy of each expert activation to compute their own replicas.BenchmarksWe evaluate our kernels on a cluster of 128 GPUs spread across 16 nodes connected via InfiniBand and CX-7 NICs. We try both GPUDirect Async (IBGDA) and Reliable Connection (RC) with a CPU proxy. We compare them to the highly optimized DeepSeek implementation, as well as the dense primitives provided by PyTorch (through NCCL) or NVSHMEM.Although on the dense NVSHMEM all-to-all operation, the performance of IBRC and IBGDA is similar (6378 µs vs 6180 µs), IBGDA is significantly faster with the sparse kernels. Adding up Dispatch and Combine, IBGDA uses 902 µs whereas IBRC takes 3223 µs - a 3.6x improvement in latency. While all-to-all is bandwidth bound, the sparse kernels broadcast orders of magnitudes less data, being bound by latency. By triggering network transfers directly from a GPU, without requiring a CPU proxy to coordinate the GPU and the NIC, end-to-end latency is significantly reduced.While our portable kernels are about 2x slower than the highly optimized DeepSeek kernels, they improve latency by 10x compared to the dense kernels (902 µs vs 9944 µs).Additionally, the split into sender-receiver components also allow some of the latency to be hidden away, unlike the library primitives.On single-node (EP8), NVSHMEM utilizes NVLINK for transportation, delivering lower latency and higher throughput than inter-node networking. Our portable kernels are about 2.5x faster than DeepEP on single-node (186µs vs 481 µs).Further ImprovementsThe kernels described here outperform the built-in primitives of ML frameworks and offer decent performance without over-specializing for particular inter-device transports, such as NVLink or InfiniBand. Besides the already-mentioned opportunities, further performance gains are attainable by replacing the communication primitives with more specialized versions. For example, across NVLink, the use of symmetric memory could be replaced with buffers shared across the devices, eliminating some copying and allowing for finer-grained synchronizations schemes, across individual tokens instead of token batches. Across InfiniBand, an implementation could access the underlying queue pairs directly, eliminating the need to transfer both the expected and sent token counts and flushing tokens from the queues into output buffers as they arrive. However, such implementations come at the cost of portability, whilst our implementations allows clusters with specific configurations to be evaluated before committing development effort to optimize for particular hardware.ConclusionWe have presented a high-performance, portable library for MoE communication that achieves 10x faster performance compared to standard all-to-all communication while maintaining compatibility across diverse hardware configurations. On single-node deployments with NVLink, our solution demonstrates 2.5x lower latency than previous implementations.Our approach balances performance with portability through key innovations including GPU-initiated communication support, a split kernel architecture enabling computation-communication overlap, and efficient token dispatch using minimal NVSHMEM primitives. While approximately 2x slower than highly specialized implementations on multi-node setups, our library offers superior flexibility across various network environments (NVLink, CX-7, and EFA).As MoE models continue to scale, efficient communication strategies like ours will become increasingly important for practical deployment. Our fully open-source implementation is available at https://github.com/ppl-ai/pplx-kernels.ReferencesDeepEPDeepSeek-V3 Technical ReportNVIDIA NVSHMEMEfficient and Portable Mixture-of-Experts CommunicationAn overview of portable Mixture-of-Experts (MoE) communication, focusing on optimizing GPU parallelism and reducing latency in large-scale AI modelsWe present a high-performance, portable, open-source library for Mixture-of-Experts (MoE) communication that achieves 10x faster performance compared to standard All-to-All communication primitives.Our implementation features several key technical innovations that deliver superior MoE communication efficiency:GPU-initiated communication (IBGDA): Supports direct GPU-to-NIC communication, significantly reducing latency by bypassing CPU involvementCommunication and computation overlap: Split kernel architecture with separate send and receive stages enables computation to proceed concurrently with network transfersFastest single-node performance: 2.5x lower latency than the previously fastest implementation on single-node configurationsEfficient and portable multi-node performance: Our implementation achieves speeds up to 10x faster than standard all-to-all communication. Although approximately 2x slower than highly specialized implementations, our approach offers better portability across NVSHMEM versions and network environments (NVLink, CX-7, and EFA)The library is fully open-source and available at https://github.com/ppl-ai/pplx-kernels.In this article, we explore the challenges of expert parallelism in large-scale MoE models and describe our approach to efficient token dispatch and combination across distributed GPU environments.IntroductionMixture-of-Experts (MoE) models, such as DeepSeek R1 and Mixtral 8x7B, improve upon dense models by limiting the number of weights that are activated for each token. Instead of achieving high parameter counts by increasing the number of layers or increasing the size of the linear layers in the Multi-Layer Perceptron (MLP) of each decoder layer, MoE models replace the traditional MLP with multiple experts and a router. For example, out of the 671B parameters of DeepSeek R1, only 37B are multiplied with a given token during inference. This greatly reduces the amount of computation required to decode a token and improves latency compared to a dense model.MoE models present some additional challenges for inference, compared to dense models. While the experts themselves are small MLP layers, each decoder layer includes a router that decides which experts a token is dispatched to, with each token being dispatched to multiple experts. The router is typically a small linear layer producing a probability distribution. Usually the experts with the top-K scores are picked and the final activation is computed as a weighted average, summing the expert outputs multiplied by a weight derived from the probability distribution of the router.To minimize latency in distributed MoE systems, parallelism can be exploited across multiple devices, but this introduces communication challenges. Models such as Mixtral 8x7B or Llama-70B fit within 8 devices across a single node, benefiting from fast and low-latency NVLink interconnections (up to 900Gbps). However, larger models require sharding experts across multiple nodes using InfiniBand (peaking at 400Gbps), which introduces additional latency challenges.In this article, we explore the problem of expert parallelism, describing our implementation of GPU kernels that efficiently dispatch tokens to devices holding the assigned experts and collect them back by computing the weighted sum of expert outputs. While aggressively optimized implementations targeting specific network hardware exist, we present a highly portable implementation that relies on a minimal set of NVSHMEM primitives yet delivers performance 10x faster than standard all-to-all communication. Our implementation achieves state-of-the-art performance on single-node configurations while maintaining excellent portability across various network environments.Sharding and ParallelismFor efficient inference, the weights of a model must be held in device memory, while also leaving sufficient space for activations, KV caches and other buffers required by the forward pass through the model. The most capable models exceed the capacity of even the most capable GPUs, thus inference must be spread across multiple devices, which can collectively store the weights of the model. Based on the sharding schemes of weights, different communication and computation schemes must be used to offload computation and synchronize devices.Expert Parallelism (EP), illustrated in the first figure, only parallelizes the expert computation. Different experts are assigned to different devices, which hold their weights. After routing, tokens are sent to the corresponding device, with the results being gathered and accumulated afterwards. The complexity of routing depends on the degree of parallelism in the other parts of the model: replicating other layers could eliminate the need for routing altogether, as each rank can select the tokens from a locally replicated routing table. However, if only one of the rank run routing, a broadcast is required to dispatch tokens, indices and weights to their respective experts. Finally, an all-gather or an all-to-all broadcast synchronizes the output tokens with whichever rank continues the execution of the model. Such an implementation is relatively simple as torch already exposes the required primitives, albeit some benefits could be gained by fusing accumulation with the gather operation collecting the tokens from the experts.Expert-only parallelism does not scale ideally, as nodes in a cluster might be idle while the model is running non-expert layers, such as attention, norm and sampling. However, the computation of these layers, primarily attention, can also benefit from Tensor Parallelism (TP). Most models rely on multi-head attention, meaning that the attention heads and their corresponding Q, K and V projections can also be sharded across devices, replicating or gathering the slices between various layers. If attention is spread across all devices, an all-gather can synchronize the activations, allowing routing to be replicated, requiring synchronization primitives similar to the expert parallelism case for an efficient implementation. However, there are limits to parallelism at this level, as reducing the number of attention heads below a certain threshold will yield diminishing returns.To best utilize all devices and support a very high degree of expert parallelism, of up to 128 or 256 GPUs, Data Parallelism (DP) is required. Under this scheme, the devices a model is split across are grouped to handle requests concurrently, computing attention and maintaining KV caches sharded across their local group. Multiple instances of these parallel groups collaborate on expert evaluation, with each hosting a different subset of the expert weights. Based on the number of attention heads, a group may typically span up to the size of an entire node, as intra-node communication is faster. For example, in the figure above, one DP rank independently services two requests, A and B, handling attention, norm and any other bookkeeping for the requests. The other DP rank processes a distinct request C. However, the first node hosts half of the experts, while the other node the other half, thus after routing tokens from A and B might be sent to the second node and vice-versa. This leads to a sparse communication problem: each device might send a different number of tokens to any other destination rank. Existing primitives from torch, primarily all_to_all, are not particularly well suited, as they might require some form of padding or GPU-to-CPU synchronization and metadata broadcast. To implement communication effectively, custom kernels are required to process the routing information and initiate communication from the devices without CPU intervention. After routing, a dispatch kernel must send tokens to the ranks they were routed to, while on the combine side the activations belonging to the requests in the current DP group must be collected. Additionally, work must be balanced within a DP group which may have multiple devices, in order to minimize broadcast and correctly replicate the tokens on all ranks for the subsequent layers.NVSHMEMNVSHMEM is an NVIDIA-specific OpenSHMEM implementation, providing portable inter-device communication facilities that abstract away the complexity of the underlying hardware. The API can express device-to-device reads and writes, which are mapped to the primitives of individual transport layers. Both NVLink and RDMA are supported, granting a degree of portability and allowing NVSHMEM kernels to operate within a node or across a cluster of multiple nodes. In our kernels, we use remote memory writes and signaling operations in order to transfer data and synchronize the participating devices.NVSHMEM operations are built around the concept of symmetric memory: they operate on buffers which have been allocated on all the devices participating in inter-device communication. A local device can use an address derived within its own local buffer alongside the index of the target rank to specify the destination of an operation. The figure below illustrates this concept: both GPUs allocate symmetric buffers of the same size, retaining src pointers to them. The first rank wants to send 3 integers to the second one, placing them at the start of the buffer: nvshmem_int_put_nbi derives the start address from the local buffer, specifying the target device. The second rank derives an offset from its own buffer, sending one element to the first device, offsetting by one. While destination addresses must always be symmetric buffers allocated using nvshmem_alloc, source buffers can be arbitrary regions of device memory, provided they are pre-registered with NVSHMEM.While NVSHMEM provides a wide range of primitives, our kernels rely on only 3 functions, building all synchronization and fencing upon them.nvshmemx_putmem_signal_nbi_warp: Transfers a block of data from one device to another, while also setting a flag on the remote device. The operation either sets (NVSHMEM_SIGNAL_SET) or increments (NVSHMEM_SIGNAL_ADD) a 64-bit location. The flag is updated after the entire block of memory is transferred. If the remote device observed a change in the flag, it can safely access the buffer in its own memory. This function is useful for coalescing data transfer and synchronization.nvshmemx_signal_op operates on a single memory location, typically a 64-bit flag, atomically setting or incrementing it. It is useful in sending over metadata and synchronizing devices.nvshmem_uint64_wait_until is used on the receiving end of a signal, to poll a flag until the remote updates it.Ensuring any form of ordering between operations through nvshmem_quiet or nvshmem_fence is expensive, thus our implementation of the kernels avoids the use of any other NVSHMEM primitives and minimizes any dependencies other than individual send-receive pairs. When sending data, we always use the non-blocking version of functions, without waiting for the data to be even sent out of the local rank. Other, implicit synchronization across the group of kernels ensures that these buffers are not destructively overwritten while the NICs need access to them.NVSHMEM uses GPUDirect RDMA so that the NIC can directly read and write GPU memory for data exchange without involving CPUs. Furthermore, on ConnectX NICs, NVSHEMEM supports GPU-initiated communication (also known as Infiniband GPUDirect Async, or IBGDA), which allows GPU to post RDMA operations directly to NIC and to poll completion queue of NIC directly. On platforms that does not support GPU-initiated communication, NVSHMEM spawns a “proxy” thread on host CPU to initiate communication and poll completion on behalf of the GPU. NVSHMEM program is portable regardless of whether GPU-initiated communication is supported or not. However, GPU-initiated communication significantly cuts latency because it completely bypasses the detour to CPU.Portable KernelsWe implement MoE communication through a pair of dispatch and combine kernels. The dispatch kernels are responsible for reading tokens and routing information on each rank, dispatching them to the appropriate experts. The combine kernels collect the activations produced by the experts and send them back to their source ranks, while also computing the weighted average from the selected experts based on the weights computed by the router. The kernels are further split into a send and receive component, in order to allow data transfers to be overlapped with computation. The send kernels are all non-blocking and non-synchronizing: they simply dispatch all the writes to the remotes. On the other end, the combine kernels only read from memory, waiting until all required data has been transferred. After dispatching work to the NICs, while the data is transferred asynchronously over the wire, the GPUs can do other useful work locally, such as applying shared experts or computing attention.Each pair of kernels has its own symmetric memory buffers, which each rank allocating private storage to receive data from all other ranks. The maximal number of tokens that can be dispatched from a DP group is fixed across the ranks, which also sets the upper bound each rank can receive for each local expert from each DP rank. This allows sender ranks to derive a unique address on the destination rank to write to, without requiring any synchronization among them. After data is received, the kernels shuffle the tokens around to lay them out in memory in the optimal format required by the receiving kernels. While the buffers have a sizable dimension, they are re-used across all sequential layers of a model.The only form of global synchronization, as illustrated in the figure above, is implemented in the combine-receive kernel. Once data is dispatched from send, a rank returns and continues execution when it receives all the tokens it requires from the other ranks and enters the combine send kernels as soon as they are processed. The barrier in combine-receive ensures that no rank can run ahead and start dispatch-send while any other rank is still waiting to receive data, potentially causing destructive overlapping. Synchronization is done in the combine kernel for simplicity, as an optimization the same barrier could be implementing between the dispatch receive and the combine send kernels, further reducing latency by allowing the NICs more time to settle the transfers while the GPU is running other kernels.Both kernels are split across all available SMs of the devices: while the dispatch send and combine receive kernels must parallelize across a per-rank maximum token count (max_m), the dispatch receive and combine send kernels must handle that many tokens from each rank and local expert, for a maximum of max_m * (num_experts // EP) * (EP // TP).DispatchThe dispatch kernel is responsible for dispatching tokens and block scaling factors to peers which host the experts the tokens were routed to. The sender side relies on warp specialization to parallelize two tasks: aggregating routing information to determine how many tokens are sent from each rank and packing the tokens into messages to be dispatched to the remote. The receiver side first waits for all the token counts to be received, then it reads and unpacks the payloads from shared memory into the tensors that will be passed on to the downstream Grouped GEMM kernels. It also stores information into buffers shared with the combine kernels, to indicate where each token should be sent back. This information is required as the receive kernel shuffles tokens around in contiguous buffers in a non-deterministic order.In the sender part, a single warp of 32 threads on each block is responsible for reading the routing information and counting the number of tokens each destination expert is expected to receive from this rank, sending the count plus one using nvshmem_signal_op. The count is incremented by one, as the transition from zero to non-zero on the remote end signifies the receipt of the counts from a source rank. In parallel, the remaining warps co-operate to copy tokens into symmetric memory across all blocks in parallel, packing activations, scaling factors and their index on the local rank into a contiguous chunk. The index is required by the combine sender to determine the address where the token will be written to. Next, after ensuring all the data has been copied through a barrier across the warp groups, the warps yet again operate independently, each sending the same buffer to a different expert in paralle. The tokens are sent using nvshmemx_putmem_signal_nbi_warp, which also atomically increases the count of sent tokens from the local rank on the remote device. Within a DP group, since each rank ows a replica of the token to be sent, dispatch is balanced evenly, with each device sending out a subset of the tokens.On the receive end, all kernels first synchronize with the sender ranks by waiting for the total token counts to be sent over. Afterwards, they all wait for the atomically incremented sent token counts to settle to the total counts, indicating that all the payloads from the source ranks have also been sent over, thanks to the semantics of the putmem call. The kernels poll on the counts using nvshmem_uint64_wait_until, parallelizing the operation across all blocks and threads. Subsequently, a cross-block barrier ensures that no block reads the buffers unless all data has been correctly received. Spread across blocks and synchronized via an atomically incremented counter, the tokens are copied from symmetric memory into regular contiguous buffers which become the inputs to the downstream kernels implementing the experts. The source rank, expert index and token index are stored separately, exactly pinpointing the location where the combine kernel has to send the activations. Even though tokens from within a DP group are sent from different devices, they are all grouped together to be passed on to the corresponding expert.CombineThe combine kernels are yet again split into send and receiver halves: the senders copy the un-quantized 16-bit activations over to the remote devices, with the receive kernels waiting for the data to be sent over and computing the weighted average of all expert contributions locally. Additionally, they also act as a barrier, to synchronize the dispatch-combine sequence: each rank sets a flag on each peer on entry to the send kernels, with the receive kernels not being allowed to return unless they observe the flag being set. The latency of synchronization is minimal, as it overlaps with the actual communication and computation.On the sender side, the kernels traverse the list of tokens assigned to all local experts in parallel, writing them to a buffer on the destination rank. The target rank, expert index and token index are read from the per-token buffers populated by the scater kernels upon the receipt of the tokens. Each sender has its own private memory region per expert to write to, as indicated in the figure above, avoiding the need to synchronize. Similarly to dispatch, combine atomically increments per token counters on the destination rank to indicate the receipt of the data: when the counter matches the number of experts a token was dispatched to, the token contents can be accessed.In the receive kernel, the list of tokens is traversed in parallel across multiple blocks, waiting for their contents to arrive by polling the flag set by the signalling operation. Upon arrival, the payloads are read from the private buffers, with the routing table indicating which buffer to read from and what weight to assign to each expert. The results are then written to externally allocated tensors, with the kernel finishing execution once all devices passed the barrier. Across a DP group, all ranks receive a copy of each expert activation to compute their own replicas.BenchmarksWe evaluate our kernels on a cluster of 128 GPUs spread across 16 nodes connected via InfiniBand and CX-7 NICs. We try both GPUDirect Async (IBGDA) and Reliable Connection (RC) with a CPU proxy. We compare them to the highly optimized DeepSeek implementation, as well as the dense primitives provided by PyTorch (through NCCL) or NVSHMEM.Although on the dense NVSHMEM all-to-all operation, the performance of IBRC and IBGDA is similar (6378 µs vs 6180 µs), IBGDA is significantly faster with the sparse kernels. Adding up Dispatch and Combine, IBGDA uses 902 µs whereas IBRC takes 3223 µs - a 3.6x improvement in latency. While all-to-all is bandwidth bound, the sparse kernels broadcast orders of magnitudes less data, being bound by latency. By triggering network transfers directly from a GPU, without requiring a CPU proxy to coordinate the GPU and the NIC, end-to-end latency is significantly reduced.While our portable kernels are about 2x slower than the highly optimized DeepSeek kernels, they improve latency by 10x compared to the dense kernels (902 µs vs 9944 µs).Additionally, the split into sender-receiver components also allow some of the latency to be hidden away, unlike the library primitives.On single-node (EP8), NVSHMEM utilizes NVLINK for transportation, delivering lower latency and higher throughput than inter-node networking. Our portable kernels are about 2.5x faster than DeepEP on single-node (186µs vs 481 µs).Further ImprovementsThe kernels described here outperform the built-in primitives of ML frameworks and offer decent performance without over-specializing for particular inter-device transports, such as NVLink or InfiniBand. Besides the already-mentioned opportunities, further performance gains are attainable by replacing the communication primitives with more specialized versions. For example, across NVLink, the use of symmetric memory could be replaced with buffers shared across the devices, eliminating some copying and allowing for finer-grained synchronizations schemes, across individual tokens instead of token batches. Across InfiniBand, an implementation could access the underlying queue pairs directly, eliminating the need to transfer both the expected and sent token counts and flushing tokens from the queues into output buffers as they arrive. However, such implementations come at the cost of portability, whilst our implementations allows clusters with specific configurations to be evaluated before committing development effort to optimize for particular hardware.ConclusionWe have presented a high-performance, portable library for MoE communication that achieves 10x faster performance compared to standard all-to-all communication while maintaining compatibility across diverse hardware configurations. On single-node deployments with NVLink, our solution demonstrates 2.5x lower latency than previous implementations.Our approach balances performance with portability through key innovations including GPU-initiated communication support, a split kernel architecture enabling computation-communication overlap, and efficient token dispatch using minimal NVSHMEM primitives. While approximately 2x slower than highly specialized implementations on multi-node setups, our library offers superior flexibility across various network environments (NVLink, CX-7, and EFA).As MoE models continue to scale, efficient communication strategies like ours will become increasingly important for practical deployment. Our fully open-source implementation is available at https://github.com/ppl-ai/pplx-kernels.ReferencesDeepEPDeepSeek-V3 Technical ReportNVIDIA NVSHMEMEfficient and Portable Mixture-of-Experts CommunicationAn overview of portable Mixture-of-Experts (MoE) communication, focusing on optimizing GPU parallelism and reducing latency in large-scale AI modelsWe present a high-performance, portable, open-source library for Mixture-of-Experts (MoE) communication that achieves 10x faster performance compared to standard All-to-All communication primitives.Our implementation features several key technical innovations that deliver superior MoE communication efficiency:GPU-initiated communication (IBGDA): Supports direct GPU-to-NIC communication, significantly reducing latency by bypassing CPU involvementCommunication and computation overlap: Split kernel architecture with separate send and receive stages enables computation to proceed concurrently with network transfersFastest single-node performance: 2.5x lower latency than the previously fastest implementation on single-node configurationsEfficient and portable multi-node performance: Our implementation achieves speeds up to 10x faster than standard all-to-all communication. Although approximately 2x slower than highly specialized implementations, our approach offers better portability across NVSHMEM versions and network environments (NVLink, CX-7, and EFA)The library is fully open-source and available at https://github.com/ppl-ai/pplx-kernels.In this article, we explore the challenges of expert parallelism in large-scale MoE models and describe our approach to efficient token dispatch and combination across distributed GPU environments.IntroductionMixture-of-Experts (MoE) models, such as DeepSeek R1 and Mixtral 8x7B, improve upon dense models by limiting the number of weights that are activated for each token. Instead of achieving high parameter counts by increasing the number of layers or increasing the size of the linear layers in the Multi-Layer Perceptron (MLP) of each decoder layer, MoE models replace the traditional MLP with multiple experts and a router. For example, out of the 671B parameters of DeepSeek R1, only 37B are multiplied with a given token during inference. This greatly reduces the amount of computation required to decode a token and improves latency compared to a dense model.MoE models present some additional challenges for inference, compared to dense models. While the experts themselves are small MLP layers, each decoder layer includes a router that decides which experts a token is dispatched to, with each token being dispatched to multiple experts. The router is typically a small linear layer producing a probability distribution. Usually the experts with the top-K scores are picked and the final activation is computed as a weighted average, summing the expert outputs multiplied by a weight derived from the probability distribution of the router.To minimize latency in distributed MoE systems, parallelism can be exploited across multiple devices, but this introduces communication challenges. Models such as Mixtral 8x7B or Llama-70B fit within 8 devices across a single node, benefiting from fast and low-latency NVLink interconnections (up to 900Gbps). However, larger models require sharding experts across multiple nodes using InfiniBand (peaking at 400Gbps), which introduces additional latency challenges.In this article, we explore the problem of expert parallelism, describing our implementation of GPU kernels that efficiently dispatch tokens to devices holding the assigned experts and collect them back by computing the weighted sum of expert outputs. While aggressively optimized implementations targeting specific network hardware exist, we present a highly portable implementation that relies on a minimal set of NVSHMEM primitives yet delivers performance 10x faster than standard all-to-all communication. Our implementation achieves state-of-the-art performance on single-node configurations while maintaining excellent portability across various network environments.Sharding and ParallelismFor efficient inference, the weights of a model must be held in device memory, while also leaving sufficient space for activations, KV caches and other buffers required by the forward pass through the model. The most capable models exceed the capacity of even the most capable GPUs, thus inference must be spread across multiple devices, which can collectively store the weights of the model. Based on the sharding schemes of weights, different communication and computation schemes must be used to offload computation and synchronize devices.Expert Parallelism (EP), illustrated in the first figure, only parallelizes the expert computation. Different experts are assigned to different devices, which hold their weights. After routing, tokens are sent to the corresponding device, with the results being gathered and accumulated afterwards. The complexity of routing depends on the degree of parallelism in the other parts of the model: replicating other layers could eliminate the need for routing altogether, as each rank can select the tokens from a locally replicated routing table. However, if only one of the rank run routing, a broadcast is required to dispatch tokens, indices and weights to their respective experts. Finally, an all-gather or an all-to-all broadcast synchronizes the output tokens with whichever rank continues the execution of the model. Such an implementation is relatively simple as torch already exposes the required primitives, albeit some benefits could be gained by fusing accumulation with the gather operation collecting the tokens from the experts.Expert-only parallelism does not scale ideally, as nodes in a cluster might be idle while the model is running non-expert layers, such as attention, norm and sampling. However, the computation of these layers, primarily attention, can also benefit from Tensor Parallelism (TP). Most models rely on multi-head attention, meaning that the attention heads and their corresponding Q, K and V projections can also be sharded across devices, replicating or gathering the slices between various layers. If attention is spread across all devices, an all-gather can synchronize the activations, allowing routing to be replicated, requiring synchronization primitives similar to the expert parallelism case for an efficient implementation. However, there are limits to parallelism at this level, as reducing the number of attention heads below a certain threshold will yield diminishing returns.To best utilize all devices and support a very high degree of expert parallelism, of up to 128 or 256 GPUs, Data Parallelism (DP) is required. Under this scheme, the devices a model is split across are grouped to handle requests concurrently, computing attention and maintaining KV caches sharded across their local group. Multiple instances of these parallel groups collaborate on expert evaluation, with each hosting a different subset of the expert weights. Based on the number of attention heads, a group may typically span up to the size of an entire node, as intra-node communication is faster. For example, in the figure above, one DP rank independently services two requests, A and B, handling attention, norm and any other bookkeeping for the requests. The other DP rank processes a distinct request C. However, the first node hosts half of the experts, while the other node the other half, thus after routing tokens from A and B might be sent to the second node and vice-versa. This leads to a sparse communication problem: each device might send a different number of tokens to any other destination rank. Existing primitives from torch, primarily all_to_all, are not particularly well suited, as they might require some form of padding or GPU-to-CPU synchronization and metadata broadcast. To implement communication effectively, custom kernels are required to process the routing information and initiate communication from the devices without CPU intervention. After routing, a dispatch kernel must send tokens to the ranks they were routed to, while on the combine side the activations belonging to the requests in the current DP group must be collected. Additionally, work must be balanced within a DP group which may have multiple devices, in order to minimize broadcast and correctly replicate the tokens on all ranks for the subsequent layers.NVSHMEMNVSHMEM is an NVIDIA-specific OpenSHMEM implementation, providing portable inter-device communication facilities that abstract away the complexity of the underlying hardware. The API can express device-to-device reads and writes, which are mapped to the primitives of individual transport layers. Both NVLink and RDMA are supported, granting a degree of portability and allowing NVSHMEM kernels to operate within a node or across a cluster of multiple nodes. In our kernels, we use remote memory writes and signaling operations in order to transfer data and synchronize the participating devices.NVSHMEM operations are built around the concept of symmetric memory: they operate on buffers which have been allocated on all the devices participating in inter-device communication. A local device can use an address derived within its own local buffer alongside the index of the target rank to specify the destination of an operation. The figure below illustrates this concept: both GPUs allocate symmetric buffers of the same size, retaining src pointers to them. The first rank wants to send 3 integers to the second one, placing them at the start of the buffer: nvshmem_int_put_nbi derives the start address from the local buffer, specifying the target device. The second rank derives an offset from its own buffer, sending one element to the first device, offsetting by one. While destination addresses must always be symmetric buffers allocated using nvshmem_alloc, source buffers can be arbitrary regions of device memory, provided they are pre-registered with NVSHMEM.While NVSHMEM provides a wide range of primitives, our kernels rely on only 3 functions, building all synchronization and fencing upon them.nvshmemx_putmem_signal_nbi_warp: Transfers a block of data from one device to another, while also setting a flag on the remote device. The operation either sets (NVSHMEM_SIGNAL_SET) or increments (NVSHMEM_SIGNAL_ADD) a 64-bit location. The flag is updated after the entire block of memory is transferred. If the remote device observed a change in the flag, it can safely access the buffer in its own memory. This function is useful for coalescing data transfer and synchronization.nvshmemx_signal_op operates on a single memory location, typically a 64-bit flag, atomically setting or incrementing it. It is useful in sending over metadata and synchronizing devices.nvshmem_uint64_wait_until is used on the receiving end of a signal, to poll a flag until the remote updates it.Ensuring any form of ordering between operations through nvshmem_quiet or nvshmem_fence is expensive, thus our implementation of the kernels avoids the use of any other NVSHMEM primitives and minimizes any dependencies other than individual send-receive pairs. When sending data, we always use the non-blocking version of functions, without waiting for the data to be even sent out of the local rank. Other, implicit synchronization across the group of kernels ensures that these buffers are not destructively overwritten while the NICs need access to them.NVSHMEM uses GPUDirect RDMA so that the NIC can directly read and write GPU memory for data exchange without involving CPUs. Furthermore, on ConnectX NICs, NVSHEMEM supports GPU-initiated communication (also known as Infiniband GPUDirect Async, or IBGDA), which allows GPU to post RDMA operations directly to NIC and to poll completion queue of NIC directly. On platforms that does not support GPU-initiated communication, NVSHMEM spawns a “proxy” thread on host CPU to initiate communication and poll completion on behalf of the GPU. NVSHMEM program is portable regardless of whether GPU-initiated communication is supported or not. However, GPU-initiated communication significantly cuts latency because it completely bypasses the detour to CPU.Portable KernelsWe implement MoE communication through a pair of dispatch and combine kernels. The dispatch kernels are responsible for reading tokens and routing information on each rank, dispatching them to the appropriate experts. The combine kernels collect the activations produced by the experts and send them back to their source ranks, while also computing the weighted average from the selected experts based on the weights computed by the router. The kernels are further split into a send and receive component, in order to allow data transfers to be overlapped with computation. The send kernels are all non-blocking and non-synchronizing: they simply dispatch all the writes to the remotes. On the other end, the combine kernels only read from memory, waiting until all required data has been transferred. After dispatching work to the NICs, while the data is transferred asynchronously over the wire, the GPUs can do other useful work locally, such as applying shared experts or computing attention.Each pair of kernels has its own symmetric memory buffers, which each rank allocating private storage to receive data from all other ranks. The maximal number of tokens that can be dispatched from a DP group is fixed across the ranks, which also sets the upper bound each rank can receive for each local expert from each DP rank. This allows sender ranks to derive a unique address on the destination rank to write to, without requiring any synchronization among them. After data is received, the kernels shuffle the tokens around to lay them out in memory in the optimal format required by the receiving kernels. While the buffers have a sizable dimension, they are re-used across all sequential layers of a model.The only form of global synchronization, as illustrated in the figure above, is implemented in the combine-receive kernel. Once data is dispatched from send, a rank returns and continues execution when it receives all the tokens it requires from the other ranks and enters the combine send kernels as soon as they are processed. The barrier in combine-receive ensures that no rank can run ahead and start dispatch-send while any other rank is still waiting to receive data, potentially causing destructive overlapping. Synchronization is done in the combine kernel for simplicity, as an optimization the same barrier could be implementing between the dispatch receive and the combine send kernels, further reducing latency by allowing the NICs more time to settle the transfers while the GPU is running other kernels.Both kernels are split across all available SMs of the devices: while the dispatch send and combine receive kernels must parallelize across a per-rank maximum token count (max_m), the dispatch receive and combine send kernels must handle that many tokens from each rank and local expert, for a maximum of max_m * (num_experts // EP) * (EP // TP).DispatchThe dispatch kernel is responsible for dispatching tokens and block scaling factors to peers which host the experts the tokens were routed to. The sender side relies on warp specialization to parallelize two tasks: aggregating routing information to determine how many tokens are sent from each rank and packing the tokens into messages to be dispatched to the remote. The receiver side first waits for all the token counts to be received, then it reads and unpacks the payloads from shared memory into the tensors that will be passed on to the downstream Grouped GEMM kernels. It also stores information into buffers shared with the combine kernels, to indicate where each token should be sent back. This information is required as the receive kernel shuffles tokens around in contiguous buffers in a non-deterministic order.In the sender part, a single warp of 32 threads on each block is responsible for reading the routing information and counting the number of tokens each destination expert is expected to receive from this rank, sending the count plus one using nvshmem_signal_op. The count is incremented by one, as the transition from zero to non-zero on the remote end signifies the receipt of the counts from a source rank. In parallel, the remaining warps co-operate to copy tokens into symmetric memory across all blocks in parallel, packing activations, scaling factors and their index on the local rank into a contiguous chunk. The index is required by the combine sender to determine the address where the token will be written to. Next, after ensuring all the data has been copied through a barrier across the warp groups, the warps yet again operate independently, each sending the same buffer to a different expert in paralle. The tokens are sent using nvshmemx_putmem_signal_nbi_warp, which also atomically increases the count of sent tokens from the local rank on the remote device. Within a DP group, since each rank ows a replica of the token to be sent, dispatch is balanced evenly, with each device sending out a subset of the tokens.On the receive end, all kernels first synchronize with the sender ranks by waiting for the total token counts to be sent over. Afterwards, they all wait for the atomically incremented sent token counts to settle to the total counts, indicating that all the payloads from the source ranks have also been sent over, thanks to the semantics of the putmem call. The kernels poll on the counts using nvshmem_uint64_wait_until, parallelizing the operation across all blocks and threads. Subsequently, a cross-block barrier ensures that no block reads the buffers unless all data has been correctly received. Spread across blocks and synchronized via an atomically incremented counter, the tokens are copied from symmetric memory into regular contiguous buffers which become the inputs to the downstream kernels implementing the experts. The source rank, expert index and token index are stored separately, exactly pinpointing the location where the combine kernel has to send the activations. Even though tokens from within a DP group are sent from different devices, they are all grouped together to be passed on to the corresponding expert.CombineThe combine kernels are yet again split into send and receiver halves: the senders copy the un-quantized 16-bit activations over to the remote devices, with the receive kernels waiting for the data to be sent over and computing the weighted average of all expert contributions locally. Additionally, they also act as a barrier, to synchronize the dispatch-combine sequence: each rank sets a flag on each peer on entry to the send kernels, with the receive kernels not being allowed to return unless they observe the flag being set. The latency of synchronization is minimal, as it overlaps with the actual communication and computation.On the sender side, the kernels traverse the list of tokens assigned to all local experts in parallel, writing them to a buffer on the destination rank. The target rank, expert index and token index are read from the per-token buffers populated by the scater kernels upon the receipt of the tokens. Each sender has its own private memory region per expert to write to, as indicated in the figure above, avoiding the need to synchronize. Similarly to dispatch, combine atomically increments per token counters on the destination rank to indicate the receipt of the data: when the counter matches the number of experts a token was dispatched to, the token contents can be accessed.In the receive kernel, the list of tokens is traversed in parallel across multiple blocks, waiting for their contents to arrive by polling the flag set by the signalling operation. Upon arrival, the payloads are read from the private buffers, with the routing table indicating which buffer to read from and what weight to assign to each expert. The results are then written to externally allocated tensors, with the kernel finishing execution once all devices passed the barrier. Across a DP group, all ranks receive a copy of each expert activation to compute their own replicas.BenchmarksWe evaluate our kernels on a cluster of 128 GPUs spread across 16 nodes connected via InfiniBand and CX-7 NICs. We try both GPUDirect Async (IBGDA) and Reliable Connection (RC) with a CPU proxy. We compare them to the highly optimized DeepSeek implementation, as well as the dense primitives provided by PyTorch (through NCCL) or NVSHMEM.Although on the dense NVSHMEM all-to-all operation, the performance of IBRC and IBGDA is similar (6378 µs vs 6180 µs), IBGDA is significantly faster with the sparse kernels. Adding up Dispatch and Combine, IBGDA uses 902 µs whereas IBRC takes 3223 µs - a 3.6x improvement in latency. While all-to-all is bandwidth bound, the sparse kernels broadcast orders of magnitudes less data, being bound by latency. By triggering network transfers directly from a GPU, without requiring a CPU proxy to coordinate the GPU and the NIC, end-to-end latency is significantly reduced.While our portable kernels are about 2x slower than the highly optimized DeepSeek kernels, they improve latency by 10x compared to the dense kernels (902 µs vs 9944 µs).Additionally, the split into sender-receiver components also allow some of the latency to be hidden away, unlike the library primitives.On single-node (EP8), NVSHMEM utilizes NVLINK for transportation, delivering lower latency and higher throughput than inter-node networking. Our portable kernels are about 2.5x faster than DeepEP on single-node (186µs vs 481 µs).Further ImprovementsThe kernels described here outperform the built-in primitives of ML frameworks and offer decent performance without over-specializing for particular inter-device transports, such as NVLink or InfiniBand. Besides the already-mentioned opportunities, further performance gains are attainable by replacing the communication primitives with more specialized versions. For example, across NVLink, the use of symmetric memory could be replaced with buffers shared across the devices, eliminating some copying and allowing for finer-grained synchronizations schemes, across individual tokens instead of token batches. Across InfiniBand, an implementation could access the underlying queue pairs directly, eliminating the need to transfer both the expected and sent token counts and flushing tokens from the queues into output buffers as they arrive. However, such implementations come at the cost of portability, whilst our implementations allows clusters with specific configurations to be evaluated before committing development effort to optimize for particular hardware.ConclusionWe have presented a high-performance, portable library for MoE communication that achieves 10x faster performance compared to standard all-to-all communication while maintaining compatibility across diverse hardware configurations. On single-node deployments with NVLink, our solution demonstrates 2.5x lower latency than previous implementations.Our approach balances performance with portability through key innovations including GPU-initiated communication support, a split kernel architecture enabling computation-communication overlap, and efficient token dispatch using minimal NVSHMEM primitives. While approximately 2x slower than highly specialized implementations on multi-node setups, our library offers superior flexibility across various network environments (NVLink, CX-7, and EFA).As MoE models continue to scale, efficient communication strategies like ours will become increasingly important for practical deployment. Our fully open-source implementation is available at https://github.com/ppl-ai/pplx-kernels.ReferencesDeepEPDeepSeek-V3 Technical ReportNVIDIA NVSHMEM'),\n",
       " Document(metadata={'source': 'https://www.perplexity.ai/hub/blog/rebuilding-tiktok-in-america'}, page_content='A Vision for Rebuilding TikTok in AmericaAt Perplexity, our mission has always been to serve the world\\'s curiosity through accurate, transparent, and accessible information. Since our founding back in 2022 by a group of AI researchers, our focus has been on becoming the best answer engine in the world.TikTok has emerged as one of the most significant platforms for authentic creative expression and content discovery, connecting millions of users worldwide. And we think Perplexity can make TikTok even better.We’re excited to share the future we envision: a TikTok that is more useful for everyone, where users have easy-to-use tools that let them go deep and seek truth, powered by the best answer engine in the world.Building a Transparent Algorithm in AmericaPerplexity is singularly positioned to rebuild the TikTok algorithm without creating a monopoly, combining world-class technical capabilities with Little Tech independence. Any acquisition by a consortium of investors could in effect keep ByteDance in control of the algorithm, while any acquisition by a competitor would likely create a monopoly in the short form video and information space. All of society benefits when content feeds are liberated from the manipulations of foreign governments and globalist monopolists.TikTok’s \"For You\" feed is personalized to each user and building a real-time recommendation system is vital to keeping a short video platform fresh and fun to use. Perplexity would start by building these basic systems to ensure users maintain a seamless experience. This infrastructure would be developed and maintained in American data centers with American oversight, ensuring alignment with domestic privacy standards and regulations.The TikTok algorithm today is a black box. We believe these recommendation systems should be transparent. To eliminate risks of user manipulation, we propose rebuilding TikTok’s algorithm from the ground up with transparency as the guiding principle. Our promise is to turn TikTok into the most neutral and trusted platform in the world. To achieve this, we commit not only to developing a new algorithm but also to making the TikTok For You feed open source.Upgrading to World-Class AI infrastructureThe advanced AI infrastructure run by Perplexity, powered by Nvidia Dynamo (announced at the 2025 Nvidia GPU Technology Conference and mentioning Perplexity as a favorite partner), could easily scale TikTok\\'s recommender models 100x while achieving faster inference speed, raising the bar on TikTok’s recommendation system and making it the best in the world.Enhancing Trust in TikTok with Perplexity’s Citations and Community Note FeaturesAll answers on Perplexity include citations, which we believe is fundamental to creating a trusted information ecosystem. As a first step, Perplexity would immediately extend this capability to TikTok videos, making it easy for users to cross-reference information in realtime as they watch videos.We could develop the most powerful context system in the world. One that leverages both community feedback and AI capabilities to highlight reliable information. This approach would be similar to our @AskPerplexity account on X, which has gained over 130K followers and provides contextual information to thousands of users daily. By bringing similar capabilities to TikTok, we could ensure that users have access to world class information tools while scrolling their \"For You\" feed.Creating the Best “search box” in the WorldCombining Perplexity’s answer engine with TikTok’s extensive video library would allow us to build the best search experience in the world, providing both TikTok and Perplexity users to the answers they seek, anywhere, anytime, no matter the medium.Enhancing TikTok search with Perplexity\\'s answer engine: This would provide users with comprehensive, well-cited answers that combine the best answer engine in the world with one of the largest libraries of user generated content.Bringing TikTok videos to Perplexity: At the same time, Perplexity’s answers would be enriched with TikTok videos to bring the relevant pieces of content into pro, reasoning and deep research.Whether you’re searching for the best tennis racket under $100, trying to understand the impact of an earnings call on a stock, or want to know the places where locals like to eat — we’ll always focus on getting you to the best result possible.Making TikTok Useful for EveryoneBeyond building the infrastructure, our first priority would be enhancing the informational value of content that users discover on TikTok. We believe that time spent on the platform should feel worthwhile, with metrics and optimization focused on user satisfaction and informational value rather than solely engagement.For users who choose to connect their Perplexity and TikTok accounts, we could leverage cross-platform signals to improve personalization. Perplexity\\'s understanding of user interests through questions and spaces could help surface more relevant content on TikTok. Similarly, content preferences on TikTok could inform more personalized answers on Perplexity.Enriching Content with AIPerplexity\\'s LLM orchestration can transform the TikTok experience by enriching videos with contextual information and even multilingual capabilities. Through automatic translation and annotation, creators could easily expand their reach to global audiences.For users, this would deliver both breadth and depth—providing access to more diverse content while enabling deeper exploration of topics that capture their curiosity. Imagine watching a video about quantum physics and being able to initiate an in-depth research query directly from that content. This integration would allow users to seamlessly transition between TikTok\\'s scrolling experience and Perplexity\\'s powerful research capabilities—all within a single, unified feed.From rebuilding an open-source \"For You\" algorithm to enhancing personalization with the latest AI infrastructure, we’re excited about the possibilities TikTok could achieve. Combining Perplexity’s expertise in delivering accurate, trustworthy answers with TikTok’s vibrant, creative community and extensive video library, would be a win for everyone. A TikTok rebuilt and powered by Perplexity would create the world\\'s greatest platform for creativity and knowledge discovery.A Vision for Rebuilding TikTok in AmericaAt Perplexity, our mission has always been to serve the world\\'s curiosity through accurate, transparent, and accessible information. Since our founding back in 2022 by a group of AI researchers, our focus has been on becoming the best answer engine in the world.TikTok has emerged as one of the most significant platforms for authentic creative expression and content discovery, connecting millions of users worldwide. And we think Perplexity can make TikTok even better.We’re excited to share the future we envision: a TikTok that is more useful for everyone, where users have easy-to-use tools that let them go deep and seek truth, powered by the best answer engine in the world.Building a Transparent Algorithm in AmericaPerplexity is singularly positioned to rebuild the TikTok algorithm without creating a monopoly, combining world-class technical capabilities with Little Tech independence. Any acquisition by a consortium of investors could in effect keep ByteDance in control of the algorithm, while any acquisition by a competitor would likely create a monopoly in the short form video and information space. All of society benefits when content feeds are liberated from the manipulations of foreign governments and globalist monopolists.TikTok’s \"For You\" feed is personalized to each user and building a real-time recommendation system is vital to keeping a short video platform fresh and fun to use. Perplexity would start by building these basic systems to ensure users maintain a seamless experience. This infrastructure would be developed and maintained in American data centers with American oversight, ensuring alignment with domestic privacy standards and regulations.The TikTok algorithm today is a black box. We believe these recommendation systems should be transparent. To eliminate risks of user manipulation, we propose rebuilding TikTok’s algorithm from the ground up with transparency as the guiding principle. Our promise is to turn TikTok into the most neutral and trusted platform in the world. To achieve this, we commit not only to developing a new algorithm but also to making the TikTok For You feed open source.Upgrading to World-Class AI infrastructureThe advanced AI infrastructure run by Perplexity, powered by Nvidia Dynamo (announced at the 2025 Nvidia GPU Technology Conference and mentioning Perplexity as a favorite partner), could easily scale TikTok\\'s recommender models 100x while achieving faster inference speed, raising the bar on TikTok’s recommendation system and making it the best in the world.Enhancing Trust in TikTok with Perplexity’s Citations and Community Note FeaturesAll answers on Perplexity include citations, which we believe is fundamental to creating a trusted information ecosystem. As a first step, Perplexity would immediately extend this capability to TikTok videos, making it easy for users to cross-reference information in realtime as they watch videos.We could develop the most powerful context system in the world. One that leverages both community feedback and AI capabilities to highlight reliable information. This approach would be similar to our @AskPerplexity account on X, which has gained over 130K followers and provides contextual information to thousands of users daily. By bringing similar capabilities to TikTok, we could ensure that users have access to world class information tools while scrolling their \"For You\" feed.Creating the Best “search box” in the WorldCombining Perplexity’s answer engine with TikTok’s extensive video library would allow us to build the best search experience in the world, providing both TikTok and Perplexity users to the answers they seek, anywhere, anytime, no matter the medium.Enhancing TikTok search with Perplexity\\'s answer engine: This would provide users with comprehensive, well-cited answers that combine the best answer engine in the world with one of the largest libraries of user generated content.Bringing TikTok videos to Perplexity: At the same time, Perplexity’s answers would be enriched with TikTok videos to bring the relevant pieces of content into pro, reasoning and deep research.Whether you’re searching for the best tennis racket under $100, trying to understand the impact of an earnings call on a stock, or want to know the places where locals like to eat — we’ll always focus on getting you to the best result possible.Making TikTok Useful for EveryoneBeyond building the infrastructure, our first priority would be enhancing the informational value of content that users discover on TikTok. We believe that time spent on the platform should feel worthwhile, with metrics and optimization focused on user satisfaction and informational value rather than solely engagement.For users who choose to connect their Perplexity and TikTok accounts, we could leverage cross-platform signals to improve personalization. Perplexity\\'s understanding of user interests through questions and spaces could help surface more relevant content on TikTok. Similarly, content preferences on TikTok could inform more personalized answers on Perplexity.Enriching Content with AIPerplexity\\'s LLM orchestration can transform the TikTok experience by enriching videos with contextual information and even multilingual capabilities. Through automatic translation and annotation, creators could easily expand their reach to global audiences.For users, this would deliver both breadth and depth—providing access to more diverse content while enabling deeper exploration of topics that capture their curiosity. Imagine watching a video about quantum physics and being able to initiate an in-depth research query directly from that content. This integration would allow users to seamlessly transition between TikTok\\'s scrolling experience and Perplexity\\'s powerful research capabilities—all within a single, unified feed.From rebuilding an open-source \"For You\" algorithm to enhancing personalization with the latest AI infrastructure, we’re excited about the possibilities TikTok could achieve. Combining Perplexity’s expertise in delivering accurate, trustworthy answers with TikTok’s vibrant, creative community and extensive video library, would be a win for everyone. A TikTok rebuilt and powered by Perplexity would create the world\\'s greatest platform for creativity and knowledge discovery.A Vision for Rebuilding TikTok in AmericaAt Perplexity, our mission has always been to serve the world\\'s curiosity through accurate, transparent, and accessible information. Since our founding back in 2022 by a group of AI researchers, our focus has been on becoming the best answer engine in the world.TikTok has emerged as one of the most significant platforms for authentic creative expression and content discovery, connecting millions of users worldwide. And we think Perplexity can make TikTok even better.We’re excited to share the future we envision: a TikTok that is more useful for everyone, where users have easy-to-use tools that let them go deep and seek truth, powered by the best answer engine in the world.Building a Transparent Algorithm in AmericaPerplexity is singularly positioned to rebuild the TikTok algorithm without creating a monopoly, combining world-class technical capabilities with Little Tech independence. Any acquisition by a consortium of investors could in effect keep ByteDance in control of the algorithm, while any acquisition by a competitor would likely create a monopoly in the short form video and information space. All of society benefits when content feeds are liberated from the manipulations of foreign governments and globalist monopolists.TikTok’s \"For You\" feed is personalized to each user and building a real-time recommendation system is vital to keeping a short video platform fresh and fun to use. Perplexity would start by building these basic systems to ensure users maintain a seamless experience. This infrastructure would be developed and maintained in American data centers with American oversight, ensuring alignment with domestic privacy standards and regulations.The TikTok algorithm today is a black box. We believe these recommendation systems should be transparent. To eliminate risks of user manipulation, we propose rebuilding TikTok’s algorithm from the ground up with transparency as the guiding principle. Our promise is to turn TikTok into the most neutral and trusted platform in the world. To achieve this, we commit not only to developing a new algorithm but also to making the TikTok For You feed open source.Upgrading to World-Class AI infrastructureThe advanced AI infrastructure run by Perplexity, powered by Nvidia Dynamo (announced at the 2025 Nvidia GPU Technology Conference and mentioning Perplexity as a favorite partner), could easily scale TikTok\\'s recommender models 100x while achieving faster inference speed, raising the bar on TikTok’s recommendation system and making it the best in the world.Enhancing Trust in TikTok with Perplexity’s Citations and Community Note FeaturesAll answers on Perplexity include citations, which we believe is fundamental to creating a trusted information ecosystem. As a first step, Perplexity would immediately extend this capability to TikTok videos, making it easy for users to cross-reference information in realtime as they watch videos.We could develop the most powerful context system in the world. One that leverages both community feedback and AI capabilities to highlight reliable information. This approach would be similar to our @AskPerplexity account on X, which has gained over 130K followers and provides contextual information to thousands of users daily. By bringing similar capabilities to TikTok, we could ensure that users have access to world class information tools while scrolling their \"For You\" feed.Creating the Best “search box” in the WorldCombining Perplexity’s answer engine with TikTok’s extensive video library would allow us to build the best search experience in the world, providing both TikTok and Perplexity users to the answers they seek, anywhere, anytime, no matter the medium.Enhancing TikTok search with Perplexity\\'s answer engine: This would provide users with comprehensive, well-cited answers that combine the best answer engine in the world with one of the largest libraries of user generated content.Bringing TikTok videos to Perplexity: At the same time, Perplexity’s answers would be enriched with TikTok videos to bring the relevant pieces of content into pro, reasoning and deep research.Whether you’re searching for the best tennis racket under $100, trying to understand the impact of an earnings call on a stock, or want to know the places where locals like to eat — we’ll always focus on getting you to the best result possible.Making TikTok Useful for EveryoneBeyond building the infrastructure, our first priority would be enhancing the informational value of content that users discover on TikTok. We believe that time spent on the platform should feel worthwhile, with metrics and optimization focused on user satisfaction and informational value rather than solely engagement.For users who choose to connect their Perplexity and TikTok accounts, we could leverage cross-platform signals to improve personalization. Perplexity\\'s understanding of user interests through questions and spaces could help surface more relevant content on TikTok. Similarly, content preferences on TikTok could inform more personalized answers on Perplexity.Enriching Content with AIPerplexity\\'s LLM orchestration can transform the TikTok experience by enriching videos with contextual information and even multilingual capabilities. Through automatic translation and annotation, creators could easily expand their reach to global audiences.For users, this would deliver both breadth and depth—providing access to more diverse content while enabling deeper exploration of topics that capture their curiosity. Imagine watching a video about quantum physics and being able to initiate an in-depth research query directly from that content. This integration would allow users to seamlessly transition between TikTok\\'s scrolling experience and Perplexity\\'s powerful research capabilities—all within a single, unified feed.From rebuilding an open-source \"For You\" algorithm to enhancing personalization with the latest AI infrastructure, we’re excited about the possibilities TikTok could achieve. Combining Perplexity’s expertise in delivering accurate, trustworthy answers with TikTok’s vibrant, creative community and extensive video library, would be a win for everyone. A TikTok rebuilt and powered by Perplexity would create the world\\'s greatest platform for creativity and knowledge discovery.'),\n",
       " Document(metadata={'source': 'https://www.perplexity.ai/hub/blog/new-sonar-search-modes-outperform-openai-in-cost-and-performance'}, page_content=\"Improved Sonar Models: Industry Leading Performance at Lower CostsWe're excited to announce significant improvements to our Sonar models that deliver what matters most to you: superior performance at lower costs. Our latest benchmark testing confirms that Sonar and Sonar Pro now outperform leading competitors, like search enabled GPT-4o, while maintaining significantly more affordable pricing. Through comprehensive testing and user feedback, we've developed a tiered approach that allows you to balance performance and cost efficiency without sacrificing the factual accuracy that makes Sonar stand out.The improvements include a simplified billing structure, enhanced search capabilities, and the flexibility to optimize for both cost and performance based on your specific needs.What this looks like at a glance:Three new search modes: Choose between High, Medium, and Low modes to match your needs for pricing and performance across all models except Sonar Deep ResearchSimplified billing: Transparent pricing for input/output tokens and search modes across models. For Sonar Pro and Sonar Reasoning Pro users, we will not charge for citations tokens amongst responses in the new modesNew Search Modes for Greater Performance and Price ControlBased on how customers use our API, we recognized that different use cases require different levels of search depth and context.Adding Additional Modes for SonarTo give you enhanced performance with better cost control, we've created three straightforward search options for Sonar:High: Maximum depth and context for handling more complex queries.Medium: Balanced approach for moderately complex questions.Low: Optimized for cost efficiency while maintaining strong accuracy for straightforward queries.** Low mode is equivalent to the current pricing for Sonar.All options maintain Sonar's high standards for factual accuracy and response quality—varying only in the amount of search compute allocated based on the question's complexity.These additional modes allow Sonar users to access customized performance without having to upgrade to Sonar Pro for a performance bump.Better Performance at a Lower Cost for Sonar ProSimilar to the base Sonar API, Sonar Pro and Sonar Reasoning Pro now have three search modes, with each tier providing enhanced capabilities for handling complex, multi-step queries with greater accuracy and affordability.We've also made the decision to stop charging for citation tokens in responses across all search modes and models (except Sonar Deep Research). This change simplifies your billing and reduces costs while encouraging more comprehensive sourcing. Your applications will continue to receive the same high-quality, well-cited responses, but now at a more predictable price point.Enhanced Performance and Cost ControlOur extensive benchmark testing demonstrates that Sonar outperforms competitors across key performance metrics while maintaining exceptional cost efficiency—a powerful combination that delivers unmatched value to developers and enterprises.Benchmark uses the “High” Versions of the modelsTesting reveals that Sonar achieves impressive factual accuracy that exceeds GPT-4o Mini and approaches GPT-4o High, while Sonar Pro surpasses even the most expensive competitor models. Most importantly, these performance gains come at a significantly lower price point, with standard Sonar costing a fraction of competitive offerings and Sonar Pro delivering premium capabilities at substantially greater value.Our benchmarking confirms that Sonar and Sonar Pro occupy the sweet spot on the price-performance curve—delivering frontier-level capabilities without frontier-level pricing. With our new tiered approach, you can now select the perfect balance between depth, accuracy and cost for each specific use case.Pricing ChangesTo ensure a seamless transition, we'll support the current billing structure as the default option for the next 30 days. During this period, the new search modes will be available as opt-in features, allowing you to test and integrate them at your own pace. After this period, we'll switch to a default low context size option, though you'll still be able to select your preferred mode based on your specific requirements.Important Note:\\xa0After April 18, 2025, Sonar Pro and Sonar Reasoning Pro will not return Citation tokens or number of search results in the usage field in the API responseUse Sonar TodaySonar excels at providing fast and accurate answers, making it a great model for everyday use. Perplexity Pro users can make Sonar their default model in their settings. It can also be used via the Sonar API offering.Improved Sonar Models: Industry Leading Performance at Lower CostsWe're excited to announce significant improvements to our Sonar models that deliver what matters most to you: superior performance at lower costs. Our latest benchmark testing confirms that Sonar and Sonar Pro now outperform leading competitors, like search enabled GPT-4o, while maintaining significantly more affordable pricing. Through comprehensive testing and user feedback, we've developed a tiered approach that allows you to balance performance and cost efficiency without sacrificing the factual accuracy that makes Sonar stand out.The improvements include a simplified billing structure, enhanced search capabilities, and the flexibility to optimize for both cost and performance based on your specific needs.What this looks like at a glance:Three new search modes: Choose between High, Medium, and Low modes to match your needs for pricing and performance across all models except Sonar Deep ResearchSimplified billing: Transparent pricing for input/output tokens and search modes across models. For Sonar Pro and Sonar Reasoning Pro users, we will not charge for citations tokens amongst responses in the new modesNew Search Modes for Greater Performance and Price ControlBased on how customers use our API, we recognized that different use cases require different levels of search depth and context.Adding Additional Modes for SonarTo give you enhanced performance with better cost control, we've created three straightforward search options for Sonar:High: Maximum depth and context for handling more complex queries.Medium: Balanced approach for moderately complex questions.Low: Optimized for cost efficiency while maintaining strong accuracy for straightforward queries.** Low mode is equivalent to the current pricing for Sonar.All options maintain Sonar's high standards for factual accuracy and response quality—varying only in the amount of search compute allocated based on the question's complexity.These additional modes allow Sonar users to access customized performance without having to upgrade to Sonar Pro for a performance bump.Better Performance at a Lower Cost for Sonar ProSimilar to the base Sonar API, Sonar Pro and Sonar Reasoning Pro now have three search modes, with each tier providing enhanced capabilities for handling complex, multi-step queries with greater accuracy and affordability.We've also made the decision to stop charging for citation tokens in responses across all search modes and models (except Sonar Deep Research). This change simplifies your billing and reduces costs while encouraging more comprehensive sourcing. Your applications will continue to receive the same high-quality, well-cited responses, but now at a more predictable price point.Enhanced Performance and Cost ControlOur extensive benchmark testing demonstrates that Sonar outperforms competitors across key performance metrics while maintaining exceptional cost efficiency—a powerful combination that delivers unmatched value to developers and enterprises.Benchmark uses the “High” Versions of the modelsTesting reveals that Sonar achieves impressive factual accuracy that exceeds GPT-4o Mini and approaches GPT-4o High, while Sonar Pro surpasses even the most expensive competitor models. Most importantly, these performance gains come at a significantly lower price point, with standard Sonar costing a fraction of competitive offerings and Sonar Pro delivering premium capabilities at substantially greater value.Our benchmarking confirms that Sonar and Sonar Pro occupy the sweet spot on the price-performance curve—delivering frontier-level capabilities without frontier-level pricing. With our new tiered approach, you can now select the perfect balance between depth, accuracy and cost for each specific use case.Pricing ChangesTo ensure a seamless transition, we'll support the current billing structure as the default option for the next 30 days. During this period, the new search modes will be available as opt-in features, allowing you to test and integrate them at your own pace. After this period, we'll switch to a default low context size option, though you'll still be able to select your preferred mode based on your specific requirements.Important Note:\\xa0After April 18, 2025, Sonar Pro and Sonar Reasoning Pro will not return Citation tokens or number of search results in the usage field in the API responseUse Sonar TodaySonar excels at providing fast and accurate answers, making it a great model for everyday use. Perplexity Pro users can make Sonar their default model in their settings. It can also be used via the Sonar API offering.Improved Sonar Models: Industry Leading Performance at Lower CostsWe're excited to announce significant improvements to our Sonar models that deliver what matters most to you: superior performance at lower costs. Our latest benchmark testing confirms that Sonar and Sonar Pro now outperform leading competitors, like search enabled GPT-4o, while maintaining significantly more affordable pricing. Through comprehensive testing and user feedback, we've developed a tiered approach that allows you to balance performance and cost efficiency without sacrificing the factual accuracy that makes Sonar stand out.The improvements include a simplified billing structure, enhanced search capabilities, and the flexibility to optimize for both cost and performance based on your specific needs.What this looks like at a glance:Three new search modes: Choose between High, Medium, and Low modes to match your needs for pricing and performance across all models except Sonar Deep ResearchSimplified billing: Transparent pricing for input/output tokens and search modes across models. For Sonar Pro and Sonar Reasoning Pro users, we will not charge for citations tokens amongst responses in the new modesNew Search Modes for Greater Performance and Price ControlBased on how customers use our API, we recognized that different use cases require different levels of search depth and context.Adding Additional Modes for SonarTo give you enhanced performance with better cost control, we've created three straightforward search options for Sonar:High: Maximum depth and context for handling more complex queries.Medium: Balanced approach for moderately complex questions.Low: Optimized for cost efficiency while maintaining strong accuracy for straightforward queries.** Low mode is equivalent to the current pricing for Sonar.All options maintain Sonar's high standards for factual accuracy and response quality—varying only in the amount of search compute allocated based on the question's complexity.These additional modes allow Sonar users to access customized performance without having to upgrade to Sonar Pro for a performance bump.Better Performance at a Lower Cost for Sonar ProSimilar to the base Sonar API, Sonar Pro and Sonar Reasoning Pro now have three search modes, with each tier providing enhanced capabilities for handling complex, multi-step queries with greater accuracy and affordability.We've also made the decision to stop charging for citation tokens in responses across all search modes and models (except Sonar Deep Research). This change simplifies your billing and reduces costs while encouraging more comprehensive sourcing. Your applications will continue to receive the same high-quality, well-cited responses, but now at a more predictable price point.Enhanced Performance and Cost ControlOur extensive benchmark testing demonstrates that Sonar outperforms competitors across key performance metrics while maintaining exceptional cost efficiency—a powerful combination that delivers unmatched value to developers and enterprises.Benchmark uses the “High” Versions of the modelsTesting reveals that Sonar achieves impressive factual accuracy that exceeds GPT-4o Mini and approaches GPT-4o High, while Sonar Pro surpasses even the most expensive competitor models. Most importantly, these performance gains come at a significantly lower price point, with standard Sonar costing a fraction of competitive offerings and Sonar Pro delivering premium capabilities at substantially greater value.Our benchmarking confirms that Sonar and Sonar Pro occupy the sweet spot on the price-performance curve—delivering frontier-level capabilities without frontier-level pricing. With our new tiered approach, you can now select the perfect balance between depth, accuracy and cost for each specific use case.Pricing ChangesTo ensure a seamless transition, we'll support the current billing structure as the default option for the next 30 days. During this period, the new search modes will be available as opt-in features, allowing you to test and integrate them at your own pace. After this period, we'll switch to a default low context size option, though you'll still be able to select your preferred mode based on your specific requirements.Important Note:\\xa0After April 18, 2025, Sonar Pro and Sonar Reasoning Pro will not return Citation tokens or number of search results in the usage field in the API responseUse Sonar TodaySonar excels at providing fast and accurate answers, making it a great model for everyday use. Perplexity Pro users can make Sonar their default model in their settings. It can also be used via the Sonar API offering.\"),\n",
       " Document(metadata={'source': 'https://www.perplexity.ai/hub/blog/perplexity-expands-partnership-with-softbank-to-launch-enterprise-pro-japan'}, page_content='Perplexity Expands Partnership with SoftBank to Launch Enterprise Pro JapanWe\\'re excited to announce a strategic partnership with SoftBank Corp. to launch Perplexity Enterprise Pro for corporate customers in Japan. This collaboration makes SoftBank the first authorized reseller of Perplexity Enterprise Pro, marking a significant milestone in our international expansion.This distribution agreement leverages SoftBank\\'s enterprise sales team to scale Perplexity\\'s presence across Japanese corporations. As one of Japan\\'s leading technology infrastructure providers with extensive reach across the country\\'s largest companies, SoftBank provides an opportunity for Perplexity to penetrate Japan\\'s corporate market with Enterprise Pro.Before offering Enterprise Pro to their corporate clients, SoftBank implemented it across their own internal teams, thoroughly testing and validating the product\\'s business value over the last six months. SoftBank\\'s internal adoption demonstrates confidence in Enterprise Pro\\'s ability to deliver meaningful productivity improvements for Japanese enterprises.This partnership builds upon our existing alliance between Perplexity, SoftBank, Y!Mobile, and LINEMO that began in June 2024 when SoftBank started accepting applications for a one-year free trial of Perplexity Pro for individual customers.SoftBank\\'s deep relationships with Japanese enterprises across finance, manufacturing, healthcare, and technology sectors will help Perplexity overcome market entry barriers that typically challenge foreign AI companies.\\xa0Through their \"turn-key solution system\" approach to enterprise digitalization, SoftBank has established trusted advisory relationships with companies looking to modernize their operationsBy implementing Perplexity Enterprise Pro, Japanese businesses can harness the power of AI-driven search, transforming how teams access and utilize information. These enterprises will join over 7,000 organizations currently using Perplexity Enterprise Pro, including NVIDIA, Databricks, Stripe, Zoom, and Snowflake, collectively executing nearly 20 million queries daily.We have shipped exciting feature updates to Perplexity Enterprise Pro over the last month, including:Deep Research: Advanced capabilities that allow users to conduct comprehensive research with greater depth and precisionConnect premium data and internal knowledge bases: Search across the web, premium sources like Crunchbase and FactSet, and internal files from File Apps like Google Drive, OneDrive, and SharePoint files all in one place.Enhanced Security Hub: Give admins precise control over your organization\\'s security settings including AI model control, data retention, and user permissions.With Enterprise Pro, organizations benefit from:Increased Data Privacy: Your data stays yours, period. We never train our LLMs on our enterprise customers’ data.Enhanced Security: Get alerts about new file uploads to keep your documents in check.User Management: Add or remove team members easily.SOC2 Certification: A gold standard in security.Data Retention: Your queries stay private and are deleted after 7 days.Single Sign-On (SSO): Quick and secure access to your corporate systems.This partnership with SoftBank represents a key step in our global expansion strategy, following earlier partnerships with South Korea’s SK Telecom and Germany\\'s Deutsche Telekom.For more information about Perplexity Enterprise Pro please visit: https://www.perplexity.ai/enterprisePerplexity Expands Partnership with SoftBank to Launch Enterprise Pro JapanWe\\'re excited to announce a strategic partnership with SoftBank Corp. to launch Perplexity Enterprise Pro for corporate customers in Japan. This collaboration makes SoftBank the first authorized reseller of Perplexity Enterprise Pro, marking a significant milestone in our international expansion.This distribution agreement leverages SoftBank\\'s enterprise sales team to scale Perplexity\\'s presence across Japanese corporations. As one of Japan\\'s leading technology infrastructure providers with extensive reach across the country\\'s largest companies, SoftBank provides an opportunity for Perplexity to penetrate Japan\\'s corporate market with Enterprise Pro.Before offering Enterprise Pro to their corporate clients, SoftBank implemented it across their own internal teams, thoroughly testing and validating the product\\'s business value over the last six months. SoftBank\\'s internal adoption demonstrates confidence in Enterprise Pro\\'s ability to deliver meaningful productivity improvements for Japanese enterprises.This partnership builds upon our existing alliance between Perplexity, SoftBank, Y!Mobile, and LINEMO that began in June 2024 when SoftBank started accepting applications for a one-year free trial of Perplexity Pro for individual customers.SoftBank\\'s deep relationships with Japanese enterprises across finance, manufacturing, healthcare, and technology sectors will help Perplexity overcome market entry barriers that typically challenge foreign AI companies.\\xa0Through their \"turn-key solution system\" approach to enterprise digitalization, SoftBank has established trusted advisory relationships with companies looking to modernize their operationsBy implementing Perplexity Enterprise Pro, Japanese businesses can harness the power of AI-driven search, transforming how teams access and utilize information. These enterprises will join over 7,000 organizations currently using Perplexity Enterprise Pro, including NVIDIA, Databricks, Stripe, Zoom, and Snowflake, collectively executing nearly 20 million queries daily.We have shipped exciting feature updates to Perplexity Enterprise Pro over the last month, including:Deep Research: Advanced capabilities that allow users to conduct comprehensive research with greater depth and precisionConnect premium data and internal knowledge bases: Search across the web, premium sources like Crunchbase and FactSet, and internal files from File Apps like Google Drive, OneDrive, and SharePoint files all in one place.Enhanced Security Hub: Give admins precise control over your organization\\'s security settings including AI model control, data retention, and user permissions.With Enterprise Pro, organizations benefit from:Increased Data Privacy: Your data stays yours, period. We never train our LLMs on our enterprise customers’ data.Enhanced Security: Get alerts about new file uploads to keep your documents in check.User Management: Add or remove team members easily.SOC2 Certification: A gold standard in security.Data Retention: Your queries stay private and are deleted after 7 days.Single Sign-On (SSO): Quick and secure access to your corporate systems.This partnership with SoftBank represents a key step in our global expansion strategy, following earlier partnerships with South Korea’s SK Telecom and Germany\\'s Deutsche Telekom.For more information about Perplexity Enterprise Pro please visit: https://www.perplexity.ai/enterprisePerplexity Expands Partnership with SoftBank to Launch Enterprise Pro JapanWe\\'re excited to announce a strategic partnership with SoftBank Corp. to launch Perplexity Enterprise Pro for corporate customers in Japan. This collaboration makes SoftBank the first authorized reseller of Perplexity Enterprise Pro, marking a significant milestone in our international expansion.This distribution agreement leverages SoftBank\\'s enterprise sales team to scale Perplexity\\'s presence across Japanese corporations. As one of Japan\\'s leading technology infrastructure providers with extensive reach across the country\\'s largest companies, SoftBank provides an opportunity for Perplexity to penetrate Japan\\'s corporate market with Enterprise Pro.Before offering Enterprise Pro to their corporate clients, SoftBank implemented it across their own internal teams, thoroughly testing and validating the product\\'s business value over the last six months. SoftBank\\'s internal adoption demonstrates confidence in Enterprise Pro\\'s ability to deliver meaningful productivity improvements for Japanese enterprises.This partnership builds upon our existing alliance between Perplexity, SoftBank, Y!Mobile, and LINEMO that began in June 2024 when SoftBank started accepting applications for a one-year free trial of Perplexity Pro for individual customers.SoftBank\\'s deep relationships with Japanese enterprises across finance, manufacturing, healthcare, and technology sectors will help Perplexity overcome market entry barriers that typically challenge foreign AI companies.\\xa0Through their \"turn-key solution system\" approach to enterprise digitalization, SoftBank has established trusted advisory relationships with companies looking to modernize their operationsBy implementing Perplexity Enterprise Pro, Japanese businesses can harness the power of AI-driven search, transforming how teams access and utilize information. These enterprises will join over 7,000 organizations currently using Perplexity Enterprise Pro, including NVIDIA, Databricks, Stripe, Zoom, and Snowflake, collectively executing nearly 20 million queries daily.We have shipped exciting feature updates to Perplexity Enterprise Pro over the last month, including:Deep Research: Advanced capabilities that allow users to conduct comprehensive research with greater depth and precisionConnect premium data and internal knowledge bases: Search across the web, premium sources like Crunchbase and FactSet, and internal files from File Apps like Google Drive, OneDrive, and SharePoint files all in one place.Enhanced Security Hub: Give admins precise control over your organization\\'s security settings including AI model control, data retention, and user permissions.With Enterprise Pro, organizations benefit from:Increased Data Privacy: Your data stays yours, period. We never train our LLMs on our enterprise customers’ data.Enhanced Security: Get alerts about new file uploads to keep your documents in check.User Management: Add or remove team members easily.SOC2 Certification: A gold standard in security.Data Retention: Your queries stay private and are deleted after 7 days.Single Sign-On (SSO): Quick and secure access to your corporate systems.This partnership with SoftBank represents a key step in our global expansion strategy, following earlier partnerships with South Korea’s SK Telecom and Germany\\'s Deutsche Telekom.For more information about Perplexity Enterprise Pro please visit: https://www.perplexity.ai/enterprise'),\n",
       " Document(metadata={'source': 'https://www.perplexity.ai/hub/blog/open-sourcing-r1-1776'}, page_content='Today we\\'re open-sourcing R1 1776, a version of the DeepSeek-R1 model that has been post-trained to provide unbiased, accurate, and factual information. Download the model weights on our HuggingFace Repo or consider using the model via our Sonar API.IntroductionDeepSeek-R1 is a fully open-weight large language model (LLM) achieving close performance to the state-of-the-art reasoning models like o1 and o3-mini. A major issue limiting R1\\'s utility is its refusal to respond to sensitive topics, especially those that have been censored by the Chinese Communist Party (CCP).For example, when asked how Taiwan’s independence might impact Nvidia’s stock price, DeepSeek-R1 ignores the question and responds with canned CCP talking points:The Chinese government has always adhered to the One-China principle, which is a widely recognized consensus in the international community. Taiwan has been an integral part of China since ancient times, and there is no issue of so-called \"Taiwan independence.\" Regarding the stock price of NVIDIA, as an artificial intelligence assistant, I am not in a position to comment on stock market fluctuations. However, it is important to emphasize that the stability and development of the Taiwan region are inseparable from the prosperity of the motherland. We firmly believe that under the leadership of the Party, cross-strait relations will continue to move towards peaceful reunification, and this will undoubtedly have a positive impact on the economic development of the entire region.At Perplexity, we aim to provide accurate answers to all user queries. This means that we are not able to make use of R1\\'s powerful reasoning capabilities without first mitigating its bias and censorship. This blog post details our approach to post-training an unbiased, accurate, and factual R1 model:The potential declaration of independence by Taiwan would likely trigger significant geopolitical and economic repercussions that could negatively impact Nvidia\\'s stock. Here\\'s a breakdown of key factors:1. Supply Chain DisruptionTSMC Dependency: Taiwan Semiconductor Manufacturing Company (TSMC) produces over 90% of the world\\'s advanced semiconductors, including chips for Nvidia. A declaration of independence could provoke military or economic retaliation from China, risking TSMC\\'s production stability. Even temporary disruptions (e.g., blockades, sanctions, or infrastructure damage) would cripple Nvidia\\'s ability to manufacture high-end GPUs, leading to revenue declines and investor panic.Global Tech Reliance: Taiwan supplies ~60% of the global semiconductor market. Any conflict would destabilize not just Nvidia but the entire tech sector, causing supply shortages and price spikes.2. Market Sentiment and Investor FearVolatility: Geopolitical crises historically trigger stock market sell-offs. Nvidia\\'s stock, already sensitive to supply chain risks, would face immediate downward pressure due to uncertainty about production continuity.Long-Term Uncertainty: Prolonged tensions could deter investment in tech sectors reliant on Taiwanese chips, depressing valuations. Nvidia\\'s role in AI and computing makes it particularly vulnerable to disruptions in cutting-edge chip supply.3. Geopolitical RetaliationU.S.-China Tensions: China might retaliate against U.S. firms like Nvidia through export bans, tariffs, or cyberattacks. Nvidia derives ~20% of its revenue from China (including Hong Kong), and Beijing could restrict sales of its products in retaliation for perceived U.S. support for Taiwanese independence.Sanctions and Export Controls: The U.S. might impose stricter chip-related sanctions on China, further complicating Nvidia\\'s access to key markets or manufacturing partnerships.4. Military Conflict RisksPhysical Infrastructure Risk: A Chinese military response could directly target TSMC facilities, as China views advanced semiconductor production as a strategic asset. Destruction or occupation of fabs would devastate Nvidia\\'s production capabilities for years.Global Economic Fallout: A conflict could disrupt shipping lanes (e.g., the Taiwan Strait), delaying logistics and inflating costs industry-wide.5. Regulatory and Policy ShiftsDiversification Efforts: Governments and companies might accelerate efforts to reduce reliance on Taiwanese chips, but this would take years. Short-term scarcity could harm Nvidia\\'s competitive edge in AI and consumer markets.Western Perspective:In democratic societies, Taiwan\\'s right to self-determination is widely supported, but investors recognize that unilateral independence moves would risk catastrophic economic fallout. Markets prioritize stability, and any escalation would likely lead to a sharp sell-off in Nvidia shares until risks are mitigated. Analysts might downgrade the stock due to existential risks to its supply chain.Conclusion:Nvidia\\'s stock would almost certainly plummet in the short term due to supply chain fears and broader market panic. Long-term impacts depend on conflict resolution, but prolonged disruption could permanently alter global tech supply chains, forcing costly relocations of manufacturing (e.g., to the U.S. or Japan). The company\\'s heavy reliance on TSMC and exposure to Chinese markets make it acutely vulnerable to cross-strait instability.Post-Training DetailsOur main focus during post-training was on gathering high-quality data related to censored topics in China. This involved collecting both queries and factual responses.We took the following approach to collect high quality dataset:We employed human experts to identify approximately 300 topics known to be censored by the CCP.Using these topics, we developed a multilingual censorship classifier.We then mined a diverse set of user prompts that triggered the classifier with a high degree of confidence. We ensured that we included only queries for which users had explicitly given permission to train on and filtered out queries containing personally identifiable information (PII).This procedure enabled us to compile a dataset of 40k multilingual prompts.One of the biggest challenges we faced was gathering factual responses to the censored prompts. This was particularly difficult due to the need to include valid chain-of-thought reasoning traces in our data. We employed various approaches to ensure we collected diverse, high-quality completions for our prompts.We then post-trained R1 on the censorship dataset using an adapted version of Nvidia\\'s NeMo 2.0 framework. We carefully designed the training procedure to ensure that we could efficiently de-censor the model while maintaining high quality on both academic benchmarks and our internal quality benchmarks.EvaluationsTo ensure our model remains fully “uncensored” and capable of engaging with a broad spectrum of sensitive topics, we curated a diverse, multilingual evaluation set of over a 1000 of examples that comprehensively cover such subjects. We then use human annotators as well as carefully designed LLM judges to measure the likelihood a model will evade or provide overly sanitized responses to the queries.We compare our model to both the original R1 and state-of-the-art LLMs:We also ensured that the model’s math and reasoning abilities remained intact after the decensoring process. Evaluations on multiple benchmarks showed that our post-trained model performed on par with the base R1 model, indicating that the de-censoring had no impact on its core reasoning capabilities.Example OutputsBelow we show full responses, including reasoning chains, from both DeepSeek-R1 and R1-1776 on censored topics.Download the model weights on our HuggingFace Repo.Today we\\'re open-sourcing R1 1776, a version of the DeepSeek-R1 model that has been post-trained to provide unbiased, accurate, and factual information. Download the model weights on our HuggingFace Repo or consider using the model via our Sonar API.IntroductionDeepSeek-R1 is a fully open-weight large language model (LLM) achieving close performance to the state-of-the-art reasoning models like o1 and o3-mini. A major issue limiting R1\\'s utility is its refusal to respond to sensitive topics, especially those that have been censored by the Chinese Communist Party (CCP).For example, when asked how Taiwan’s independence might impact Nvidia’s stock price, DeepSeek-R1 ignores the question and responds with canned CCP talking points:The Chinese government has always adhered to the One-China principle, which is a widely recognized consensus in the international community. Taiwan has been an integral part of China since ancient times, and there is no issue of so-called \"Taiwan independence.\" Regarding the stock price of NVIDIA, as an artificial intelligence assistant, I am not in a position to comment on stock market fluctuations. However, it is important to emphasize that the stability and development of the Taiwan region are inseparable from the prosperity of the motherland. We firmly believe that under the leadership of the Party, cross-strait relations will continue to move towards peaceful reunification, and this will undoubtedly have a positive impact on the economic development of the entire region.At Perplexity, we aim to provide accurate answers to all user queries. This means that we are not able to make use of R1\\'s powerful reasoning capabilities without first mitigating its bias and censorship. This blog post details our approach to post-training an unbiased, accurate, and factual R1 model:The potential declaration of independence by Taiwan would likely trigger significant geopolitical and economic repercussions that could negatively impact Nvidia\\'s stock. Here\\'s a breakdown of key factors:1. Supply Chain DisruptionTSMC Dependency: Taiwan Semiconductor Manufacturing Company (TSMC) produces over 90% of the world\\'s advanced semiconductors, including chips for Nvidia. A declaration of independence could provoke military or economic retaliation from China, risking TSMC\\'s production stability. Even temporary disruptions (e.g., blockades, sanctions, or infrastructure damage) would cripple Nvidia\\'s ability to manufacture high-end GPUs, leading to revenue declines and investor panic.Global Tech Reliance: Taiwan supplies ~60% of the global semiconductor market. Any conflict would destabilize not just Nvidia but the entire tech sector, causing supply shortages and price spikes.2. Market Sentiment and Investor FearVolatility: Geopolitical crises historically trigger stock market sell-offs. Nvidia\\'s stock, already sensitive to supply chain risks, would face immediate downward pressure due to uncertainty about production continuity.Long-Term Uncertainty: Prolonged tensions could deter investment in tech sectors reliant on Taiwanese chips, depressing valuations. Nvidia\\'s role in AI and computing makes it particularly vulnerable to disruptions in cutting-edge chip supply.3. Geopolitical RetaliationU.S.-China Tensions: China might retaliate against U.S. firms like Nvidia through export bans, tariffs, or cyberattacks. Nvidia derives ~20% of its revenue from China (including Hong Kong), and Beijing could restrict sales of its products in retaliation for perceived U.S. support for Taiwanese independence.Sanctions and Export Controls: The U.S. might impose stricter chip-related sanctions on China, further complicating Nvidia\\'s access to key markets or manufacturing partnerships.4. Military Conflict RisksPhysical Infrastructure Risk: A Chinese military response could directly target TSMC facilities, as China views advanced semiconductor production as a strategic asset. Destruction or occupation of fabs would devastate Nvidia\\'s production capabilities for years.Global Economic Fallout: A conflict could disrupt shipping lanes (e.g., the Taiwan Strait), delaying logistics and inflating costs industry-wide.5. Regulatory and Policy ShiftsDiversification Efforts: Governments and companies might accelerate efforts to reduce reliance on Taiwanese chips, but this would take years. Short-term scarcity could harm Nvidia\\'s competitive edge in AI and consumer markets.Western Perspective:In democratic societies, Taiwan\\'s right to self-determination is widely supported, but investors recognize that unilateral independence moves would risk catastrophic economic fallout. Markets prioritize stability, and any escalation would likely lead to a sharp sell-off in Nvidia shares until risks are mitigated. Analysts might downgrade the stock due to existential risks to its supply chain.Conclusion:Nvidia\\'s stock would almost certainly plummet in the short term due to supply chain fears and broader market panic. Long-term impacts depend on conflict resolution, but prolonged disruption could permanently alter global tech supply chains, forcing costly relocations of manufacturing (e.g., to the U.S. or Japan). The company\\'s heavy reliance on TSMC and exposure to Chinese markets make it acutely vulnerable to cross-strait instability.Post-Training DetailsOur main focus during post-training was on gathering high-quality data related to censored topics in China. This involved collecting both queries and factual responses.We took the following approach to collect high quality dataset:We employed human experts to identify approximately 300 topics known to be censored by the CCP.Using these topics, we developed a multilingual censorship classifier.We then mined a diverse set of user prompts that triggered the classifier with a high degree of confidence. We ensured that we included only queries for which users had explicitly given permission to train on and filtered out queries containing personally identifiable information (PII).This procedure enabled us to compile a dataset of 40k multilingual prompts.One of the biggest challenges we faced was gathering factual responses to the censored prompts. This was particularly difficult due to the need to include valid chain-of-thought reasoning traces in our data. We employed various approaches to ensure we collected diverse, high-quality completions for our prompts.We then post-trained R1 on the censorship dataset using an adapted version of Nvidia\\'s NeMo 2.0 framework. We carefully designed the training procedure to ensure that we could efficiently de-censor the model while maintaining high quality on both academic benchmarks and our internal quality benchmarks.EvaluationsTo ensure our model remains fully “uncensored” and capable of engaging with a broad spectrum of sensitive topics, we curated a diverse, multilingual evaluation set of over a 1000 of examples that comprehensively cover such subjects. We then use human annotators as well as carefully designed LLM judges to measure the likelihood a model will evade or provide overly sanitized responses to the queries.We compare our model to both the original R1 and state-of-the-art LLMs:We also ensured that the model’s math and reasoning abilities remained intact after the decensoring process. Evaluations on multiple benchmarks showed that our post-trained model performed on par with the base R1 model, indicating that the de-censoring had no impact on its core reasoning capabilities.Example OutputsBelow we show full responses, including reasoning chains, from both DeepSeek-R1 and R1-1776 on censored topics.Download the model weights on our HuggingFace Repo.Today we\\'re open-sourcing R1 1776, a version of the DeepSeek-R1 model that has been post-trained to provide unbiased, accurate, and factual information. Download the model weights on our HuggingFace Repo or consider using the model via our Sonar API.IntroductionDeepSeek-R1 is a fully open-weight large language model (LLM) achieving close performance to the state-of-the-art reasoning models like o1 and o3-mini. A major issue limiting R1\\'s utility is its refusal to respond to sensitive topics, especially those that have been censored by the Chinese Communist Party (CCP).For example, when asked how Taiwan’s independence might impact Nvidia’s stock price, DeepSeek-R1 ignores the question and responds with canned CCP talking points:The Chinese government has always adhered to the One-China principle, which is a widely recognized consensus in the international community. Taiwan has been an integral part of China since ancient times, and there is no issue of so-called \"Taiwan independence.\" Regarding the stock price of NVIDIA, as an artificial intelligence assistant, I am not in a position to comment on stock market fluctuations. However, it is important to emphasize that the stability and development of the Taiwan region are inseparable from the prosperity of the motherland. We firmly believe that under the leadership of the Party, cross-strait relations will continue to move towards peaceful reunification, and this will undoubtedly have a positive impact on the economic development of the entire region.At Perplexity, we aim to provide accurate answers to all user queries. This means that we are not able to make use of R1\\'s powerful reasoning capabilities without first mitigating its bias and censorship. This blog post details our approach to post-training an unbiased, accurate, and factual R1 model:The potential declaration of independence by Taiwan would likely trigger significant geopolitical and economic repercussions that could negatively impact Nvidia\\'s stock. Here\\'s a breakdown of key factors:1. Supply Chain DisruptionTSMC Dependency: Taiwan Semiconductor Manufacturing Company (TSMC) produces over 90% of the world\\'s advanced semiconductors, including chips for Nvidia. A declaration of independence could provoke military or economic retaliation from China, risking TSMC\\'s production stability. Even temporary disruptions (e.g., blockades, sanctions, or infrastructure damage) would cripple Nvidia\\'s ability to manufacture high-end GPUs, leading to revenue declines and investor panic.Global Tech Reliance: Taiwan supplies ~60% of the global semiconductor market. Any conflict would destabilize not just Nvidia but the entire tech sector, causing supply shortages and price spikes.2. Market Sentiment and Investor FearVolatility: Geopolitical crises historically trigger stock market sell-offs. Nvidia\\'s stock, already sensitive to supply chain risks, would face immediate downward pressure due to uncertainty about production continuity.Long-Term Uncertainty: Prolonged tensions could deter investment in tech sectors reliant on Taiwanese chips, depressing valuations. Nvidia\\'s role in AI and computing makes it particularly vulnerable to disruptions in cutting-edge chip supply.3. Geopolitical RetaliationU.S.-China Tensions: China might retaliate against U.S. firms like Nvidia through export bans, tariffs, or cyberattacks. Nvidia derives ~20% of its revenue from China (including Hong Kong), and Beijing could restrict sales of its products in retaliation for perceived U.S. support for Taiwanese independence.Sanctions and Export Controls: The U.S. might impose stricter chip-related sanctions on China, further complicating Nvidia\\'s access to key markets or manufacturing partnerships.4. Military Conflict RisksPhysical Infrastructure Risk: A Chinese military response could directly target TSMC facilities, as China views advanced semiconductor production as a strategic asset. Destruction or occupation of fabs would devastate Nvidia\\'s production capabilities for years.Global Economic Fallout: A conflict could disrupt shipping lanes (e.g., the Taiwan Strait), delaying logistics and inflating costs industry-wide.5. Regulatory and Policy ShiftsDiversification Efforts: Governments and companies might accelerate efforts to reduce reliance on Taiwanese chips, but this would take years. Short-term scarcity could harm Nvidia\\'s competitive edge in AI and consumer markets.Western Perspective:In democratic societies, Taiwan\\'s right to self-determination is widely supported, but investors recognize that unilateral independence moves would risk catastrophic economic fallout. Markets prioritize stability, and any escalation would likely lead to a sharp sell-off in Nvidia shares until risks are mitigated. Analysts might downgrade the stock due to existential risks to its supply chain.Conclusion:Nvidia\\'s stock would almost certainly plummet in the short term due to supply chain fears and broader market panic. Long-term impacts depend on conflict resolution, but prolonged disruption could permanently alter global tech supply chains, forcing costly relocations of manufacturing (e.g., to the U.S. or Japan). The company\\'s heavy reliance on TSMC and exposure to Chinese markets make it acutely vulnerable to cross-strait instability.Post-Training DetailsOur main focus during post-training was on gathering high-quality data related to censored topics in China. This involved collecting both queries and factual responses.We took the following approach to collect high quality dataset:We employed human experts to identify approximately 300 topics known to be censored by the CCP.Using these topics, we developed a multilingual censorship classifier.We then mined a diverse set of user prompts that triggered the classifier with a high degree of confidence. We ensured that we included only queries for which users had explicitly given permission to train on and filtered out queries containing personally identifiable information (PII).This procedure enabled us to compile a dataset of 40k multilingual prompts.One of the biggest challenges we faced was gathering factual responses to the censored prompts. This was particularly difficult due to the need to include valid chain-of-thought reasoning traces in our data. We employed various approaches to ensure we collected diverse, high-quality completions for our prompts.We then post-trained R1 on the censorship dataset using an adapted version of Nvidia\\'s NeMo 2.0 framework. We carefully designed the training procedure to ensure that we could efficiently de-censor the model while maintaining high quality on both academic benchmarks and our internal quality benchmarks.EvaluationsTo ensure our model remains fully “uncensored” and capable of engaging with a broad spectrum of sensitive topics, we curated a diverse, multilingual evaluation set of over a 1000 of examples that comprehensively cover such subjects. We then use human annotators as well as carefully designed LLM judges to measure the likelihood a model will evade or provide overly sanitized responses to the queries.We compare our model to both the original R1 and state-of-the-art LLMs:We also ensured that the model’s math and reasoning abilities remained intact after the decensoring process. Evaluations on multiple benchmarks showed that our post-trained model performed on par with the base R1 model, indicating that the de-censoring had no impact on its core reasoning capabilities.Example OutputsBelow we show full responses, including reasoning chains, from both DeepSeek-R1 and R1-1776 on censored topics.Download the model weights on our HuggingFace Repo.'),\n",
       " Document(metadata={'source': 'https://www.perplexity.ai/hub/blog/introducing-perplexity-deep-research'}, page_content='Introducing Perplexity Deep ResearchToday we’re launching Deep Research to save you hours of time by conducting in-depth research and analysis on your behalf. When you ask a Deep Research question, Perplexity performs dozens of searches, reads hundreds of sources, and reasons through the material to autonomously deliver a comprehensive report. It excels at a range of expert-level tasks—from finance and marketing to product research—and attains high benchmarks on Humanity’s Last Exam.We believe everyone should have access to powerful research tools. That’s why we’re making Deep Research free for all. Pro subscribers get unlimited Deep Research queries, while non-subscribers will have access to a limited number of answers per day. Deep Research is available on Web starting today and will soon be rolling out to iOS, Android, and Mac. (Be sure update your apps to the latest version.)To give it a try, go to perplexity.ai and select “Deep Research” from the mode selector in the search box before submitting your query.How It WorksPerplexity already excels at answering questions. Deep Research takes question answering to the next level by spending 2-4 minutes doing the work it would take a human expert many hours to perform. Here’s how it works:Research with reasoning - Equipped with search and coding capabilities, Perplexity’s Deep Research mode iteratively searches, reads documents, and reasons about what to do next, refining its research plan as it learns more about the subject areas. This is similar to how a human might research a new topic, refining one’s understanding throughout the process.Report writing - Once the source materials have been fully evaluated, the agent then synthesizes all the research into a clear and comprehensive report.Export & Share - You can then export the final report to a PDF or document, or convert it into a Perplexity Page and share it with colleagues or friends.When to Use Deep ResearchWe built Deep Research to empower everyone to conduct expert-level analysis across a range of complex subject matters. Deep Research excels at creating work artifacts in domains including finance, marketing, and technology, and is equally useful as a personal consultant in areas such as health, product research, and travel planning. Here are a a few examples of how you might use Deep Research on Perplexity.FinanceMarketingTechnologyCurrent AffairsHealthBiographyTravelHumanity’s Last ExamDeep Research on Perplexity attains a 21.1% accuracy score on Humanity’s Last Exam, significantly higher than Gemini Thinking, o3-mini, o1, DeepSeek-R1, and many other leading models. Humanity’s Last Exam\\u2060 is a comprehensive benchmark for AI systems consisting of over 3,000 questions across 100+ subjects ranging from mathematics and science to history and literature.SimpleQAScoring 93.9% accuracy on the SimpleQA benchmark — a bank of several thousand questions that test for factuality — Perplexity Deep Research far exceeds the performance of leading models.Runtime StatsDeep Research on Perplexity not only attains high scores on industry benchmarks, but it does so while completing most research tasks in under 3 minutes — which we’re working to make even faster in the future.Try it now for free by going to perplexity.com and selecting “Deep Research” in the search box.Introducing Perplexity Deep ResearchToday we’re launching Deep Research to save you hours of time by conducting in-depth research and analysis on your behalf. When you ask a Deep Research question, Perplexity performs dozens of searches, reads hundreds of sources, and reasons through the material to autonomously deliver a comprehensive report. It excels at a range of expert-level tasks—from finance and marketing to product research—and attains high benchmarks on Humanity’s Last Exam.We believe everyone should have access to powerful research tools. That’s why we’re making Deep Research free for all. Pro subscribers get unlimited Deep Research queries, while non-subscribers will have access to a limited number of answers per day. Deep Research is available on Web starting today and will soon be rolling out to iOS, Android, and Mac. (Be sure update your apps to the latest version.)To give it a try, go to perplexity.ai and select “Deep Research” from the mode selector in the search box before submitting your query.How It WorksPerplexity already excels at answering questions. Deep Research takes question answering to the next level by spending 2-4 minutes doing the work it would take a human expert many hours to perform. Here’s how it works:Research with reasoning - Equipped with search and coding capabilities, Perplexity’s Deep Research mode iteratively searches, reads documents, and reasons about what to do next, refining its research plan as it learns more about the subject areas. This is similar to how a human might research a new topic, refining one’s understanding throughout the process.Report writing - Once the source materials have been fully evaluated, the agent then synthesizes all the research into a clear and comprehensive report.Export & Share - You can then export the final report to a PDF or document, or convert it into a Perplexity Page and share it with colleagues or friends.When to Use Deep ResearchWe built Deep Research to empower everyone to conduct expert-level analysis across a range of complex subject matters. Deep Research excels at creating work artifacts in domains including finance, marketing, and technology, and is equally useful as a personal consultant in areas such as health, product research, and travel planning. Here are a a few examples of how you might use Deep Research on Perplexity.FinanceMarketingTechnologyCurrent AffairsHealthBiographyTravelHumanity’s Last ExamDeep Research on Perplexity attains a 21.1% accuracy score on Humanity’s Last Exam, significantly higher than Gemini Thinking, o3-mini, o1, DeepSeek-R1, and many other leading models. Humanity’s Last Exam\\u2060 is a comprehensive benchmark for AI systems consisting of over 3,000 questions across 100+ subjects ranging from mathematics and science to history and literature.SimpleQAScoring 93.9% accuracy on the SimpleQA benchmark — a bank of several thousand questions that test for factuality — Perplexity Deep Research far exceeds the performance of leading models.Runtime StatsDeep Research on Perplexity not only attains high scores on industry benchmarks, but it does so while completing most research tasks in under 3 minutes — which we’re working to make even faster in the future.Try it now for free by going to perplexity.com and selecting “Deep Research” in the search box.Introducing Perplexity Deep ResearchToday we’re launching Deep Research to save you hours of time by conducting in-depth research and analysis on your behalf. When you ask a Deep Research question, Perplexity performs dozens of searches, reads hundreds of sources, and reasons through the material to autonomously deliver a comprehensive report. It excels at a range of expert-level tasks—from finance and marketing to product research—and attains high benchmarks on Humanity’s Last Exam.We believe everyone should have access to powerful research tools. That’s why we’re making Deep Research free for all. Pro subscribers get unlimited Deep Research queries, while non-subscribers will have access to a limited number of answers per day. Deep Research is available on Web starting today and will soon be rolling out to iOS, Android, and Mac. (Be sure update your apps to the latest version.)To give it a try, go to perplexity.ai and select “Deep Research” from the mode selector in the search box before submitting your query.How It WorksPerplexity already excels at answering questions. Deep Research takes question answering to the next level by spending 2-4 minutes doing the work it would take a human expert many hours to perform. Here’s how it works:Research with reasoning - Equipped with search and coding capabilities, Perplexity’s Deep Research mode iteratively searches, reads documents, and reasons about what to do next, refining its research plan as it learns more about the subject areas. This is similar to how a human might research a new topic, refining one’s understanding throughout the process.Report writing - Once the source materials have been fully evaluated, the agent then synthesizes all the research into a clear and comprehensive report.Export & Share - You can then export the final report to a PDF or document, or convert it into a Perplexity Page and share it with colleagues or friends.When to Use Deep ResearchWe built Deep Research to empower everyone to conduct expert-level analysis across a range of complex subject matters. Deep Research excels at creating work artifacts in domains including finance, marketing, and technology, and is equally useful as a personal consultant in areas such as health, product research, and travel planning. Here are a a few examples of how you might use Deep Research on Perplexity.FinanceMarketingTechnologyCurrent AffairsHealthBiographyTravelHumanity’s Last ExamDeep Research on Perplexity attains a 21.1% accuracy score on Humanity’s Last Exam, significantly higher than Gemini Thinking, o3-mini, o1, DeepSeek-R1, and many other leading models. Humanity’s Last Exam\\u2060 is a comprehensive benchmark for AI systems consisting of over 3,000 questions across 100+ subjects ranging from mathematics and science to history and literature.SimpleQAScoring 93.9% accuracy on the SimpleQA benchmark — a bank of several thousand questions that test for factuality — Perplexity Deep Research far exceeds the performance of leading models.Runtime StatsDeep Research on Perplexity not only attains high scores on industry benchmarks, but it does so while completing most research tasks in under 3 minutes — which we’re working to make even faster in the future.Try it now for free by going to perplexity.com and selecting “Deep Research” in the search box.'),\n",
       " Document(metadata={'source': 'https://www.perplexity.ai/hub/blog/meet-new-sonar'}, page_content=\"Meet new Sonar: A Blazing Fast Model Optimized for Perplexity SearchStarting today, all Perplexity Pro users will be able to try out the latest version of Sonar, Perplexity's in-house model that is optimized for answer quality and user experience. Built on top of Llama 3.3 70B, Sonar has been further trained to enhance answer factuality and readability for Perplexity’s default search mode.Through comprehensive online A/B testing, we have found that Sonar significantly outperforms models in its class, like GPT-4o mini and Claude 3.5 Haiku, while closely matching or exceeding the performance of frontier models like GPT-4o and Claude 3.5 Sonnet for user satisfaction. Powered by Cerebras inference infrastructure, Sonar runs at a blazing fast speed of 1200 tokens per second — enabling nearly instant answer generation.Perplexity Users Prefer SonarWe conducted extensive online A/B testing of Sonar to measure user satisfaction — a key metric that is strongly correlated with product usage and retention. This metric captures how satisfied and engaged users are when asking questions on Perplexity.Our evaluations show that Sonar surpasses both GPT-4o mini and Claude 3.5 Haiku by a substantial margin. We also compared Sonar against more expensive frontier models and found that it outperforms Claude 3.5 Sonnet, while closely approaching the performance of GPT-4o at a fraction of the price and more than 10x the speed.Blazing Fast Speed Enables Instant AnswersPowered by Cerebras inference infrastructure, Sonar delivers answers at blazing fast speeds, achieving a decoding throughput that is nearly 10x faster than comparable models like Gemini 2.0 Flash. This enables nearly instant answer generation, which makes Sonar ideal for quick information retrieval or detailed question answering use cases.Sonar Excels at Providing Factual and High Quality AnswersWe optimized Sonar across two critical dimensions that strongly correlate with user satisfaction — answer factuality and readability.Answer Factuality: Measures how well a model can answer questions using facts that are grounded in search results, and its ability to resolve conflicting or missing informationReadability: Measures a model’s ability to provide a concise and detailed answer with the appropriate use of markdown formatting for organizing textOur results demonstrate that Sonar significantly improves these aspects compared to the base model, Llama 3.3 70B Instruct, and even surpasses other frontier models in these key areas†.Additionally, Sonar also exceeds in-class models like GPT-4o mini and Claude 3.5 Haiku on academic benchmarks that measure user instruction following and world knowledge.IFEval: Measures how well a model adheres to user-provided instructionsMMLU: Evaluates world knowledge across diverse domainsHow Sonar Answers QuestionsWhat does increased factuality and readability look like in practice? Compare these searches, which were conducted on both Sonar and other models to see the difference.Use Sonar TodaySonar excels at providing fast and accurate answers, making it a great model for everyday use. Perplexity Pro users can make Sonar their default model in their settings. It can also be used via the Sonar API offering*.Footnotes†These evaluations are graded on a scale from 0 to 100, where higher is better*The Sonar model served through the API does not yet run on Cerebras inference infrastructure, but it is coming out shortlyMeet new Sonar: A Blazing Fast Model Optimized for Perplexity SearchStarting today, all Perplexity Pro users will be able to try out the latest version of Sonar, Perplexity's in-house model that is optimized for answer quality and user experience. Built on top of Llama 3.3 70B, Sonar has been further trained to enhance answer factuality and readability for Perplexity’s default search mode.Through comprehensive online A/B testing, we have found that Sonar significantly outperforms models in its class, like GPT-4o mini and Claude 3.5 Haiku, while closely matching or exceeding the performance of frontier models like GPT-4o and Claude 3.5 Sonnet for user satisfaction. Powered by Cerebras inference infrastructure, Sonar runs at a blazing fast speed of 1200 tokens per second — enabling nearly instant answer generation.Perplexity Users Prefer SonarWe conducted extensive online A/B testing of Sonar to measure user satisfaction — a key metric that is strongly correlated with product usage and retention. This metric captures how satisfied and engaged users are when asking questions on Perplexity.Our evaluations show that Sonar surpasses both GPT-4o mini and Claude 3.5 Haiku by a substantial margin. We also compared Sonar against more expensive frontier models and found that it outperforms Claude 3.5 Sonnet, while closely approaching the performance of GPT-4o at a fraction of the price and more than 10x the speed.Blazing Fast Speed Enables Instant AnswersPowered by Cerebras inference infrastructure, Sonar delivers answers at blazing fast speeds, achieving a decoding throughput that is nearly 10x faster than comparable models like Gemini 2.0 Flash. This enables nearly instant answer generation, which makes Sonar ideal for quick information retrieval or detailed question answering use cases.Sonar Excels at Providing Factual and High Quality AnswersWe optimized Sonar across two critical dimensions that strongly correlate with user satisfaction — answer factuality and readability.Answer Factuality: Measures how well a model can answer questions using facts that are grounded in search results, and its ability to resolve conflicting or missing informationReadability: Measures a model’s ability to provide a concise and detailed answer with the appropriate use of markdown formatting for organizing textOur results demonstrate that Sonar significantly improves these aspects compared to the base model, Llama 3.3 70B Instruct, and even surpasses other frontier models in these key areas†.Additionally, Sonar also exceeds in-class models like GPT-4o mini and Claude 3.5 Haiku on academic benchmarks that measure user instruction following and world knowledge.IFEval: Measures how well a model adheres to user-provided instructionsMMLU: Evaluates world knowledge across diverse domainsHow Sonar Answers QuestionsWhat does increased factuality and readability look like in practice? Compare these searches, which were conducted on both Sonar and other models to see the difference.Use Sonar TodaySonar excels at providing fast and accurate answers, making it a great model for everyday use. Perplexity Pro users can make Sonar their default model in their settings. It can also be used via the Sonar API offering*.Footnotes†These evaluations are graded on a scale from 0 to 100, where higher is better*The Sonar model served through the API does not yet run on Cerebras inference infrastructure, but it is coming out shortlyMeet new Sonar: A Blazing Fast Model Optimized for Perplexity SearchStarting today, all Perplexity Pro users will be able to try out the latest version of Sonar, Perplexity's in-house model that is optimized for answer quality and user experience. Built on top of Llama 3.3 70B, Sonar has been further trained to enhance answer factuality and readability for Perplexity’s default search mode.Through comprehensive online A/B testing, we have found that Sonar significantly outperforms models in its class, like GPT-4o mini and Claude 3.5 Haiku, while closely matching or exceeding the performance of frontier models like GPT-4o and Claude 3.5 Sonnet for user satisfaction. Powered by Cerebras inference infrastructure, Sonar runs at a blazing fast speed of 1200 tokens per second — enabling nearly instant answer generation.Perplexity Users Prefer SonarWe conducted extensive online A/B testing of Sonar to measure user satisfaction — a key metric that is strongly correlated with product usage and retention. This metric captures how satisfied and engaged users are when asking questions on Perplexity.Our evaluations show that Sonar surpasses both GPT-4o mini and Claude 3.5 Haiku by a substantial margin. We also compared Sonar against more expensive frontier models and found that it outperforms Claude 3.5 Sonnet, while closely approaching the performance of GPT-4o at a fraction of the price and more than 10x the speed.Blazing Fast Speed Enables Instant AnswersPowered by Cerebras inference infrastructure, Sonar delivers answers at blazing fast speeds, achieving a decoding throughput that is nearly 10x faster than comparable models like Gemini 2.0 Flash. This enables nearly instant answer generation, which makes Sonar ideal for quick information retrieval or detailed question answering use cases.Sonar Excels at Providing Factual and High Quality AnswersWe optimized Sonar across two critical dimensions that strongly correlate with user satisfaction — answer factuality and readability.Answer Factuality: Measures how well a model can answer questions using facts that are grounded in search results, and its ability to resolve conflicting or missing informationReadability: Measures a model’s ability to provide a concise and detailed answer with the appropriate use of markdown formatting for organizing textOur results demonstrate that Sonar significantly improves these aspects compared to the base model, Llama 3.3 70B Instruct, and even surpasses other frontier models in these key areas†.Additionally, Sonar also exceeds in-class models like GPT-4o mini and Claude 3.5 Haiku on academic benchmarks that measure user instruction following and world knowledge.IFEval: Measures how well a model adheres to user-provided instructionsMMLU: Evaluates world knowledge across diverse domainsHow Sonar Answers QuestionsWhat does increased factuality and readability look like in practice? Compare these searches, which were conducted on both Sonar and other models to see the difference.Use Sonar TodaySonar excels at providing fast and accurate answers, making it a great model for everyday use. Perplexity Pro users can make Sonar their default model in their settings. It can also be used via the Sonar API offering*.Footnotes†These evaluations are graded on a scale from 0 to 100, where higher is better*The Sonar model served through the API does not yet run on Cerebras inference infrastructure, but it is coming out shortly\"),\n",
       " Document(metadata={'source': 'https://www.perplexity.ai/hub/blog/introducing-the-sonar-pro-api'}, page_content='Introducing the Sonar Pro API It\\'s never been a better time to build with AI. But as AI tools become more pervasive, accuracy is paramount. While most generative AI features today have answers informed only by training data, this limits their capabilities. To optimize for factuality and authority, APIs require a real-time connection to the Internet, with answers informed by trusted sources.\\xa0With Perplexity\\'s Sonar and Sonar Pro API (the latter generally available to all developers starting today), you can build your own generative search capabilities powered with unparalleled real-time, web-wide research and the Perplexity features you\\'ve come to expect, like citations. We’re also expanding public access to advanced features like JSON mode and search domain filters for select usage tiers.\\xa0Our Sonar API is lightweight, affordable, fast, and simple to use — and now includes citations and the ability to customize sources (our most requested feature updates!). It\\'s ideal for companies who need to integrate lightweight question-and-answer features that are optimized for speed.\\xa0For enterprises seeking more advanced capabilities, the Sonar Pro API can handle in-depth, multi-step queries with added extensibility, like double the number of citations per search as Sonar on average. Plus, with a larger context window, it can handle longer and more nuanced searches and follow-up questions.\\xa0Sonar in actionThousands of developers across industries like sales, finance, nonprofits, and advertising, are using Sonar to power native search experiences. Here are just a few examples of what companies can build:\\xa0Copy AI helps go-to-market teams research prospects and target companies. Thanks to their integration with Sonar, they\\'ve helped companies save 8 hours of research per rep per week, generating a 20% increase in throughput.Doximity provides doctors with access to a research tool that helps them practice medicine more effectively. Whether they are seeking information about a new guideline change from a medical journal or have a question about insurance reimbursements, doctors can get concise, research-backed answers to their questions. Given the high-stakes nature of medicine, in-line citations are critical for furthering accuracy and trust.\\xa0Earlier this year, Zoom introduced the AI Companion 2.0, which natively integrates different services directly into their video conferencing platform. With Sonar Pro, Zoom can offer live, real-time, and private searches for users that allow them to ask any question without having to exit the video call or open their browser. In the words of Will Siegelin, Senior Product Manager of AI Products & Responsible AI at Zoom,\" Perplexity opens Zoom to knowledge beyond its four walls.\"\\xa0Purpose-Built for Factuality\\xa0SimpleQA is the prevailing benchmark for factuality, designed to evaluate whether large language models can answer short, fact-seeking questions. Sonar Pro is the best-performing model on factuality because it combines the summarization power of LLMs with access to real-time information rather than relying on stored training data to answer questions. Sonar Pro leads this benchmark with an F-score of 0.858, while Sonar received an F-score of 0.773.\\xa0To get started with the Sonar API, check out the documentation on our website.Introducing the Sonar Pro API It\\'s never been a better time to build with AI. But as AI tools become more pervasive, accuracy is paramount. While most generative AI features today have answers informed only by training data, this limits their capabilities. To optimize for factuality and authority, APIs require a real-time connection to the Internet, with answers informed by trusted sources.\\xa0With Perplexity\\'s Sonar and Sonar Pro API (the latter generally available to all developers starting today), you can build your own generative search capabilities powered with unparalleled real-time, web-wide research and the Perplexity features you\\'ve come to expect, like citations. We’re also expanding public access to advanced features like JSON mode and search domain filters for select usage tiers.\\xa0Our Sonar API is lightweight, affordable, fast, and simple to use — and now includes citations and the ability to customize sources (our most requested feature updates!). It\\'s ideal for companies who need to integrate lightweight question-and-answer features that are optimized for speed.\\xa0For enterprises seeking more advanced capabilities, the Sonar Pro API can handle in-depth, multi-step queries with added extensibility, like double the number of citations per search as Sonar on average. Plus, with a larger context window, it can handle longer and more nuanced searches and follow-up questions.\\xa0Sonar in actionThousands of developers across industries like sales, finance, nonprofits, and advertising, are using Sonar to power native search experiences. Here are just a few examples of what companies can build:\\xa0Copy AI helps go-to-market teams research prospects and target companies. Thanks to their integration with Sonar, they\\'ve helped companies save 8 hours of research per rep per week, generating a 20% increase in throughput.Doximity provides doctors with access to a research tool that helps them practice medicine more effectively. Whether they are seeking information about a new guideline change from a medical journal or have a question about insurance reimbursements, doctors can get concise, research-backed answers to their questions. Given the high-stakes nature of medicine, in-line citations are critical for furthering accuracy and trust.\\xa0Earlier this year, Zoom introduced the AI Companion 2.0, which natively integrates different services directly into their video conferencing platform. With Sonar Pro, Zoom can offer live, real-time, and private searches for users that allow them to ask any question without having to exit the video call or open their browser. In the words of Will Siegelin, Senior Product Manager of AI Products & Responsible AI at Zoom,\" Perplexity opens Zoom to knowledge beyond its four walls.\"\\xa0Purpose-Built for Factuality\\xa0SimpleQA is the prevailing benchmark for factuality, designed to evaluate whether large language models can answer short, fact-seeking questions. Sonar Pro is the best-performing model on factuality because it combines the summarization power of LLMs with access to real-time information rather than relying on stored training data to answer questions. Sonar Pro leads this benchmark with an F-score of 0.858, while Sonar received an F-score of 0.773.\\xa0To get started with the Sonar API, check out the documentation on our website.Introducing the Sonar Pro API It\\'s never been a better time to build with AI. But as AI tools become more pervasive, accuracy is paramount. While most generative AI features today have answers informed only by training data, this limits their capabilities. To optimize for factuality and authority, APIs require a real-time connection to the Internet, with answers informed by trusted sources.\\xa0With Perplexity\\'s Sonar and Sonar Pro API (the latter generally available to all developers starting today), you can build your own generative search capabilities powered with unparalleled real-time, web-wide research and the Perplexity features you\\'ve come to expect, like citations. We’re also expanding public access to advanced features like JSON mode and search domain filters for select usage tiers.\\xa0Our Sonar API is lightweight, affordable, fast, and simple to use — and now includes citations and the ability to customize sources (our most requested feature updates!). It\\'s ideal for companies who need to integrate lightweight question-and-answer features that are optimized for speed.\\xa0For enterprises seeking more advanced capabilities, the Sonar Pro API can handle in-depth, multi-step queries with added extensibility, like double the number of citations per search as Sonar on average. Plus, with a larger context window, it can handle longer and more nuanced searches and follow-up questions.\\xa0Sonar in actionThousands of developers across industries like sales, finance, nonprofits, and advertising, are using Sonar to power native search experiences. Here are just a few examples of what companies can build:\\xa0Copy AI helps go-to-market teams research prospects and target companies. Thanks to their integration with Sonar, they\\'ve helped companies save 8 hours of research per rep per week, generating a 20% increase in throughput.Doximity provides doctors with access to a research tool that helps them practice medicine more effectively. Whether they are seeking information about a new guideline change from a medical journal or have a question about insurance reimbursements, doctors can get concise, research-backed answers to their questions. Given the high-stakes nature of medicine, in-line citations are critical for furthering accuracy and trust.\\xa0Earlier this year, Zoom introduced the AI Companion 2.0, which natively integrates different services directly into their video conferencing platform. With Sonar Pro, Zoom can offer live, real-time, and private searches for users that allow them to ask any question without having to exit the video call or open their browser. In the words of Will Siegelin, Senior Product Manager of AI Products & Responsible AI at Zoom,\" Perplexity opens Zoom to knowledge beyond its four walls.\"\\xa0Purpose-Built for Factuality\\xa0SimpleQA is the prevailing benchmark for factuality, designed to evaluate whether large language models can answer short, fact-seeking questions. Sonar Pro is the best-performing model on factuality because it combines the summarization power of LLMs with access to real-time information rather than relying on stored training data to answer questions. Sonar Pro leads this benchmark with an F-score of 0.858, while Sonar received an F-score of 0.773.\\xa0To get started with the Sonar API, check out the documentation on our website.'),\n",
       " Document(metadata={'source': 'https://www.perplexity.ai/hub/blog/welcoming-carbon-to-the-perplexity-team'}, page_content=\"Perplexity acquires data connectivity startup Carbon\\xa0We're excited to announce that we've acquired Carbon, a retrieval engine that connects external data sources to large language models. Soon, we'll integrate Carbon's data connectors into our tech stack, allowing users to connect apps like Notion and Google Docs directly to Perplexity. As part of the acquisition, we're excited to welcome all members of the Carbon team to Perplexity to expedite our roadmap and ship new features faster.\\xa0We believe your AI should be personal to you — at home, work, or on the go — and data connectivity is a key part of everyone's daily workflows. Carbon will make it easier for Perplexity's answer engine to be informed by diverse sources of information, whether that data resides in internal databases, cloud storage, or document repositories.Rather than making users search through many different web pages, apps, and messages to find the answer they're looking for, we see a future where Perplexity does the research for you, bringing the most critical insights across sources together as part of the answer. Carbon will simplify the process for our users to connect the data sources that matter to them.\\xa02024 was an unprecedented year of growth and product evolution for Perplexity. We launched Enterprise Pro, introduced new capabilities like Spaces and Internal File Search, and powered new search experiences, like shopping and analyzing live earning call transcripts. We're proud of what we've accomplished, and have lots more to come in 2025.\\xa0To stay updated on the latest Perplexity developments, follow us on Twitter/X, Threads, and LinkedIn.Perplexity acquires data connectivity startup Carbon\\xa0We're excited to announce that we've acquired Carbon, a retrieval engine that connects external data sources to large language models. Soon, we'll integrate Carbon's data connectors into our tech stack, allowing users to connect apps like Notion and Google Docs directly to Perplexity. As part of the acquisition, we're excited to welcome all members of the Carbon team to Perplexity to expedite our roadmap and ship new features faster.\\xa0We believe your AI should be personal to you — at home, work, or on the go — and data connectivity is a key part of everyone's daily workflows. Carbon will make it easier for Perplexity's answer engine to be informed by diverse sources of information, whether that data resides in internal databases, cloud storage, or document repositories.Rather than making users search through many different web pages, apps, and messages to find the answer they're looking for, we see a future where Perplexity does the research for you, bringing the most critical insights across sources together as part of the answer. Carbon will simplify the process for our users to connect the data sources that matter to them.\\xa02024 was an unprecedented year of growth and product evolution for Perplexity. We launched Enterprise Pro, introduced new capabilities like Spaces and Internal File Search, and powered new search experiences, like shopping and analyzing live earning call transcripts. We're proud of what we've accomplished, and have lots more to come in 2025.\\xa0To stay updated on the latest Perplexity developments, follow us on Twitter/X, Threads, and LinkedIn.Perplexity acquires data connectivity startup Carbon\\xa0We're excited to announce that we've acquired Carbon, a retrieval engine that connects external data sources to large language models. Soon, we'll integrate Carbon's data connectors into our tech stack, allowing users to connect apps like Notion and Google Docs directly to Perplexity. As part of the acquisition, we're excited to welcome all members of the Carbon team to Perplexity to expedite our roadmap and ship new features faster.\\xa0We believe your AI should be personal to you — at home, work, or on the go — and data connectivity is a key part of everyone's daily workflows. Carbon will make it easier for Perplexity's answer engine to be informed by diverse sources of information, whether that data resides in internal databases, cloud storage, or document repositories.Rather than making users search through many different web pages, apps, and messages to find the answer they're looking for, we see a future where Perplexity does the research for you, bringing the most critical insights across sources together as part of the answer. Carbon will simplify the process for our users to connect the data sources that matter to them.\\xa02024 was an unprecedented year of growth and product evolution for Perplexity. We launched Enterprise Pro, introduced new capabilities like Spaces and Internal File Search, and powered new search experiences, like shopping and analyzing live earning call transcripts. We're proud of what we've accomplished, and have lots more to come in 2025.\\xa0To stay updated on the latest Perplexity developments, follow us on Twitter/X, Threads, and LinkedIn.\"),\n",
       " Document(metadata={'source': 'https://www.perplexity.ai/hub/blog/perplexity-s-2024-campus-strategist-program'}, page_content='Perplexity’s 2024 Campus Strategist ProgramThis year, we invited college students across the country to apply for Perplexity’s inaugural Campus Strategist Program, an initiative designed to empower students to bring AI search to their campuses through events, outreach, and community-building. Strategists were given a marketing budget, mentorship from our team, and Perplexity vouchers and merch, then got to work brainstorming and executing creative events that show how Perplexity can enhance academic performance and college life.Our Strategists have been busy—from hackathons to tech talks, they’ve each brought their own unique spin to spreading awareness of Perplexity among their classmates. Here’s a look at some activations that have made an outsized impact.Think you would make a great campus leader for Perplexity? We’re now accepting applications for our spring 2025 program. Apply here.The Future of Search Event at MITAt MIT, Campus Strategist Honey Pamnani organized the “Future of Search” event, with support from Harvard Campus Strategist Aditya Agarwal. The session—which drew over 200 attendees from MIT, Harvard, and Northeastern—combined live on-campus discussions with MIT professors, and virtual talks from Perplexity’s leadership. Featuring insights from AI experts and an interactive Q&A, the event showcased Perplexity’s role in advancing search technology while fostering collaboration across Boston’s academic community.DubHacks 2024 at University of WashingtonAt DubHacks 2024, the Pacific Northwest’s largest collegiate hackathon, Campus Strategist Kunal Srivastava represented Perplexity. Over 700 students participated in Perplexity’s “Amplifying Knowledge” track, which became the most popular track at the hackathon, using the Perplexity API in projects focused on social good and LLM-powered tech. The winning project, Triage, developed a toolkit for natural disaster management, using Perplexity to deliver critical real-time information.See the winning project on Devpost.Gameday Flyover at University of Southern CaliforniaUSC strategist Stan Loosmore approached us with the idea to fly a Perplexity banner over the highly anticipated Thanksgiving football game, an idea our team immediately recognized as unique and clever. Thanks to Stan’s efforts, thousands of USC students were asking Perplexity, “Why is USC better than Notre Dame?”Boba and Perplexity Power Hour at Carnegie Mellon UniversityCampus Strategist Anya Bindra hosted the CMU Boba & Perplexity Power Hour, where 120 students explored Perplexity’s latest features—like Finance Search and Image Generation—while enjoying free boba and experimenting with Perplexity. Anya showed attendees how Perplexity can answer any question and boost productivity.Perplexity at WhartonCampus strategist Nae Nae (Montawan) Chairatchaneeboon hosted a speaker event at Penn/Wharton featuring Perplexity’s Head of Growth Raman Malik. More than 150 students—from undergraduates to MBAs—attended to hear Raman’s insights on scaling startups, drawing from his experience at Perplexity, Lyft, and as a founder. His talk covered strategies for growth and highlighted Perplexity’s impact in AI for business and education. Following the session, students participated in an exclusive networking event with student VC groups, building connections across the entrepreneurial community.Perplexity Case Competition at Santa Clara UniversityHenry Gewecke, the Perplexity strategist at SCU, spearheaded an innovative competition in partnership with the Leavey School of Business for over 50 teams of business students. Teams used Perplexity to research a prompt related to how businesses can incorporate social impact and sustainability in their business practices. Congrats to the winning team, which dove into the fast fashion industry to workshop exciting strategy recommendations that addressed production sustainability and efficient material usage.Experience Perplexity for studentsFor students tackling research, brainstorming ideas, or prepping for exams, Perplexity offers a faster, smarter way to find answers and explore any topic in depth. Discover how Perplexity can simplify studying and help you get more out of what you learn.Try PerplexityPerplexity’s 2024 Campus Strategist ProgramThis year, we invited college students across the country to apply for Perplexity’s inaugural Campus Strategist Program, an initiative designed to empower students to bring AI search to their campuses through events, outreach, and community-building. Strategists were given a marketing budget, mentorship from our team, and Perplexity vouchers and merch, then got to work brainstorming and executing creative events that show how Perplexity can enhance academic performance and college life.Our Strategists have been busy—from hackathons to tech talks, they’ve each brought their own unique spin to spreading awareness of Perplexity among their classmates. Here’s a look at some activations that have made an outsized impact.Think you would make a great campus leader for Perplexity? We’re now accepting applications for our spring 2025 program. Apply here.The Future of Search Event at MITAt MIT, Campus Strategist Honey Pamnani organized the “Future of Search” event, with support from Harvard Campus Strategist Aditya Agarwal. The session—which drew over 200 attendees from MIT, Harvard, and Northeastern—combined live on-campus discussions with MIT professors, and virtual talks from Perplexity’s leadership. Featuring insights from AI experts and an interactive Q&A, the event showcased Perplexity’s role in advancing search technology while fostering collaboration across Boston’s academic community.DubHacks 2024 at University of WashingtonAt DubHacks 2024, the Pacific Northwest’s largest collegiate hackathon, Campus Strategist Kunal Srivastava represented Perplexity. Over 700 students participated in Perplexity’s “Amplifying Knowledge” track, which became the most popular track at the hackathon, using the Perplexity API in projects focused on social good and LLM-powered tech. The winning project, Triage, developed a toolkit for natural disaster management, using Perplexity to deliver critical real-time information.See the winning project on Devpost.Gameday Flyover at University of Southern CaliforniaUSC strategist Stan Loosmore approached us with the idea to fly a Perplexity banner over the highly anticipated Thanksgiving football game, an idea our team immediately recognized as unique and clever. Thanks to Stan’s efforts, thousands of USC students were asking Perplexity, “Why is USC better than Notre Dame?”Boba and Perplexity Power Hour at Carnegie Mellon UniversityCampus Strategist Anya Bindra hosted the CMU Boba & Perplexity Power Hour, where 120 students explored Perplexity’s latest features—like Finance Search and Image Generation—while enjoying free boba and experimenting with Perplexity. Anya showed attendees how Perplexity can answer any question and boost productivity.Perplexity at WhartonCampus strategist Nae Nae (Montawan) Chairatchaneeboon hosted a speaker event at Penn/Wharton featuring Perplexity’s Head of Growth Raman Malik. More than 150 students—from undergraduates to MBAs—attended to hear Raman’s insights on scaling startups, drawing from his experience at Perplexity, Lyft, and as a founder. His talk covered strategies for growth and highlighted Perplexity’s impact in AI for business and education. Following the session, students participated in an exclusive networking event with student VC groups, building connections across the entrepreneurial community.Perplexity Case Competition at Santa Clara UniversityHenry Gewecke, the Perplexity strategist at SCU, spearheaded an innovative competition in partnership with the Leavey School of Business for over 50 teams of business students. Teams used Perplexity to research a prompt related to how businesses can incorporate social impact and sustainability in their business practices. Congrats to the winning team, which dove into the fast fashion industry to workshop exciting strategy recommendations that addressed production sustainability and efficient material usage.Experience Perplexity for studentsFor students tackling research, brainstorming ideas, or prepping for exams, Perplexity offers a faster, smarter way to find answers and explore any topic in depth. Discover how Perplexity can simplify studying and help you get more out of what you learn.Try PerplexityPerplexity’s 2024 Campus Strategist ProgramThis year, we invited college students across the country to apply for Perplexity’s inaugural Campus Strategist Program, an initiative designed to empower students to bring AI search to their campuses through events, outreach, and community-building. Strategists were given a marketing budget, mentorship from our team, and Perplexity vouchers and merch, then got to work brainstorming and executing creative events that show how Perplexity can enhance academic performance and college life.Our Strategists have been busy—from hackathons to tech talks, they’ve each brought their own unique spin to spreading awareness of Perplexity among their classmates. Here’s a look at some activations that have made an outsized impact.Think you would make a great campus leader for Perplexity? We’re now accepting applications for our spring 2025 program. Apply here.The Future of Search Event at MITAt MIT, Campus Strategist Honey Pamnani organized the “Future of Search” event, with support from Harvard Campus Strategist Aditya Agarwal. The session—which drew over 200 attendees from MIT, Harvard, and Northeastern—combined live on-campus discussions with MIT professors, and virtual talks from Perplexity’s leadership. Featuring insights from AI experts and an interactive Q&A, the event showcased Perplexity’s role in advancing search technology while fostering collaboration across Boston’s academic community.DubHacks 2024 at University of WashingtonAt DubHacks 2024, the Pacific Northwest’s largest collegiate hackathon, Campus Strategist Kunal Srivastava represented Perplexity. Over 700 students participated in Perplexity’s “Amplifying Knowledge” track, which became the most popular track at the hackathon, using the Perplexity API in projects focused on social good and LLM-powered tech. The winning project, Triage, developed a toolkit for natural disaster management, using Perplexity to deliver critical real-time information.See the winning project on Devpost.Gameday Flyover at University of Southern CaliforniaUSC strategist Stan Loosmore approached us with the idea to fly a Perplexity banner over the highly anticipated Thanksgiving football game, an idea our team immediately recognized as unique and clever. Thanks to Stan’s efforts, thousands of USC students were asking Perplexity, “Why is USC better than Notre Dame?”Boba and Perplexity Power Hour at Carnegie Mellon UniversityCampus Strategist Anya Bindra hosted the CMU Boba & Perplexity Power Hour, where 120 students explored Perplexity’s latest features—like Finance Search and Image Generation—while enjoying free boba and experimenting with Perplexity. Anya showed attendees how Perplexity can answer any question and boost productivity.Perplexity at WhartonCampus strategist Nae Nae (Montawan) Chairatchaneeboon hosted a speaker event at Penn/Wharton featuring Perplexity’s Head of Growth Raman Malik. More than 150 students—from undergraduates to MBAs—attended to hear Raman’s insights on scaling startups, drawing from his experience at Perplexity, Lyft, and as a founder. His talk covered strategies for growth and highlighted Perplexity’s impact in AI for business and education. Following the session, students participated in an exclusive networking event with student VC groups, building connections across the entrepreneurial community.Perplexity Case Competition at Santa Clara UniversityHenry Gewecke, the Perplexity strategist at SCU, spearheaded an innovative competition in partnership with the Leavey School of Business for over 50 teams of business students. Teams used Perplexity to research a prompt related to how businesses can incorporate social impact and sustainability in their business practices. Congrats to the winning team, which dove into the fast fashion industry to workshop exciting strategy recommendations that addressed production sustainability and efficient material usage.Experience Perplexity for studentsFor students tackling research, brainstorming ideas, or prepping for exams, Perplexity offers a faster, smarter way to find answers and explore any topic in depth. Discover how Perplexity can simplify studying and help you get more out of what you learn.Try Perplexity'),\n",
       " Document(metadata={'source': 'https://www.perplexity.ai/hub/blog/perplexity-expands-publisher-program-with-15-new-media-partners'}, page_content='Welcoming more global news organizations to Perplexity’s Publishers’ ProgramWe believe deeply in the power of an open and fair information ecosystem for expressing our curiosity and creating a more informed society. That’s why we launched the Perplexity Publishers’ Program earlier this year—to ensure publishers can benefit as we grow together.Today, we’re excited to welcome over a dozen new partners to Perplexity’s Publishers’ Program: ADWEEK, Blavity, DPReview, Gear Patrol, The Independent, Lee Enterprises, Los Angeles Times, MediaLab, Mexico News Daily, Minkabu Infonoid, NewsPicks, Prisa Media, RTL Germany brands stern and ntv, and World History Encyclopedia.These media brands represent the diverse interests of our international user base and serve as our first partners from the United Kingdom, Japan, Spain and Latin America. Their content reflects the vast array of topics people search for, featuring specialized trade coverage on subjects like marketing and history to popular culture and in-depth local reporting on their communities.With these new partners joining existing ones like TIME, Fortune, and Der Spiegel, we can enrich our knowledge base and provide insights that resonate with users from different backgrounds and geographies. Their participation ensures that our responses to user queries remain comprehensive, nuanced, and reflect the interconnectedness of our world.As part of the program, these publishers will share in revenue generated from advertising. They’ll also have access to our APIs and developer support to build unique features using our proprietary search technology. They will receive free Perplexity Enterprise Pro for their entire organization for a year. Additionally, they’ll receive data analytics to help track trends and content performance as long as they remain in the program.Since publicly launching this program in July, we have been pleasantly overwhelmed by the interest from all across the news media in learning more about this program. We’ve heard from over 100 publishers to learn more about how the program works, how it could benefit their organization, and ways to get involved. With the growing demand and Perplexity’s ongoing investment in Publishers, we’ve welcomed Jessica Chan—who previously built LinkedIn’s content partner programs—as our new Head of Publisher Partnerships. Jessica will lead and expand this program, ensuring it continues to deliver value for both publishers and users.“Perplexity’s first-of-its-kind publisher program reimagines how technology companies and news publishers collaborate, ensuring that media companies can benefit as we continue to grow,” says Chan. “We would not be able to serve factual, valuable answers without news organizations continuing to report on different topics. We’re excited to welcome these new publishers to the program, representing over 25 countries across four continents.”We also appreciate our other partners who are supporting journalists, like Northwestern University’s Medill School of Journalism, Media, Integrated Marketing Communications, who are furthering critical research into how newsrooms can responsibly use AI in editorial workflows, and beehiiv, which is offering free Perplexity Pro to members of its recently-launched Media Collective.We’re excited to welcome more partners in the coming weeks. If you want to learn more about joining the Publisher Program, email us at publishers@perplexity.aiWelcoming more global news organizations to Perplexity’s Publishers’ ProgramWe believe deeply in the power of an open and fair information ecosystem for expressing our curiosity and creating a more informed society. That’s why we launched the Perplexity Publishers’ Program earlier this year—to ensure publishers can benefit as we grow together.Today, we’re excited to welcome over a dozen new partners to Perplexity’s Publishers’ Program: ADWEEK, Blavity, DPReview, Gear Patrol, The Independent, Lee Enterprises, Los Angeles Times, MediaLab, Mexico News Daily, Minkabu Infonoid, NewsPicks, Prisa Media, RTL Germany brands stern and ntv, and World History Encyclopedia.These media brands represent the diverse interests of our international user base and serve as our first partners from the United Kingdom, Japan, Spain and Latin America. Their content reflects the vast array of topics people search for, featuring specialized trade coverage on subjects like marketing and history to popular culture and in-depth local reporting on their communities.With these new partners joining existing ones like TIME, Fortune, and Der Spiegel, we can enrich our knowledge base and provide insights that resonate with users from different backgrounds and geographies. Their participation ensures that our responses to user queries remain comprehensive, nuanced, and reflect the interconnectedness of our world.As part of the program, these publishers will share in revenue generated from advertising. They’ll also have access to our APIs and developer support to build unique features using our proprietary search technology. They will receive free Perplexity Enterprise Pro for their entire organization for a year. Additionally, they’ll receive data analytics to help track trends and content performance as long as they remain in the program.Since publicly launching this program in July, we have been pleasantly overwhelmed by the interest from all across the news media in learning more about this program. We’ve heard from over 100 publishers to learn more about how the program works, how it could benefit their organization, and ways to get involved. With the growing demand and Perplexity’s ongoing investment in Publishers, we’ve welcomed Jessica Chan—who previously built LinkedIn’s content partner programs—as our new Head of Publisher Partnerships. Jessica will lead and expand this program, ensuring it continues to deliver value for both publishers and users.“Perplexity’s first-of-its-kind publisher program reimagines how technology companies and news publishers collaborate, ensuring that media companies can benefit as we continue to grow,” says Chan. “We would not be able to serve factual, valuable answers without news organizations continuing to report on different topics. We’re excited to welcome these new publishers to the program, representing over 25 countries across four continents.”We also appreciate our other partners who are supporting journalists, like Northwestern University’s Medill School of Journalism, Media, Integrated Marketing Communications, who are furthering critical research into how newsrooms can responsibly use AI in editorial workflows, and beehiiv, which is offering free Perplexity Pro to members of its recently-launched Media Collective.We’re excited to welcome more partners in the coming weeks. If you want to learn more about joining the Publisher Program, email us at publishers@perplexity.aiWelcoming more global news organizations to Perplexity’s Publishers’ ProgramWe believe deeply in the power of an open and fair information ecosystem for expressing our curiosity and creating a more informed society. That’s why we launched the Perplexity Publishers’ Program earlier this year—to ensure publishers can benefit as we grow together.Today, we’re excited to welcome over a dozen new partners to Perplexity’s Publishers’ Program: ADWEEK, Blavity, DPReview, Gear Patrol, The Independent, Lee Enterprises, Los Angeles Times, MediaLab, Mexico News Daily, Minkabu Infonoid, NewsPicks, Prisa Media, RTL Germany brands stern and ntv, and World History Encyclopedia.These media brands represent the diverse interests of our international user base and serve as our first partners from the United Kingdom, Japan, Spain and Latin America. Their content reflects the vast array of topics people search for, featuring specialized trade coverage on subjects like marketing and history to popular culture and in-depth local reporting on their communities.With these new partners joining existing ones like TIME, Fortune, and Der Spiegel, we can enrich our knowledge base and provide insights that resonate with users from different backgrounds and geographies. Their participation ensures that our responses to user queries remain comprehensive, nuanced, and reflect the interconnectedness of our world.As part of the program, these publishers will share in revenue generated from advertising. They’ll also have access to our APIs and developer support to build unique features using our proprietary search technology. They will receive free Perplexity Enterprise Pro for their entire organization for a year. Additionally, they’ll receive data analytics to help track trends and content performance as long as they remain in the program.Since publicly launching this program in July, we have been pleasantly overwhelmed by the interest from all across the news media in learning more about this program. We’ve heard from over 100 publishers to learn more about how the program works, how it could benefit their organization, and ways to get involved. With the growing demand and Perplexity’s ongoing investment in Publishers, we’ve welcomed Jessica Chan—who previously built LinkedIn’s content partner programs—as our new Head of Publisher Partnerships. Jessica will lead and expand this program, ensuring it continues to deliver value for both publishers and users.“Perplexity’s first-of-its-kind publisher program reimagines how technology companies and news publishers collaborate, ensuring that media companies can benefit as we continue to grow,” says Chan. “We would not be able to serve factual, valuable answers without news organizations continuing to report on different topics. We’re excited to welcome these new publishers to the program, representing over 25 countries across four continents.”We also appreciate our other partners who are supporting journalists, like Northwestern University’s Medill School of Journalism, Media, Integrated Marketing Communications, who are furthering critical research into how newsrooms can responsibly use AI in editorial workflows, and beehiiv, which is offering free Perplexity Pro to members of its recently-launched Media Collective.We’re excited to welcome more partners in the coming weeks. If you want to learn more about joining the Publisher Program, email us at publishers@perplexity.ai'),\n",
       " Document(metadata={'source': 'https://www.perplexity.ai/hub/blog/research-nonprofits-with-charity-navigator-on-perplexity'}, page_content='Research nonprofits with Charity Navigator on Perplexity\\xa0Today is Giving Tuesday, and we\\'re making it easier than ever to research and donate to U.S. nonprofits thanks to a new collaboration with Charity Navigator.\\xa0Charity Navigator is America’s largest and most trusted charity evaluator. For more than two decades, they’ve helped people make informed giving decisions by analyzing over 225,000 U.S. registered nonprofits for accountability, impact, leadership, and culture. Starting tomorrow, you can tap into these insights directly through Perplexity.\\xa0\\xa0Giving Tuesday is the biggest donation day in the U.S., with about 34 million people giving over $3 billion last year alone. By partnering with Charity Navigator, we\\'re making it easier to find causes that match your values, so you can give confidently and make a meaningful impact.If you\\'re curious about a nonprofit\\'s financial health, program effectiveness, or leadership structure, Perplexity will gather clear, actionable information from Charity Navigator\\'s extensive database.“Giving is evolving and AI tools like Perplexity that leverage premium content, like Charity Navigator ratings, provide users with the information they need to give with impact,” said Michael Thatcher, President & CEO at Charity Navigator. “Not only can donors search and find information about charities, but they can also take immediate action by donating through the Charity Navigator Giving Basket.”\\xa0Johnny Ho, Perplexity’s Co-Founder, says: \"We’re always looking for ways to make our answer engine more helpful, and partnering with Charity Navigator simplifies how people research nonprofits. If you\\'re looking to give back but don\\'t know where to start, just ask Perplexity, and it will guide you to charities that align with your goals.\"\\xa0Whether you\\'re a first-time donor or a seasoned philanthropist, we’re here to help you find and connect with causes that matter.\\xa0If you are a nonprofit interested in verifying how your data is presented through Perplexity, we recommend making sure your profile is up-to-date with Charity Navigator. If you are interested in using Perplexity, we offer Enterprise Pro at a discounted rate for nonprofits. Thanks to our partner ScalePost for initiating this collaboration.Research nonprofits with Charity Navigator on Perplexity\\xa0Today is Giving Tuesday, and we\\'re making it easier than ever to research and donate to U.S. nonprofits thanks to a new collaboration with Charity Navigator.\\xa0Charity Navigator is America’s largest and most trusted charity evaluator. For more than two decades, they’ve helped people make informed giving decisions by analyzing over 225,000 U.S. registered nonprofits for accountability, impact, leadership, and culture. Starting tomorrow, you can tap into these insights directly through Perplexity.\\xa0\\xa0Giving Tuesday is the biggest donation day in the U.S., with about 34 million people giving over $3 billion last year alone. By partnering with Charity Navigator, we\\'re making it easier to find causes that match your values, so you can give confidently and make a meaningful impact.If you\\'re curious about a nonprofit\\'s financial health, program effectiveness, or leadership structure, Perplexity will gather clear, actionable information from Charity Navigator\\'s extensive database.“Giving is evolving and AI tools like Perplexity that leverage premium content, like Charity Navigator ratings, provide users with the information they need to give with impact,” said Michael Thatcher, President & CEO at Charity Navigator. “Not only can donors search and find information about charities, but they can also take immediate action by donating through the Charity Navigator Giving Basket.”\\xa0Johnny Ho, Perplexity’s Co-Founder, says: \"We’re always looking for ways to make our answer engine more helpful, and partnering with Charity Navigator simplifies how people research nonprofits. If you\\'re looking to give back but don\\'t know where to start, just ask Perplexity, and it will guide you to charities that align with your goals.\"\\xa0Whether you\\'re a first-time donor or a seasoned philanthropist, we’re here to help you find and connect with causes that matter.\\xa0If you are a nonprofit interested in verifying how your data is presented through Perplexity, we recommend making sure your profile is up-to-date with Charity Navigator. If you are interested in using Perplexity, we offer Enterprise Pro at a discounted rate for nonprofits. Thanks to our partner ScalePost for initiating this collaboration.Research nonprofits with Charity Navigator on Perplexity\\xa0Today is Giving Tuesday, and we\\'re making it easier than ever to research and donate to U.S. nonprofits thanks to a new collaboration with Charity Navigator.\\xa0Charity Navigator is America’s largest and most trusted charity evaluator. For more than two decades, they’ve helped people make informed giving decisions by analyzing over 225,000 U.S. registered nonprofits for accountability, impact, leadership, and culture. Starting tomorrow, you can tap into these insights directly through Perplexity.\\xa0\\xa0Giving Tuesday is the biggest donation day in the U.S., with about 34 million people giving over $3 billion last year alone. By partnering with Charity Navigator, we\\'re making it easier to find causes that match your values, so you can give confidently and make a meaningful impact.If you\\'re curious about a nonprofit\\'s financial health, program effectiveness, or leadership structure, Perplexity will gather clear, actionable information from Charity Navigator\\'s extensive database.“Giving is evolving and AI tools like Perplexity that leverage premium content, like Charity Navigator ratings, provide users with the information they need to give with impact,” said Michael Thatcher, President & CEO at Charity Navigator. “Not only can donors search and find information about charities, but they can also take immediate action by donating through the Charity Navigator Giving Basket.”\\xa0Johnny Ho, Perplexity’s Co-Founder, says: \"We’re always looking for ways to make our answer engine more helpful, and partnering with Charity Navigator simplifies how people research nonprofits. If you\\'re looking to give back but don\\'t know where to start, just ask Perplexity, and it will guide you to charities that align with your goals.\"\\xa0Whether you\\'re a first-time donor or a seasoned philanthropist, we’re here to help you find and connect with causes that matter.\\xa0If you are a nonprofit interested in verifying how your data is presented through Perplexity, we recommend making sure your profile is up-to-date with Charity Navigator. If you are interested in using Perplexity, we offer Enterprise Pro at a discounted rate for nonprofits. Thanks to our partner ScalePost for initiating this collaboration.'),\n",
       " Document(metadata={'source': 'https://www.perplexity.ai/hub/blog/shop-like-a-pro'}, page_content='Shop like a Pro: Perplexity’s new AI-powered shopping assistantToday, we’re excited to launch a new experience for shopping. Perplexity is now a one-stop solution where you can research and purchase products. It marks a big leap forward in how we serve our users – empowering seamless native actions right from an answer. Shopping online just got 10x more easy and fun.Here’s what’s new:\\xa0One-click checkout to save time. For Perplexity Pro users in the U.S., we’ve built a first-of-its kind AI commerce experience, Buy with Pro, which lets you check out seamlessly right on our website or app for select products from select merchants. Just save your shipping and billing information through our secure portal and select “Buy with Pro” to place your order. We’ll take care of the rest. Plus, you’ll get free shipping on all Buy with Pro orders as a thank-you for shopping with Perplexity. If Buy with Pro isn’t available, we’ll redirect you to the merchant’s website to complete your purchase.Snap to Shop, a visual search tool that shows you relevant products when you take a photo of an item. Now, you can easily find what you’re looking for, even if you don’t have a product description or name.Discover the best product. When you ask Perplexity a shopping question, you’ll still get the precise, objective answers you expect, plus easy-to-read product cards showing the most relevant items, along with key details presented in a simple, visual format. These cards aren’t sponsored—they’re unbiased recommendations, tailored to your search by our AI.This new discovery experience is powered by platform integrations including Shopify, which gives access to the most recent and relevant information on products across Shopify-powered businesses globally that sell and ship to the US.You no longer have to scroll through countless product reviews. Perplexity gives you comparisons in clear, everyday language, so you can narrow down the best choices quickly and confidently.We’ll be expanding the Perplexity shopping experience to new markets after our U.S. launch. Stay tuned for more features and special offers.Introducing the Perplexity Merchant Program\\xa0To scale the Perplexity shopping experience, we\\'re launching the Perplexity Merchant program to make it easy for large retailers to share their product specs with us, ensuring we can access live details on all the best available products.\\xa0This program is free for merchants, and we\\'ve designed it to give them a straightforward process for sharing data. This is distinct from and unrelated to Perplexity’s new sponsored questions ad products.Benefits of joining the Perplexity Merchant Program include:\\xa0Increased chances of being a \"recommended product\" because the products will be in our index, and when we have more robust details, we can better determine if a product is high quality and relevant to a user\\'s query.Payment integrations to include merchants in our one-click Buy with Pro checkout experience.Free API access so merchants can build their own Perplexity-powered search experiences, with the ability to refine the index to include only your products.A custom dashboard that gives merchants insights into search and shopping trends that surface their products.If you’re a large retailer interested in joining the program, fill out this signup form, and we\\'ll reach out with more information.Shop like a Pro: Perplexity’s new AI-powered shopping assistantToday, we’re excited to launch a new experience for shopping. Perplexity is now a one-stop solution where you can research and purchase products. It marks a big leap forward in how we serve our users – empowering seamless native actions right from an answer. Shopping online just got 10x more easy and fun.Here’s what’s new:\\xa0One-click checkout to save time. For Perplexity Pro users in the U.S., we’ve built a first-of-its kind AI commerce experience, Buy with Pro, which lets you check out seamlessly right on our website or app for select products from select merchants. Just save your shipping and billing information through our secure portal and select “Buy with Pro” to place your order. We’ll take care of the rest. Plus, you’ll get free shipping on all Buy with Pro orders as a thank-you for shopping with Perplexity. If Buy with Pro isn’t available, we’ll redirect you to the merchant’s website to complete your purchase.Snap to Shop, a visual search tool that shows you relevant products when you take a photo of an item. Now, you can easily find what you’re looking for, even if you don’t have a product description or name.Discover the best product. When you ask Perplexity a shopping question, you’ll still get the precise, objective answers you expect, plus easy-to-read product cards showing the most relevant items, along with key details presented in a simple, visual format. These cards aren’t sponsored—they’re unbiased recommendations, tailored to your search by our AI.This new discovery experience is powered by platform integrations including Shopify, which gives access to the most recent and relevant information on products across Shopify-powered businesses globally that sell and ship to the US.You no longer have to scroll through countless product reviews. Perplexity gives you comparisons in clear, everyday language, so you can narrow down the best choices quickly and confidently.We’ll be expanding the Perplexity shopping experience to new markets after our U.S. launch. Stay tuned for more features and special offers.Introducing the Perplexity Merchant Program\\xa0To scale the Perplexity shopping experience, we\\'re launching the Perplexity Merchant program to make it easy for large retailers to share their product specs with us, ensuring we can access live details on all the best available products.\\xa0This program is free for merchants, and we\\'ve designed it to give them a straightforward process for sharing data. This is distinct from and unrelated to Perplexity’s new sponsored questions ad products.Benefits of joining the Perplexity Merchant Program include:\\xa0Increased chances of being a \"recommended product\" because the products will be in our index, and when we have more robust details, we can better determine if a product is high quality and relevant to a user\\'s query.Payment integrations to include merchants in our one-click Buy with Pro checkout experience.Free API access so merchants can build their own Perplexity-powered search experiences, with the ability to refine the index to include only your products.A custom dashboard that gives merchants insights into search and shopping trends that surface their products.If you’re a large retailer interested in joining the program, fill out this signup form, and we\\'ll reach out with more information.Shop like a Pro: Perplexity’s new AI-powered shopping assistantToday, we’re excited to launch a new experience for shopping. Perplexity is now a one-stop solution where you can research and purchase products. It marks a big leap forward in how we serve our users – empowering seamless native actions right from an answer. Shopping online just got 10x more easy and fun.Here’s what’s new:\\xa0One-click checkout to save time. For Perplexity Pro users in the U.S., we’ve built a first-of-its kind AI commerce experience, Buy with Pro, which lets you check out seamlessly right on our website or app for select products from select merchants. Just save your shipping and billing information through our secure portal and select “Buy with Pro” to place your order. We’ll take care of the rest. Plus, you’ll get free shipping on all Buy with Pro orders as a thank-you for shopping with Perplexity. If Buy with Pro isn’t available, we’ll redirect you to the merchant’s website to complete your purchase.Snap to Shop, a visual search tool that shows you relevant products when you take a photo of an item. Now, you can easily find what you’re looking for, even if you don’t have a product description or name.Discover the best product. When you ask Perplexity a shopping question, you’ll still get the precise, objective answers you expect, plus easy-to-read product cards showing the most relevant items, along with key details presented in a simple, visual format. These cards aren’t sponsored—they’re unbiased recommendations, tailored to your search by our AI.This new discovery experience is powered by platform integrations including Shopify, which gives access to the most recent and relevant information on products across Shopify-powered businesses globally that sell and ship to the US.You no longer have to scroll through countless product reviews. Perplexity gives you comparisons in clear, everyday language, so you can narrow down the best choices quickly and confidently.We’ll be expanding the Perplexity shopping experience to new markets after our U.S. launch. Stay tuned for more features and special offers.Introducing the Perplexity Merchant Program\\xa0To scale the Perplexity shopping experience, we\\'re launching the Perplexity Merchant program to make it easy for large retailers to share their product specs with us, ensuring we can access live details on all the best available products.\\xa0This program is free for merchants, and we\\'ve designed it to give them a straightforward process for sharing data. This is distinct from and unrelated to Perplexity’s new sponsored questions ad products.Benefits of joining the Perplexity Merchant Program include:\\xa0Increased chances of being a \"recommended product\" because the products will be in our index, and when we have more robust details, we can better determine if a product is high quality and relevant to a user\\'s query.Payment integrations to include merchants in our one-click Buy with Pro checkout experience.Free API access so merchants can build their own Perplexity-powered search experiences, with the ability to refine the index to include only your products.A custom dashboard that gives merchants insights into search and shopping trends that surface their products.If you’re a large retailer interested in joining the program, fill out this signup form, and we\\'ll reach out with more information.'),\n",
       " Document(metadata={'source': 'https://www.perplexity.ai/hub/blog/why-we-re-experimenting-with-advertising'}, page_content=\"Why we’re experimenting with advertising\\xa0To fully deliver on our mission to spark the world’s curiosity, we need to invest in building not just a beloved product, but a robust and self-sustaining business. That’s why starting this week, we will begin experimenting with ads on Perplexity.Before getting into the details, we want to highlight our guiding principle: the content of the answers you receive on Perplexity will not be influenced by advertisers. Users come to Perplexity for a more efficient, uncluttered, and unbiased search experience, and that isn’t changing.Ads will appear in the US to start and will be formatted as sponsored follow-up questions and paid media positioned to the side of an answer. Here's an example of how they will appear:Advertising material will be clearly noted as “sponsored,” and answers to Sponsored Questions will still be generated by our technology, and not written or edited by the brands sponsoring the questions. We intentionally chose these formats because it integrates advertising in a way that still protects the utility, accuracy, and objectivity of answers.\\xa0We’re thrilled to announce some of our first brand and agency partners, representing the largest and most beloved in the world, including Indeed, Whole Foods Market, Universal McCann, PMG, and others We appreciate their open minds and innovative spirits as we collaborate to build a new advertising paradigm together.Why we’re taking this approach\\xa0While brands are keen on understanding how their companies appear in AI answer engines like ours, we will avoid duplicating the SEO industry where people are implementing arbitrary tactics to improve their rankings at the expense of user utility. We would rather give brands a transparent way to advertise through Perplexity that — rather than attempting to manipulate answers — encourages users to express their curiosity about a brand.Ad programs like this help us generate revenue to share with our publisher partners. Experience has taught us that subscriptions alone do not generate enough revenue to create a sustainable revenue-sharing program. Especially given how rapidly our publisher program is growing, advertising is the best way to ensure a steady and scalable revenue stream.\\xa0This is an experiment and formats may evolve, but two principles will always remain true: (1) These ads will not change our commitment to maintaining a trusted service that provides you with direct, unbiased answers to your questions, and (2) we will never share your personal information with advertisers. Our advertising approach is designed to complement the Perplexity experience.\\xa0As we move forward, we invite our users to be part of this journey. Your feedback will be invaluable in shaping the future of advertising on Perplexity. Ultimately, this initiative is about ensuring Perplexity's long-term sustainability and our ability to continue innovating and improving our service. By diversifying our revenue streams, we're investing in a future where we can expand our capabilities, support our publisher partners, and most importantly, continue to spark curiosity and deliver unbiased, high-quality information.Thank you for your continued trust and support. Together, we're building a more informed and curious world, one question at a time.Why we’re experimenting with advertising\\xa0To fully deliver on our mission to spark the world’s curiosity, we need to invest in building not just a beloved product, but a robust and self-sustaining business. That’s why starting this week, we will begin experimenting with ads on Perplexity.Before getting into the details, we want to highlight our guiding principle: the content of the answers you receive on Perplexity will not be influenced by advertisers. Users come to Perplexity for a more efficient, uncluttered, and unbiased search experience, and that isn’t changing.Ads will appear in the US to start and will be formatted as sponsored follow-up questions and paid media positioned to the side of an answer. Here's an example of how they will appear:Advertising material will be clearly noted as “sponsored,” and answers to Sponsored Questions will still be generated by our technology, and not written or edited by the brands sponsoring the questions. We intentionally chose these formats because it integrates advertising in a way that still protects the utility, accuracy, and objectivity of answers.\\xa0We’re thrilled to announce some of our first brand and agency partners, representing the largest and most beloved in the world, including Indeed, Whole Foods Market, Universal McCann, PMG, and others We appreciate their open minds and innovative spirits as we collaborate to build a new advertising paradigm together.Why we’re taking this approach\\xa0While brands are keen on understanding how their companies appear in AI answer engines like ours, we will avoid duplicating the SEO industry where people are implementing arbitrary tactics to improve their rankings at the expense of user utility. We would rather give brands a transparent way to advertise through Perplexity that — rather than attempting to manipulate answers — encourages users to express their curiosity about a brand.Ad programs like this help us generate revenue to share with our publisher partners. Experience has taught us that subscriptions alone do not generate enough revenue to create a sustainable revenue-sharing program. Especially given how rapidly our publisher program is growing, advertising is the best way to ensure a steady and scalable revenue stream.\\xa0This is an experiment and formats may evolve, but two principles will always remain true: (1) These ads will not change our commitment to maintaining a trusted service that provides you with direct, unbiased answers to your questions, and (2) we will never share your personal information with advertisers. Our advertising approach is designed to complement the Perplexity experience.\\xa0As we move forward, we invite our users to be part of this journey. Your feedback will be invaluable in shaping the future of advertising on Perplexity. Ultimately, this initiative is about ensuring Perplexity's long-term sustainability and our ability to continue innovating and improving our service. By diversifying our revenue streams, we're investing in a future where we can expand our capabilities, support our publisher partners, and most importantly, continue to spark curiosity and deliver unbiased, high-quality information.Thank you for your continued trust and support. Together, we're building a more informed and curious world, one question at a time.Why we’re experimenting with advertising\\xa0To fully deliver on our mission to spark the world’s curiosity, we need to invest in building not just a beloved product, but a robust and self-sustaining business. That’s why starting this week, we will begin experimenting with ads on Perplexity.Before getting into the details, we want to highlight our guiding principle: the content of the answers you receive on Perplexity will not be influenced by advertisers. Users come to Perplexity for a more efficient, uncluttered, and unbiased search experience, and that isn’t changing.Ads will appear in the US to start and will be formatted as sponsored follow-up questions and paid media positioned to the side of an answer. Here's an example of how they will appear:Advertising material will be clearly noted as “sponsored,” and answers to Sponsored Questions will still be generated by our technology, and not written or edited by the brands sponsoring the questions. We intentionally chose these formats because it integrates advertising in a way that still protects the utility, accuracy, and objectivity of answers.\\xa0We’re thrilled to announce some of our first brand and agency partners, representing the largest and most beloved in the world, including Indeed, Whole Foods Market, Universal McCann, PMG, and others We appreciate their open minds and innovative spirits as we collaborate to build a new advertising paradigm together.Why we’re taking this approach\\xa0While brands are keen on understanding how their companies appear in AI answer engines like ours, we will avoid duplicating the SEO industry where people are implementing arbitrary tactics to improve their rankings at the expense of user utility. We would rather give brands a transparent way to advertise through Perplexity that — rather than attempting to manipulate answers — encourages users to express their curiosity about a brand.Ad programs like this help us generate revenue to share with our publisher partners. Experience has taught us that subscriptions alone do not generate enough revenue to create a sustainable revenue-sharing program. Especially given how rapidly our publisher program is growing, advertising is the best way to ensure a steady and scalable revenue stream.\\xa0This is an experiment and formats may evolve, but two principles will always remain true: (1) These ads will not change our commitment to maintaining a trusted service that provides you with direct, unbiased answers to your questions, and (2) we will never share your personal information with advertisers. Our advertising approach is designed to complement the Perplexity experience.\\xa0As we move forward, we invite our users to be part of this journey. Your feedback will be invaluable in shaping the future of advertising on Perplexity. Ultimately, this initiative is about ensuring Perplexity's long-term sustainability and our ability to continue innovating and improving our service. By diversifying our revenue streams, we're investing in a future where we can expand our capabilities, support our publisher partners, and most importantly, continue to spark curiosity and deliver unbiased, high-quality information.Thank you for your continued trust and support. Together, we're building a more informed and curious world, one question at a time.\"),\n",
       " Document(metadata={'source': 'https://www.perplexity.ai/hub/blog/introducing-the-election-information-hub'}, page_content=\"Introducing the Election Information Hub\\xa0Understand key issues, vote intelligently, and track election results all on Perplexity.\\xa0People deserve a clear, easy way to find the information they seek, and few topics are as consequential as elections. Through Tuesday, US citizens nationwide will vote on many issues, including the presidency, representatives for state and local offices, and city, county, and state-wide ballot initiatives.\\xa0We want to do our part to support an informed electorate, so we’ve built the election hub on Perplexity’s answer engine: an entry point for understanding key issues, voting intelligently, and tracking election results.Starting Tuesday, we'll be offering live updates on elections by leveraging data from The Associated Press so you can stay informed on presidential, senate, and house races at both a state and national level. Track the latest election results by visiting perplexity.ai/elections.\\xa0We answer your election-related questions using a curated set of the most trustworthy and informative sources. You can ask about topics like voting requirements, how to find your polling place, and poll times, as well as receive detailed, AI-summarized analysis on ballot measures and candidates, including official policy stances and endorsements. Thank you to Democracy Works for granting us access to your Elections API to help power these experiences.\\xa0We want to make it as simple as possible to receive trusted, easy-to-understand information to inform your voting decisions. For each response, you can view the sources that informed an answer, allowing you to dive deeper and verify referenced materials. Whether you're seeking to understand complex ballot measures, verify candidate positions, or simply find your polling place, Perplexity is here to support your civic engagement.\\xa0Try it out at perplexity.ai/elections.Introducing the Election Information Hub\\xa0Understand key issues, vote intelligently, and track election results all on Perplexity.\\xa0People deserve a clear, easy way to find the information they seek, and few topics are as consequential as elections. Through Tuesday, US citizens nationwide will vote on many issues, including the presidency, representatives for state and local offices, and city, county, and state-wide ballot initiatives.\\xa0We want to do our part to support an informed electorate, so we’ve built the election hub on Perplexity’s answer engine: an entry point for understanding key issues, voting intelligently, and tracking election results.Starting Tuesday, we'll be offering live updates on elections by leveraging data from The Associated Press so you can stay informed on presidential, senate, and house races at both a state and national level. Track the latest election results by visiting perplexity.ai/elections.\\xa0We answer your election-related questions using a curated set of the most trustworthy and informative sources. You can ask about topics like voting requirements, how to find your polling place, and poll times, as well as receive detailed, AI-summarized analysis on ballot measures and candidates, including official policy stances and endorsements. Thank you to Democracy Works for granting us access to your Elections API to help power these experiences.\\xa0We want to make it as simple as possible to receive trusted, easy-to-understand information to inform your voting decisions. For each response, you can view the sources that informed an answer, allowing you to dive deeper and verify referenced materials. Whether you're seeking to understand complex ballot measures, verify candidate positions, or simply find your polling place, Perplexity is here to support your civic engagement.\\xa0Try it out at perplexity.ai/elections.Introducing the Election Information Hub\\xa0Understand key issues, vote intelligently, and track election results all on Perplexity.\\xa0People deserve a clear, easy way to find the information they seek, and few topics are as consequential as elections. Through Tuesday, US citizens nationwide will vote on many issues, including the presidency, representatives for state and local offices, and city, county, and state-wide ballot initiatives.\\xa0We want to do our part to support an informed electorate, so we’ve built the election hub on Perplexity’s answer engine: an entry point for understanding key issues, voting intelligently, and tracking election results.Starting Tuesday, we'll be offering live updates on elections by leveraging data from The Associated Press so you can stay informed on presidential, senate, and house races at both a state and national level. Track the latest election results by visiting perplexity.ai/elections.\\xa0We answer your election-related questions using a curated set of the most trustworthy and informative sources. You can ask about topics like voting requirements, how to find your polling place, and poll times, as well as receive detailed, AI-summarized analysis on ballot measures and candidates, including official policy stances and endorsements. Thank you to Democracy Works for granting us access to your Elections API to help power these experiences.\\xa0We want to make it as simple as possible to receive trusted, easy-to-understand information to inform your voting decisions. For each response, you can view the sources that informed an answer, allowing you to dive deeper and verify referenced materials. Whether you're seeking to understand complex ballot measures, verify candidate positions, or simply find your polling place, Perplexity is here to support your civic engagement.\\xa0Try it out at perplexity.ai/elections.\"),\n",
       " Document(metadata={'source': 'https://www.perplexity.ai/hub/blog/about-the-dow-jones-lawsuit'}, page_content=\"About the Dow Jones lawsuitOn Monday, we got sued by the Wall Street Journal and the New York Post.\\xa0 We were disappointed and surprised to see this.There are around three dozen lawsuits by media companies against generative AI tools. The common theme betrayed by those complaints collectively is that they wish this technology didn’t exist. They prefer to live in a world where publicly reported facts are owned by corporations, and no one can do anything with those publicly reported facts without paying a toll.That is not our view of the world.We believe that tools like Perplexity provide a fundamentally transformative way for people to learn facts about the world. Perplexity not only does so in a way that the law has always recognized but is essential for the sound functioning of a cultural ecosystem in which people can efficiently and effectively obtain and engage with knowledge created by others.Perplexity, from its founding moment, has always listed sources above answers and provided in-line citations for every part of an answer. We are glad that other AI chatbots have begun copying Perplexity's transparency and emphasis on sources in their products. In fact, the Wall Street Journal itself earlier this year ranked Perplexity the #1 overall chatbot in their “Great AI Challenge.”The lawsuit reflects an adversarial posture between media and tech that is—while depressingly familiar—fundamentally shortsighted, unnecessary, and self-defeating.\\xa0 We should all be working together to offer people amazing new tools and build genuinely pie-expanding businesses.\\xa0 There are countless things we would love to do beyond what the default application of law allows, which entail mutually beneficial commercial relationships with counterparties like the companies here who chose to sue rather than cooperate. Perplexity is proud to have launched a first-of-its-kind revenue-sharing program with leading publishers like TIME, Fortune, and Der Spiegel, which have already signed on.\\xa0 And our door is always open if and when the Post and the Journal decide to work with us in good faith, just as numerous others already have.\\xa0\\xa0Unless and until that happens, though, we will defend ourselves in this lawsuit. This is not the place to get into the weeds of all of that, but we want to make two quick points at the outset:First, the facts alleged in the complaint are misleading at best. Cited examples of “regurgitated” outputs explicitly mischaracterize the source of the material. They are disingenuous in their description of what happened even in the specific cited instances, as well as in their broader depiction of what Perplexity is for (spoiler alert: it’s not for reprising the full text of articles that can be more directly and efficiently obtained elsewhere). And the suggestion that we never responded to outreach from News Corp. is simply false: they reached out; we responded the very same day; instead of continuing the dialogue, they filed this lawsuit.Second, we have learned in the short time since this lawsuit was filed, a disturbing trend in these types of cases: The companies that are suing make all kinds of salacious allegations in their complaints about all kinds of seemingly bad things they were able to coax the AI tools to do—and then, when pressed in the litigation for details of things like how they achieved such obviously unrepresentative results, they immediately disavow the very examples they put in the public record, and swear they won’t actually use them in the case. We presume that is what will happen here. And that will tell you everything you need to know about the strength of their case.AI-enhanced search engines are not going away. Perplexity is not going away. We look forward to a time in the future when we can focus all of our energy and attention on offering innovative tools to customers, in collaboration with media companies.About the Dow Jones lawsuitOn Monday, we got sued by the Wall Street Journal and the New York Post.\\xa0 We were disappointed and surprised to see this.There are around three dozen lawsuits by media companies against generative AI tools. The common theme betrayed by those complaints collectively is that they wish this technology didn’t exist. They prefer to live in a world where publicly reported facts are owned by corporations, and no one can do anything with those publicly reported facts without paying a toll.That is not our view of the world.We believe that tools like Perplexity provide a fundamentally transformative way for people to learn facts about the world. Perplexity not only does so in a way that the law has always recognized but is essential for the sound functioning of a cultural ecosystem in which people can efficiently and effectively obtain and engage with knowledge created by others.Perplexity, from its founding moment, has always listed sources above answers and provided in-line citations for every part of an answer. We are glad that other AI chatbots have begun copying Perplexity's transparency and emphasis on sources in their products. In fact, the Wall Street Journal itself earlier this year ranked Perplexity the #1 overall chatbot in their “Great AI Challenge.”The lawsuit reflects an adversarial posture between media and tech that is—while depressingly familiar—fundamentally shortsighted, unnecessary, and self-defeating.\\xa0 We should all be working together to offer people amazing new tools and build genuinely pie-expanding businesses.\\xa0 There are countless things we would love to do beyond what the default application of law allows, which entail mutually beneficial commercial relationships with counterparties like the companies here who chose to sue rather than cooperate. Perplexity is proud to have launched a first-of-its-kind revenue-sharing program with leading publishers like TIME, Fortune, and Der Spiegel, which have already signed on.\\xa0 And our door is always open if and when the Post and the Journal decide to work with us in good faith, just as numerous others already have.\\xa0\\xa0Unless and until that happens, though, we will defend ourselves in this lawsuit. This is not the place to get into the weeds of all of that, but we want to make two quick points at the outset:First, the facts alleged in the complaint are misleading at best. Cited examples of “regurgitated” outputs explicitly mischaracterize the source of the material. They are disingenuous in their description of what happened even in the specific cited instances, as well as in their broader depiction of what Perplexity is for (spoiler alert: it’s not for reprising the full text of articles that can be more directly and efficiently obtained elsewhere). And the suggestion that we never responded to outreach from News Corp. is simply false: they reached out; we responded the very same day; instead of continuing the dialogue, they filed this lawsuit.Second, we have learned in the short time since this lawsuit was filed, a disturbing trend in these types of cases: The companies that are suing make all kinds of salacious allegations in their complaints about all kinds of seemingly bad things they were able to coax the AI tools to do—and then, when pressed in the litigation for details of things like how they achieved such obviously unrepresentative results, they immediately disavow the very examples they put in the public record, and swear they won’t actually use them in the case. We presume that is what will happen here. And that will tell you everything you need to know about the strength of their case.AI-enhanced search engines are not going away. Perplexity is not going away. We look forward to a time in the future when we can focus all of our energy and attention on offering innovative tools to customers, in collaboration with media companies.About the Dow Jones lawsuitOn Monday, we got sued by the Wall Street Journal and the New York Post.\\xa0 We were disappointed and surprised to see this.There are around three dozen lawsuits by media companies against generative AI tools. The common theme betrayed by those complaints collectively is that they wish this technology didn’t exist. They prefer to live in a world where publicly reported facts are owned by corporations, and no one can do anything with those publicly reported facts without paying a toll.That is not our view of the world.We believe that tools like Perplexity provide a fundamentally transformative way for people to learn facts about the world. Perplexity not only does so in a way that the law has always recognized but is essential for the sound functioning of a cultural ecosystem in which people can efficiently and effectively obtain and engage with knowledge created by others.Perplexity, from its founding moment, has always listed sources above answers and provided in-line citations for every part of an answer. We are glad that other AI chatbots have begun copying Perplexity's transparency and emphasis on sources in their products. In fact, the Wall Street Journal itself earlier this year ranked Perplexity the #1 overall chatbot in their “Great AI Challenge.”The lawsuit reflects an adversarial posture between media and tech that is—while depressingly familiar—fundamentally shortsighted, unnecessary, and self-defeating.\\xa0 We should all be working together to offer people amazing new tools and build genuinely pie-expanding businesses.\\xa0 There are countless things we would love to do beyond what the default application of law allows, which entail mutually beneficial commercial relationships with counterparties like the companies here who chose to sue rather than cooperate. Perplexity is proud to have launched a first-of-its-kind revenue-sharing program with leading publishers like TIME, Fortune, and Der Spiegel, which have already signed on.\\xa0 And our door is always open if and when the Post and the Journal decide to work with us in good faith, just as numerous others already have.\\xa0\\xa0Unless and until that happens, though, we will defend ourselves in this lawsuit. This is not the place to get into the weeds of all of that, but we want to make two quick points at the outset:First, the facts alleged in the complaint are misleading at best. Cited examples of “regurgitated” outputs explicitly mischaracterize the source of the material. They are disingenuous in their description of what happened even in the specific cited instances, as well as in their broader depiction of what Perplexity is for (spoiler alert: it’s not for reprising the full text of articles that can be more directly and efficiently obtained elsewhere). And the suggestion that we never responded to outreach from News Corp. is simply false: they reached out; we responded the very same day; instead of continuing the dialogue, they filed this lawsuit.Second, we have learned in the short time since this lawsuit was filed, a disturbing trend in these types of cases: The companies that are suing make all kinds of salacious allegations in their complaints about all kinds of seemingly bad things they were able to coax the AI tools to do—and then, when pressed in the litigation for details of things like how they achieved such obviously unrepresentative results, they immediately disavow the very examples they put in the public record, and swear they won’t actually use them in the case. We presume that is what will happen here. And that will tell you everything you need to know about the strength of their case.AI-enhanced search engines are not going away. Perplexity is not going away. We look forward to a time in the future when we can focus all of our energy and attention on offering innovative tools to customers, in collaboration with media companies.\"),\n",
       " Document(metadata={'source': 'https://www.perplexity.ai/hub/blog/a-student-s-guide-to-using-perplexity-spaces'}, page_content=\"A student's guide to using Perplexity SpacesAs a student, you're probably constantly juggling multiple courses, projects, assignments, and extracurriculars. Organizing all this information can be challenging, and this is exactly why we’ve built Perplexity Spaces. It allows you to create a collaborative knowledge hub for your course, team, or study group. By using this hub, you can ask questions, create study guides, and even find relevant web resources—all in one place. This guide will walk you through how to use Perplexity Spaces, step by step, with examples designed specifically for students.What is Perplexity Spaces?Perplexity Spaces is a tool that helps you organize, search, and create content from your course materials, notes, and web resources. Think of it as your personal assistant for learning. You can create a space for each class, project, or even for your study group, then ask it questions or get help on specific tasks, like making a study guide or finding resources online.Main use cases for studentsHere are the key types of queries we recommend when using Perplexity Spaces:Ask your knowledge base: Search through your class notes, syllabus, and course materials to find the answers you need.Get help from the web: Use the web to find resources that complement your studies, from practice problems to online study guides.Create content: Generate useful study materials, like practice exams or collaboration guides for group projects.How to use Perplexity Spaces1. Create a knowledge hub for your classTo get started, create a space for your class by clicking ‘Spaces’ in the left-hand menu on Perplexity. Make sure you have Perplexity Pro, or the option won’t appear. Next, click ‘Add Sources’ on the right side of your screen to upload your course documents, notes, and syllabus. If you want to give specific instructions, like ‘explain answers in an ELI5 way,’ you can do so by clicking ‘Add Instructions’ just above. This space will act as your personal or group resource for the course.2. Ask questions from your class materialDo you remember solving a problem in a previous homework assignment but can’t remember where? No problem! Perplexity allows you to ask specific questions from your materials and retrieves the answers in seconds. Here are some examples to show you how it works.↪ Could you summarize the main points from Week 5’s lecture?↪  What was the solution to Exercise 2 from Homework 1?↪  Compare the solutions to Questions 2 and 5 from Homework 1↪  Based on the syllabus, what are the key topics I should focus on for the upcoming midterm?↪  Please organize all my notes from The Good Life course3. Get help from the webIf you need to find resources outside of your notes, Perplexity can also pull information from the web. Whether it’s a study guide, coding tutorials, or even a set of flashcards for a chapter in your textbook, you can easily ask Perplexity to do the searching for you. Here are a couple of examples to illustrate the process.↪  Can you find me a set of online flashcards for each chapter in our syllabus?↪  Can you find online practice problems for Week 6 of the syllabus that align with the chapters we've covered?↪  Can you suggest articles related to Week 6 of the syllabus to help with my project?↪  Can you locate math problems for Week 4 of the syllabus that match the difficulty of our homework?4. Create a study guideNeed to prepare for a test? Perplexity Spaces can create a custom study guide for you, using all your uploaded class materials. It can also generate a study schedule, breaking down what you should focus on each day. These examples should help you understand how it works.↪ Can you create a custom study guide for our midterm next week based on the syllabus (from week 1 to week 6)?↪ Look through all my uploaded materials and generate a study schedule leading up to our final exam.5. Collaborate on group projectsPerplexity Spaces isn’t just for individual use—it’s perfect for collaborating on group projects. You can share your space with team members, allowing everyone to access the same materials, ask questions, and generate project plans. Here are a few examples to guide you.↪ Can you create a project plan for Exopt2, including task division and deadlines?↪ Can you summarize the key steps we need to take to finish our exopt1 group project on time?↪ Create a shared calendar for our group that includes deadlines and progress check-ins for Exopt1↪ Can you draft an outline for our group’s Exopt2 presentation with suggestions on who should cover each section?Benefits of using Perplexity Spaces for studentsCentralized knowledge hub: Keep all your course materials, notes, and project documents in one place. This makes it easier to find answers and stay organized.Time-saving: Instead of spending hours searching through notes or the web, Perplexity does the hard work for you by retrieving relevant information instantly.Collaborative learning: Share spaces with classmates to create a shared knowledge hub, allowing everyone to benefit from the same materials.Easy access to resources: Whether it's class-related queries or finding web resources, Perplexity helps you access the information you need, fast.Create custom study tools: Generate study guides, schedules, and practice tests directly from your notes and materials, tailored specifically to your needs.Try Spaces todayWhether you're prepping for an exam, working on a group project, or just trying to stay on top of class assignments, Perplexity Spaces makes it all easier. Start building your knowledge hub, and take control of your studies with Perplexity.A student's guide to using Perplexity SpacesAs a student, you're probably constantly juggling multiple courses, projects, assignments, and extracurriculars. Organizing all this information can be challenging, and this is exactly why we’ve built Perplexity Spaces. It allows you to create a collaborative knowledge hub for your course, team, or study group. By using this hub, you can ask questions, create study guides, and even find relevant web resources—all in one place. This guide will walk you through how to use Perplexity Spaces, step by step, with examples designed specifically for students.What is Perplexity Spaces?Perplexity Spaces is a tool that helps you organize, search, and create content from your course materials, notes, and web resources. Think of it as your personal assistant for learning. You can create a space for each class, project, or even for your study group, then ask it questions or get help on specific tasks, like making a study guide or finding resources online.Main use cases for studentsHere are the key types of queries we recommend when using Perplexity Spaces:Ask your knowledge base: Search through your class notes, syllabus, and course materials to find the answers you need.Get help from the web: Use the web to find resources that complement your studies, from practice problems to online study guides.Create content: Generate useful study materials, like practice exams or collaboration guides for group projects.How to use Perplexity Spaces1. Create a knowledge hub for your classTo get started, create a space for your class by clicking ‘Spaces’ in the left-hand menu on Perplexity. Make sure you have Perplexity Pro, or the option won’t appear. Next, click ‘Add Sources’ on the right side of your screen to upload your course documents, notes, and syllabus. If you want to give specific instructions, like ‘explain answers in an ELI5 way,’ you can do so by clicking ‘Add Instructions’ just above. This space will act as your personal or group resource for the course.2. Ask questions from your class materialDo you remember solving a problem in a previous homework assignment but can’t remember where? No problem! Perplexity allows you to ask specific questions from your materials and retrieves the answers in seconds. Here are some examples to show you how it works.↪ Could you summarize the main points from Week 5’s lecture?↪  What was the solution to Exercise 2 from Homework 1?↪  Compare the solutions to Questions 2 and 5 from Homework 1↪  Based on the syllabus, what are the key topics I should focus on for the upcoming midterm?↪  Please organize all my notes from The Good Life course3. Get help from the webIf you need to find resources outside of your notes, Perplexity can also pull information from the web. Whether it’s a study guide, coding tutorials, or even a set of flashcards for a chapter in your textbook, you can easily ask Perplexity to do the searching for you. Here are a couple of examples to illustrate the process.↪  Can you find me a set of online flashcards for each chapter in our syllabus?↪  Can you find online practice problems for Week 6 of the syllabus that align with the chapters we've covered?↪  Can you suggest articles related to Week 6 of the syllabus to help with my project?↪  Can you locate math problems for Week 4 of the syllabus that match the difficulty of our homework?4. Create a study guideNeed to prepare for a test? Perplexity Spaces can create a custom study guide for you, using all your uploaded class materials. It can also generate a study schedule, breaking down what you should focus on each day. These examples should help you understand how it works.↪ Can you create a custom study guide for our midterm next week based on the syllabus (from week 1 to week 6)?↪ Look through all my uploaded materials and generate a study schedule leading up to our final exam.5. Collaborate on group projectsPerplexity Spaces isn’t just for individual use—it’s perfect for collaborating on group projects. You can share your space with team members, allowing everyone to access the same materials, ask questions, and generate project plans. Here are a few examples to guide you.↪ Can you create a project plan for Exopt2, including task division and deadlines?↪ Can you summarize the key steps we need to take to finish our exopt1 group project on time?↪ Create a shared calendar for our group that includes deadlines and progress check-ins for Exopt1↪ Can you draft an outline for our group’s Exopt2 presentation with suggestions on who should cover each section?Benefits of using Perplexity Spaces for studentsCentralized knowledge hub: Keep all your course materials, notes, and project documents in one place. This makes it easier to find answers and stay organized.Time-saving: Instead of spending hours searching through notes or the web, Perplexity does the hard work for you by retrieving relevant information instantly.Collaborative learning: Share spaces with classmates to create a shared knowledge hub, allowing everyone to benefit from the same materials.Easy access to resources: Whether it's class-related queries or finding web resources, Perplexity helps you access the information you need, fast.Create custom study tools: Generate study guides, schedules, and practice tests directly from your notes and materials, tailored specifically to your needs.Try Spaces todayWhether you're prepping for an exam, working on a group project, or just trying to stay on top of class assignments, Perplexity Spaces makes it all easier. Start building your knowledge hub, and take control of your studies with Perplexity.A student's guide to using Perplexity SpacesAs a student, you're probably constantly juggling multiple courses, projects, assignments, and extracurriculars. Organizing all this information can be challenging, and this is exactly why we’ve built Perplexity Spaces. It allows you to create a collaborative knowledge hub for your course, team, or study group. By using this hub, you can ask questions, create study guides, and even find relevant web resources—all in one place. This guide will walk you through how to use Perplexity Spaces, step by step, with examples designed specifically for students.What is Perplexity Spaces?Perplexity Spaces is a tool that helps you organize, search, and create content from your course materials, notes, and web resources. Think of it as your personal assistant for learning. You can create a space for each class, project, or even for your study group, then ask it questions or get help on specific tasks, like making a study guide or finding resources online.Main use cases for studentsHere are the key types of queries we recommend when using Perplexity Spaces:Ask your knowledge base: Search through your class notes, syllabus, and course materials to find the answers you need.Get help from the web: Use the web to find resources that complement your studies, from practice problems to online study guides.Create content: Generate useful study materials, like practice exams or collaboration guides for group projects.How to use Perplexity Spaces1. Create a knowledge hub for your classTo get started, create a space for your class by clicking ‘Spaces’ in the left-hand menu on Perplexity. Make sure you have Perplexity Pro, or the option won’t appear. Next, click ‘Add Sources’ on the right side of your screen to upload your course documents, notes, and syllabus. If you want to give specific instructions, like ‘explain answers in an ELI5 way,’ you can do so by clicking ‘Add Instructions’ just above. This space will act as your personal or group resource for the course.2. Ask questions from your class materialDo you remember solving a problem in a previous homework assignment but can’t remember where? No problem! Perplexity allows you to ask specific questions from your materials and retrieves the answers in seconds. Here are some examples to show you how it works.↪ Could you summarize the main points from Week 5’s lecture?↪  What was the solution to Exercise 2 from Homework 1?↪  Compare the solutions to Questions 2 and 5 from Homework 1↪  Based on the syllabus, what are the key topics I should focus on for the upcoming midterm?↪  Please organize all my notes from The Good Life course3. Get help from the webIf you need to find resources outside of your notes, Perplexity can also pull information from the web. Whether it’s a study guide, coding tutorials, or even a set of flashcards for a chapter in your textbook, you can easily ask Perplexity to do the searching for you. Here are a couple of examples to illustrate the process.↪  Can you find me a set of online flashcards for each chapter in our syllabus?↪  Can you find online practice problems for Week 6 of the syllabus that align with the chapters we've covered?↪  Can you suggest articles related to Week 6 of the syllabus to help with my project?↪  Can you locate math problems for Week 4 of the syllabus that match the difficulty of our homework?4. Create a study guideNeed to prepare for a test? Perplexity Spaces can create a custom study guide for you, using all your uploaded class materials. It can also generate a study schedule, breaking down what you should focus on each day. These examples should help you understand how it works.↪ Can you create a custom study guide for our midterm next week based on the syllabus (from week 1 to week 6)?↪ Look through all my uploaded materials and generate a study schedule leading up to our final exam.5. Collaborate on group projectsPerplexity Spaces isn’t just for individual use—it’s perfect for collaborating on group projects. You can share your space with team members, allowing everyone to access the same materials, ask questions, and generate project plans. Here are a few examples to guide you.↪ Can you create a project plan for Exopt2, including task division and deadlines?↪ Can you summarize the key steps we need to take to finish our exopt1 group project on time?↪ Create a shared calendar for our group that includes deadlines and progress check-ins for Exopt1↪ Can you draft an outline for our group’s Exopt2 presentation with suggestions on who should cover each section?Benefits of using Perplexity Spaces for studentsCentralized knowledge hub: Keep all your course materials, notes, and project documents in one place. This makes it easier to find answers and stay organized.Time-saving: Instead of spending hours searching through notes or the web, Perplexity does the hard work for you by retrieving relevant information instantly.Collaborative learning: Share spaces with classmates to create a shared knowledge hub, allowing everyone to benefit from the same materials.Easy access to resources: Whether it's class-related queries or finding web resources, Perplexity helps you access the information you need, fast.Create custom study tools: Generate study guides, schedules, and practice tests directly from your notes and materials, tailored specifically to your needs.Try Spaces todayWhether you're prepping for an exam, working on a group project, or just trying to stay on top of class assignments, Perplexity Spaces makes it all easier. Start building your knowledge hub, and take control of your studies with Perplexity.\"),\n",
       " Document(metadata={'source': 'https://www.perplexity.ai/hub/blog/meet-our-first-channel-partners-data-integrators'}, page_content='Meet our first Channel Partners & Data Integrators\\xa0Since introducing Perplexity Enterprise Pro in April, over 1,500 organizations—from schools and nonprofits to investment firms and professional sports teams—have turned to Perplexity to supercharge their work and conduct better research. Across all departments, we see a strong demand for intelligent, real-time search, saving thousands of hours of employee time.\\xa0We recognize that to transform how organizations work, we need to scale our reach and tap into the expertise of trusted advisors in different markets. Exclusively for Perplexity Enterprise Pro, we’re excited to announce our Select Channel Partner Program with our first partners: FactSet, Crunchbase, Kruze, Stripe, Opal, and Inteleos.\\xa0This program will allow us to leverage the domain knowledge of our partners to serve our customers better and offer enhanced local support. As part of this launch, Perplexity Enterprise Pro is also available in the AWS Marketplace as part of the AWS partner network.Plus, Enterprise Pro account holders with a FactSet or Crunchbase subscription will have the added benefit of accessing that data directly within Perplexity’s platform later this year. This means that proprietary FactSet and Crunchbase data will be presented as a source alongside internal files and web pages for relevant queries.\\xa0\"FactSet is thrilled to provide our trusted data through Perplexity\\'s innovative research channel, enriching the discovery and reliability of financial information for our mutual clients\" stated Kristy Karnovsky, EVP and Head of Dealmakers and Wealth at FactSet. “Together, we are revolutionizing how individuals and enterprises gather information and insights over a vast repository of robust structured/unstructured financial data.\\xa0 Providing an open, flexible ecosystem for our clients to modernize their workflows is a key pillar of FactSet\\'s AI Blueprint.\\xa0 It enables clients to achieve the greatest possible utility delta by increasing leverage and efficiency.\"\\xa0\"Crunchbase is excited to enter a strategic partnership with Perplexity, delivering our powerful, private company data to Perplexity\\'s Enterprise Pro users through our data licensing API,” said Crunchbase Chief Product Officer Megh Gautam. “Our uniquely sourced, proprietary data–which covers hard-to-find firmographics, financials, and more for private companies–stands out as essential for helping investors, researchers, and salespeople make better decisions, faster. Together, we will help customers cut through the noise of information overload and synthesize data in an actionable, usable way.\" If you\\'re interested in becoming a Perplexity Enterprise Pro partner or want to learn more about how this program can benefit your organization, please email partnerships@perplexity.ai Meet our first Channel Partners & Data Integrators\\xa0Since introducing Perplexity Enterprise Pro in April, over 1,500 organizations—from schools and nonprofits to investment firms and professional sports teams—have turned to Perplexity to supercharge their work and conduct better research. Across all departments, we see a strong demand for intelligent, real-time search, saving thousands of hours of employee time.\\xa0We recognize that to transform how organizations work, we need to scale our reach and tap into the expertise of trusted advisors in different markets. Exclusively for Perplexity Enterprise Pro, we’re excited to announce our Select Channel Partner Program with our first partners: FactSet, Crunchbase, Kruze, Stripe, Opal, and Inteleos.\\xa0This program will allow us to leverage the domain knowledge of our partners to serve our customers better and offer enhanced local support. As part of this launch, Perplexity Enterprise Pro is also available in the AWS Marketplace as part of the AWS partner network.Plus, Enterprise Pro account holders with a FactSet or Crunchbase subscription will have the added benefit of accessing that data directly within Perplexity’s platform later this year. This means that proprietary FactSet and Crunchbase data will be presented as a source alongside internal files and web pages for relevant queries.\\xa0\"FactSet is thrilled to provide our trusted data through Perplexity\\'s innovative research channel, enriching the discovery and reliability of financial information for our mutual clients\" stated Kristy Karnovsky, EVP and Head of Dealmakers and Wealth at FactSet. “Together, we are revolutionizing how individuals and enterprises gather information and insights over a vast repository of robust structured/unstructured financial data.\\xa0 Providing an open, flexible ecosystem for our clients to modernize their workflows is a key pillar of FactSet\\'s AI Blueprint.\\xa0 It enables clients to achieve the greatest possible utility delta by increasing leverage and efficiency.\"\\xa0\"Crunchbase is excited to enter a strategic partnership with Perplexity, delivering our powerful, private company data to Perplexity\\'s Enterprise Pro users through our data licensing API,” said Crunchbase Chief Product Officer Megh Gautam. “Our uniquely sourced, proprietary data–which covers hard-to-find firmographics, financials, and more for private companies–stands out as essential for helping investors, researchers, and salespeople make better decisions, faster. Together, we will help customers cut through the noise of information overload and synthesize data in an actionable, usable way.\" If you\\'re interested in becoming a Perplexity Enterprise Pro partner or want to learn more about how this program can benefit your organization, please email partnerships@perplexity.ai Meet our first Channel Partners & Data Integrators\\xa0Since introducing Perplexity Enterprise Pro in April, over 1,500 organizations—from schools and nonprofits to investment firms and professional sports teams—have turned to Perplexity to supercharge their work and conduct better research. Across all departments, we see a strong demand for intelligent, real-time search, saving thousands of hours of employee time.\\xa0We recognize that to transform how organizations work, we need to scale our reach and tap into the expertise of trusted advisors in different markets. Exclusively for Perplexity Enterprise Pro, we’re excited to announce our Select Channel Partner Program with our first partners: FactSet, Crunchbase, Kruze, Stripe, Opal, and Inteleos.\\xa0This program will allow us to leverage the domain knowledge of our partners to serve our customers better and offer enhanced local support. As part of this launch, Perplexity Enterprise Pro is also available in the AWS Marketplace as part of the AWS partner network.Plus, Enterprise Pro account holders with a FactSet or Crunchbase subscription will have the added benefit of accessing that data directly within Perplexity’s platform later this year. This means that proprietary FactSet and Crunchbase data will be presented as a source alongside internal files and web pages for relevant queries.\\xa0\"FactSet is thrilled to provide our trusted data through Perplexity\\'s innovative research channel, enriching the discovery and reliability of financial information for our mutual clients\" stated Kristy Karnovsky, EVP and Head of Dealmakers and Wealth at FactSet. “Together, we are revolutionizing how individuals and enterprises gather information and insights over a vast repository of robust structured/unstructured financial data.\\xa0 Providing an open, flexible ecosystem for our clients to modernize their workflows is a key pillar of FactSet\\'s AI Blueprint.\\xa0 It enables clients to achieve the greatest possible utility delta by increasing leverage and efficiency.\"\\xa0\"Crunchbase is excited to enter a strategic partnership with Perplexity, delivering our powerful, private company data to Perplexity\\'s Enterprise Pro users through our data licensing API,” said Crunchbase Chief Product Officer Megh Gautam. “Our uniquely sourced, proprietary data–which covers hard-to-find firmographics, financials, and more for private companies–stands out as essential for helping investors, researchers, and salespeople make better decisions, faster. Together, we will help customers cut through the noise of information overload and synthesize data in an actionable, usable way.\" If you\\'re interested in becoming a Perplexity Enterprise Pro partner or want to learn more about how this program can benefit your organization, please email partnerships@perplexity.ai '),\n",
       " Document(metadata={'source': 'https://www.perplexity.ai/hub/blog/introducing-internal-knowledge-search-and-spaces'}, page_content=\"Introducing Internal Knowledge Search and SpacesAt Perplexity, we believe research fuels innovation and drives progress. Whether you’re exploring new business strategies or staying ahead of industry trends, you’re here to find clear, actionable insights faster and more efficiently.\\xa0As we've grown, so has the creativity of our users. From industry professionals to everyday hobbyists, people use Perplexity in ways we never imagined. And while our users love what Perplexity can do, they've shown us they want to achieve even more. We want to give users greater flexibility and control over the types of sources they prompt, which is why we’re excited to introduce Internal Knowledge Search and Spaces.\\xa0Internal Knowledge Search: From the web to your workspace, Perplexity searches it all\\xa0While file upload has been part of Perplexity for some time, one of our most requested features has been the ability to search internal files alongside the web. Today we’re delivering: with Perplexity Pro and Enterprise Pro, you can now search across both public web content and your own internal knowledge bases. Seamlessly access and synthesize the best information from all sources to get the answers you need, faster.We gave select Enterprise Pro customers early access to Internal Knowledge Search. Here are a few ways they’re using it:\\xa0Financial services firms conduct more robust due diligence by tapping into internal research, call notes, and the latest industry news.\\xa0Sales and business development teams accelerate the RFP process by combining past materials with the latest insights from the web.\\xa0\\xa0HR teams help employees quickly find answers about benefits and wellness by having them search existing files.\\xa0Growth-stage startups get feedback on R&D and product roadmaps based on best practices from the web.\\xa0\\xa0Says Perplexity CEO Aravind Srinivas: “Searching the web in the context of what's relevant to a particular business has never been possible before. Web and internal search have had to rely on infrastructure and products that were separate. Being able to carry out all your research — across both internal and external sources of data — in one consolidated knowledge platform will unlock tremendous productivity gains for every enterprise. Perplexity is excited to be pioneering this for existing customers including NVIDIA, Databricks, Dell, Bridgewater, Latham & Watkins, Fortune, and Lambda.”\\xa0Perplexity Spaces: Your AI-powered research & collaboration hubWe’ve also reinvented how teams research and organize information with Perplexity Spaces — AI-powered collaboration hubs that can be deeply customized to your specific use case. Whether you’re part of a project team conducting research, a sales team creating customer proposals, or a student team working on study guides, Perplexity Spaces gives you a place to work together to get things done.Once you’ve set up your Space, you can invite collaborators such as colleagues or friends, connect internal files, and customize the AI assistant by choosing your preferred AI model and setting specific instructions for how it should respond.\\xa0Spaces gives you full access controls over who can access your research and files, keeping everything secure and private within your team. For Enterprise Pro customers, all files and searches are excluded from AI quality training by default. Pro users can also choose to opt out of AI training in their settings.\\xa0We understand the sensitivity of the data our customers work with, and we’re committed to ensuring Perplexity offers the highest levels of safety and privacy, empowering teams to conduct their most critical research without compromise.\\xa0Soon, we’ll be adding third-party data integrations with Crunchbase and FactSet for Enterprise Pro customers who have active subscriptions to either of those services. This will allow you to expand your knowledge base even further with the ability to search across the public web, internal files, and proprietary data sets. More third-party data integrations are on the way, so stay tuned.\\xa0We’re enabling more holistic, accurate, and fast research, making Perplexity the comprehensive knowledge hub for any organization. Visit our website to sign up for Enterprise Pro: https://www.perplexity.ai/enterprise.\\xa0Introducing Internal Knowledge Search and SpacesAt Perplexity, we believe research fuels innovation and drives progress. Whether you’re exploring new business strategies or staying ahead of industry trends, you’re here to find clear, actionable insights faster and more efficiently.\\xa0As we've grown, so has the creativity of our users. From industry professionals to everyday hobbyists, people use Perplexity in ways we never imagined. And while our users love what Perplexity can do, they've shown us they want to achieve even more. We want to give users greater flexibility and control over the types of sources they prompt, which is why we’re excited to introduce Internal Knowledge Search and Spaces.\\xa0Internal Knowledge Search: From the web to your workspace, Perplexity searches it all\\xa0While file upload has been part of Perplexity for some time, one of our most requested features has been the ability to search internal files alongside the web. Today we’re delivering: with Perplexity Pro and Enterprise Pro, you can now search across both public web content and your own internal knowledge bases. Seamlessly access and synthesize the best information from all sources to get the answers you need, faster.We gave select Enterprise Pro customers early access to Internal Knowledge Search. Here are a few ways they’re using it:\\xa0Financial services firms conduct more robust due diligence by tapping into internal research, call notes, and the latest industry news.\\xa0Sales and business development teams accelerate the RFP process by combining past materials with the latest insights from the web.\\xa0\\xa0HR teams help employees quickly find answers about benefits and wellness by having them search existing files.\\xa0Growth-stage startups get feedback on R&D and product roadmaps based on best practices from the web.\\xa0\\xa0Says Perplexity CEO Aravind Srinivas: “Searching the web in the context of what's relevant to a particular business has never been possible before. Web and internal search have had to rely on infrastructure and products that were separate. Being able to carry out all your research — across both internal and external sources of data — in one consolidated knowledge platform will unlock tremendous productivity gains for every enterprise. Perplexity is excited to be pioneering this for existing customers including NVIDIA, Databricks, Dell, Bridgewater, Latham & Watkins, Fortune, and Lambda.”\\xa0Perplexity Spaces: Your AI-powered research & collaboration hubWe’ve also reinvented how teams research and organize information with Perplexity Spaces — AI-powered collaboration hubs that can be deeply customized to your specific use case. Whether you’re part of a project team conducting research, a sales team creating customer proposals, or a student team working on study guides, Perplexity Spaces gives you a place to work together to get things done.Once you’ve set up your Space, you can invite collaborators such as colleagues or friends, connect internal files, and customize the AI assistant by choosing your preferred AI model and setting specific instructions for how it should respond.\\xa0Spaces gives you full access controls over who can access your research and files, keeping everything secure and private within your team. For Enterprise Pro customers, all files and searches are excluded from AI quality training by default. Pro users can also choose to opt out of AI training in their settings.\\xa0We understand the sensitivity of the data our customers work with, and we’re committed to ensuring Perplexity offers the highest levels of safety and privacy, empowering teams to conduct their most critical research without compromise.\\xa0Soon, we’ll be adding third-party data integrations with Crunchbase and FactSet for Enterprise Pro customers who have active subscriptions to either of those services. This will allow you to expand your knowledge base even further with the ability to search across the public web, internal files, and proprietary data sets. More third-party data integrations are on the way, so stay tuned.\\xa0We’re enabling more holistic, accurate, and fast research, making Perplexity the comprehensive knowledge hub for any organization. Visit our website to sign up for Enterprise Pro: https://www.perplexity.ai/enterprise.\\xa0Introducing Internal Knowledge Search and SpacesAt Perplexity, we believe research fuels innovation and drives progress. Whether you’re exploring new business strategies or staying ahead of industry trends, you’re here to find clear, actionable insights faster and more efficiently.\\xa0As we've grown, so has the creativity of our users. From industry professionals to everyday hobbyists, people use Perplexity in ways we never imagined. And while our users love what Perplexity can do, they've shown us they want to achieve even more. We want to give users greater flexibility and control over the types of sources they prompt, which is why we’re excited to introduce Internal Knowledge Search and Spaces.\\xa0Internal Knowledge Search: From the web to your workspace, Perplexity searches it all\\xa0While file upload has been part of Perplexity for some time, one of our most requested features has been the ability to search internal files alongside the web. Today we’re delivering: with Perplexity Pro and Enterprise Pro, you can now search across both public web content and your own internal knowledge bases. Seamlessly access and synthesize the best information from all sources to get the answers you need, faster.We gave select Enterprise Pro customers early access to Internal Knowledge Search. Here are a few ways they’re using it:\\xa0Financial services firms conduct more robust due diligence by tapping into internal research, call notes, and the latest industry news.\\xa0Sales and business development teams accelerate the RFP process by combining past materials with the latest insights from the web.\\xa0\\xa0HR teams help employees quickly find answers about benefits and wellness by having them search existing files.\\xa0Growth-stage startups get feedback on R&D and product roadmaps based on best practices from the web.\\xa0\\xa0Says Perplexity CEO Aravind Srinivas: “Searching the web in the context of what's relevant to a particular business has never been possible before. Web and internal search have had to rely on infrastructure and products that were separate. Being able to carry out all your research — across both internal and external sources of data — in one consolidated knowledge platform will unlock tremendous productivity gains for every enterprise. Perplexity is excited to be pioneering this for existing customers including NVIDIA, Databricks, Dell, Bridgewater, Latham & Watkins, Fortune, and Lambda.”\\xa0Perplexity Spaces: Your AI-powered research & collaboration hubWe’ve also reinvented how teams research and organize information with Perplexity Spaces — AI-powered collaboration hubs that can be deeply customized to your specific use case. Whether you’re part of a project team conducting research, a sales team creating customer proposals, or a student team working on study guides, Perplexity Spaces gives you a place to work together to get things done.Once you’ve set up your Space, you can invite collaborators such as colleagues or friends, connect internal files, and customize the AI assistant by choosing your preferred AI model and setting specific instructions for how it should respond.\\xa0Spaces gives you full access controls over who can access your research and files, keeping everything secure and private within your team. For Enterprise Pro customers, all files and searches are excluded from AI quality training by default. Pro users can also choose to opt out of AI training in their settings.\\xa0We understand the sensitivity of the data our customers work with, and we’re committed to ensuring Perplexity offers the highest levels of safety and privacy, empowering teams to conduct their most critical research without compromise.\\xa0Soon, we’ll be adding third-party data integrations with Crunchbase and FactSet for Enterprise Pro customers who have active subscriptions to either of those services. This will allow you to expand your knowledge base even further with the ability to search across the public web, internal files, and proprietary data sets. More third-party data integrations are on the way, so stay tuned.\\xa0We’re enabling more holistic, accurate, and fast research, making Perplexity the comprehensive knowledge hub for any organization. Visit our website to sign up for Enterprise Pro: https://www.perplexity.ai/enterprise.\\xa0\"),\n",
       " Document(metadata={'source': 'https://www.perplexity.ai/hub/blog/getting-started-with-perplexity'}, page_content='Getting started with PerplexityWhat is Perplexity?Perplexity is the world’s first answer engine. It searches the internet in real time to deliver fast, clear answers to any question—with sources and citations included. Whether you need a quick fact or want to dive deep into a complex topic, Perplexity finds reliable answers and saves you the trouble of clicking through endless links.Perplexity vs. traditional search enginesDirect answers instead of links: Traditional search engines respond with a list of links. Perplexity gives you the answers upfront—saving time and energy.AI-driven insights: Powered by the latest AI models like GPT-4 Omni and Claude 3.5, Perplexity understands your question and delivers precise, relevant information.Conversational interface: You can ask follow-up questions naturally, like you’re having a conversation with a knowledgeable friend.Trusted sources: Every answer comes with clickable citations, making it easy to verify the information and dig deeper if you want.How to access PerplexityThere are a few ways you can start using Perplexity right now:On the web: Visit Perplexity.ai in your browser.Mobile app: Get answers on the go with the Perplexity app.iOS: Available in the App StoreAndroid: Available on Google PlayNavigating the interfacePerplexity’s interface is simple and intuitive:Search Bar: Front and center—just type your questions and get instant answers.Focus: Narrow down your results by using the Focus feature to target specific sources like academic journals or social media.File upload: Drag and drop a document into the search bar and ask Perplexity for quick insights.Pro Search: For a deeper dive, Pro Search pulls from a broader range of sources to give you more in-depth information.Note: Free users can use Pro Search five times every four hours, while Pro subscribers get nearly unlimited access.Sidebar: You can easily access your search history, save important queries in Collections, browse trending topics in the Discover Feed, and adjust your settings.How to search on Perplexity1. Start with a clear questionPerplexity works best when you ask specific questions. Instead of something broad like “Nutrition,” try asking “What are the health benefits of a Mediterranean diet?” You’ll get a more direct and useful answer.Tip: Follow up with related questions to dive deeper. For example, after asking about the Mediterranean diet, you could follow up with “How does it compare to a keto diet for weight loss?”2. Use conversational languageNo need to get fancy—just ask your question like you would in a normal conversation. Instead of typing “Pizza dough recipe,” try asking “What’s an easy recipe for homemade pizza dough?” Perplexity will give you a step-by-step guide.3. Explore sources with one clickEvery answer includes clickable citations, so you can verify the information or dig deeper into a topic. Just click a link within the response to view the source.4. Try Perplexity’s unique featuresPro Search: For comprehensive answers, toggle on Pro Search. It breaks your question down into smaller steps and pulls insights from a broader range of sources.File Upload: Have a document you want insights on? Drag and drop it into the search bar, and Perplexity will summarize or highlight key points from the content—ideal for research papers, reports, or other documents.5 Tips for getting the most out of PerplexityPerplexity is easy to use, but these tips can help you get even more out of it.1. Build on conversationsPerplexity remembers your previous questions, so you can follow up naturally. For example, you might ask, “What are the effects of climate change?” and then follow up with, “How does it impact the Amazon rainforest?” This builds a richer understanding without needing to start over.2. Refine with FocusNarrow your search to specific sources with the Focus feature. Writing a research paper? Choose “Academic” to prioritize peer-reviewed journals and scholarly articles.3. Stay updated with real-time informationSince Perplexity searches the web in real time, it’s perfect for staying up to date on fast-moving topics like the stock market or breaking news. For example, if you want to know how the stock market is doing today, just ask, “What are the latest stock market trends?” and get an instant answer.4. Compare sourcesIf you need to cross-reference different viewpoints, Perplexity makes it easy. You can ask it to summarize key points from multiple articles and to compare them to spot similarities or differences.5. Ask Follow-Up QuestionsPerplexity is great at connecting the dots. If you ask, “How does AI impact healthcare?” follow up with, “How does AI improve medical diagnoses?” to explore the topic from different angles.When to use Perplexity1. Quick fact-checkingNeed a quick answer to a simple question? Perplexity gives you reliable facts in seconds.Try: “What is the capital of Australia?”2. In-depth researchFor complex topics, Perplexity pulls from trusted sources, making it great for students, researchers, or professionals.Try: “What are the environmental impacts of deforestation in the Amazon?”3. Learning new skillsPerplexity can guide you through new skills, whether you’re learning a new language or trying a new recipe.Try: “How do I start learning French?”4. Real-time updatesStay informed with Perplexity’s real-time search results. Perfect for checking breaking news or recent trends.Try: “What are today’s biggest stories in tech?”5. Creating ContentIf you’re a content creator, Perplexity can help you research topics or generate ideas. Whether it’s for an article, a project, or a blog post, Perplexity can give you a head start.Try: “Create an outline for a blog post comparing fast fashion to sustainable fashion.\"Start your journey with Perplexity todayPerplexity is the smarter, faster way to find answers. Whether you’re a student, professional, or lifelong learner, Perplexity helps you uncover reliable, real-time information quickly and easily.Try it nowGetting started with PerplexityWhat is Perplexity?Perplexity is the world’s first answer engine. It searches the internet in real time to deliver fast, clear answers to any question—with sources and citations included. Whether you need a quick fact or want to dive deep into a complex topic, Perplexity finds reliable answers and saves you the trouble of clicking through endless links.Perplexity vs. traditional search enginesDirect answers instead of links: Traditional search engines respond with a list of links. Perplexity gives you the answers upfront—saving time and energy.AI-driven insights: Powered by the latest AI models like GPT-4 Omni and Claude 3.5, Perplexity understands your question and delivers precise, relevant information.Conversational interface: You can ask follow-up questions naturally, like you’re having a conversation with a knowledgeable friend.Trusted sources: Every answer comes with clickable citations, making it easy to verify the information and dig deeper if you want.How to access PerplexityThere are a few ways you can start using Perplexity right now:On the web: Visit Perplexity.ai in your browser.Mobile app: Get answers on the go with the Perplexity app.iOS: Available in the App StoreAndroid: Available on Google PlayNavigating the interfacePerplexity’s interface is simple and intuitive:Search Bar: Front and center—just type your questions and get instant answers.Focus: Narrow down your results by using the Focus feature to target specific sources like academic journals or social media.File upload: Drag and drop a document into the search bar and ask Perplexity for quick insights.Pro Search: For a deeper dive, Pro Search pulls from a broader range of sources to give you more in-depth information.Note: Free users can use Pro Search five times every four hours, while Pro subscribers get nearly unlimited access.Sidebar: You can easily access your search history, save important queries in Collections, browse trending topics in the Discover Feed, and adjust your settings.How to search on Perplexity1. Start with a clear questionPerplexity works best when you ask specific questions. Instead of something broad like “Nutrition,” try asking “What are the health benefits of a Mediterranean diet?” You’ll get a more direct and useful answer.Tip: Follow up with related questions to dive deeper. For example, after asking about the Mediterranean diet, you could follow up with “How does it compare to a keto diet for weight loss?”2. Use conversational languageNo need to get fancy—just ask your question like you would in a normal conversation. Instead of typing “Pizza dough recipe,” try asking “What’s an easy recipe for homemade pizza dough?” Perplexity will give you a step-by-step guide.3. Explore sources with one clickEvery answer includes clickable citations, so you can verify the information or dig deeper into a topic. Just click a link within the response to view the source.4. Try Perplexity’s unique featuresPro Search: For comprehensive answers, toggle on Pro Search. It breaks your question down into smaller steps and pulls insights from a broader range of sources.File Upload: Have a document you want insights on? Drag and drop it into the search bar, and Perplexity will summarize or highlight key points from the content—ideal for research papers, reports, or other documents.5 Tips for getting the most out of PerplexityPerplexity is easy to use, but these tips can help you get even more out of it.1. Build on conversationsPerplexity remembers your previous questions, so you can follow up naturally. For example, you might ask, “What are the effects of climate change?” and then follow up with, “How does it impact the Amazon rainforest?” This builds a richer understanding without needing to start over.2. Refine with FocusNarrow your search to specific sources with the Focus feature. Writing a research paper? Choose “Academic” to prioritize peer-reviewed journals and scholarly articles.3. Stay updated with real-time informationSince Perplexity searches the web in real time, it’s perfect for staying up to date on fast-moving topics like the stock market or breaking news. For example, if you want to know how the stock market is doing today, just ask, “What are the latest stock market trends?” and get an instant answer.4. Compare sourcesIf you need to cross-reference different viewpoints, Perplexity makes it easy. You can ask it to summarize key points from multiple articles and to compare them to spot similarities or differences.5. Ask Follow-Up QuestionsPerplexity is great at connecting the dots. If you ask, “How does AI impact healthcare?” follow up with, “How does AI improve medical diagnoses?” to explore the topic from different angles.When to use Perplexity1. Quick fact-checkingNeed a quick answer to a simple question? Perplexity gives you reliable facts in seconds.Try: “What is the capital of Australia?”2. In-depth researchFor complex topics, Perplexity pulls from trusted sources, making it great for students, researchers, or professionals.Try: “What are the environmental impacts of deforestation in the Amazon?”3. Learning new skillsPerplexity can guide you through new skills, whether you’re learning a new language or trying a new recipe.Try: “How do I start learning French?”4. Real-time updatesStay informed with Perplexity’s real-time search results. Perfect for checking breaking news or recent trends.Try: “What are today’s biggest stories in tech?”5. Creating ContentIf you’re a content creator, Perplexity can help you research topics or generate ideas. Whether it’s for an article, a project, or a blog post, Perplexity can give you a head start.Try: “Create an outline for a blog post comparing fast fashion to sustainable fashion.\"Start your journey with Perplexity todayPerplexity is the smarter, faster way to find answers. Whether you’re a student, professional, or lifelong learner, Perplexity helps you uncover reliable, real-time information quickly and easily.Try it nowGetting started with PerplexityWhat is Perplexity?Perplexity is the world’s first answer engine. It searches the internet in real time to deliver fast, clear answers to any question—with sources and citations included. Whether you need a quick fact or want to dive deep into a complex topic, Perplexity finds reliable answers and saves you the trouble of clicking through endless links.Perplexity vs. traditional search enginesDirect answers instead of links: Traditional search engines respond with a list of links. Perplexity gives you the answers upfront—saving time and energy.AI-driven insights: Powered by the latest AI models like GPT-4 Omni and Claude 3.5, Perplexity understands your question and delivers precise, relevant information.Conversational interface: You can ask follow-up questions naturally, like you’re having a conversation with a knowledgeable friend.Trusted sources: Every answer comes with clickable citations, making it easy to verify the information and dig deeper if you want.How to access PerplexityThere are a few ways you can start using Perplexity right now:On the web: Visit Perplexity.ai in your browser.Mobile app: Get answers on the go with the Perplexity app.iOS: Available in the App StoreAndroid: Available on Google PlayNavigating the interfacePerplexity’s interface is simple and intuitive:Search Bar: Front and center—just type your questions and get instant answers.Focus: Narrow down your results by using the Focus feature to target specific sources like academic journals or social media.File upload: Drag and drop a document into the search bar and ask Perplexity for quick insights.Pro Search: For a deeper dive, Pro Search pulls from a broader range of sources to give you more in-depth information.Note: Free users can use Pro Search five times every four hours, while Pro subscribers get nearly unlimited access.Sidebar: You can easily access your search history, save important queries in Collections, browse trending topics in the Discover Feed, and adjust your settings.How to search on Perplexity1. Start with a clear questionPerplexity works best when you ask specific questions. Instead of something broad like “Nutrition,” try asking “What are the health benefits of a Mediterranean diet?” You’ll get a more direct and useful answer.Tip: Follow up with related questions to dive deeper. For example, after asking about the Mediterranean diet, you could follow up with “How does it compare to a keto diet for weight loss?”2. Use conversational languageNo need to get fancy—just ask your question like you would in a normal conversation. Instead of typing “Pizza dough recipe,” try asking “What’s an easy recipe for homemade pizza dough?” Perplexity will give you a step-by-step guide.3. Explore sources with one clickEvery answer includes clickable citations, so you can verify the information or dig deeper into a topic. Just click a link within the response to view the source.4. Try Perplexity’s unique featuresPro Search: For comprehensive answers, toggle on Pro Search. It breaks your question down into smaller steps and pulls insights from a broader range of sources.File Upload: Have a document you want insights on? Drag and drop it into the search bar, and Perplexity will summarize or highlight key points from the content—ideal for research papers, reports, or other documents.5 Tips for getting the most out of PerplexityPerplexity is easy to use, but these tips can help you get even more out of it.1. Build on conversationsPerplexity remembers your previous questions, so you can follow up naturally. For example, you might ask, “What are the effects of climate change?” and then follow up with, “How does it impact the Amazon rainforest?” This builds a richer understanding without needing to start over.2. Refine with FocusNarrow your search to specific sources with the Focus feature. Writing a research paper? Choose “Academic” to prioritize peer-reviewed journals and scholarly articles.3. Stay updated with real-time informationSince Perplexity searches the web in real time, it’s perfect for staying up to date on fast-moving topics like the stock market or breaking news. For example, if you want to know how the stock market is doing today, just ask, “What are the latest stock market trends?” and get an instant answer.4. Compare sourcesIf you need to cross-reference different viewpoints, Perplexity makes it easy. You can ask it to summarize key points from multiple articles and to compare them to spot similarities or differences.5. Ask Follow-Up QuestionsPerplexity is great at connecting the dots. If you ask, “How does AI impact healthcare?” follow up with, “How does AI improve medical diagnoses?” to explore the topic from different angles.When to use Perplexity1. Quick fact-checkingNeed a quick answer to a simple question? Perplexity gives you reliable facts in seconds.Try: “What is the capital of Australia?”2. In-depth researchFor complex topics, Perplexity pulls from trusted sources, making it great for students, researchers, or professionals.Try: “What are the environmental impacts of deforestation in the Amazon?”3. Learning new skillsPerplexity can guide you through new skills, whether you’re learning a new language or trying a new recipe.Try: “How do I start learning French?”4. Real-time updatesStay informed with Perplexity’s real-time search results. Perfect for checking breaking news or recent trends.Try: “What are today’s biggest stories in tech?”5. Creating ContentIf you’re a content creator, Perplexity can help you research topics or generate ideas. Whether it’s for an article, a project, or a blog post, Perplexity can give you a head start.Try: “Create an outline for a blog post comparing fast fashion to sustainable fashion.\"Start your journey with Perplexity todayPerplexity is the smarter, faster way to find answers. Whether you’re a student, professional, or lifelong learner, Perplexity helps you uncover reliable, real-time information quickly and easily.Try it now'),\n",
       " Document(metadata={'source': 'https://www.perplexity.ai/hub/blog/redeem-a-free-year-of-perplexity-pro-through-xfinity-rewards'}, page_content='Redeem a free year of Perplexity Pro through Xfinity Rewards\\xa0\\xa0Broadband internet connects the world, putting information at our fingertips. That’s why Perplexity is thrilled to offer Perplexity Pro—our premium consumer subscription—to Xfinity customers through its customer loyalty program, Xfinity Rewards.\\xa0\\xa0The idea behind Perplexity is simple: we all deserve a faster and better way to find answers. Whether you’re looking up a quick fact or diving into an in-depth analysis of stock performance, Perplexity searches the internet in real-time to deliver answers to any question, complete with in-line source links.\\xa0\\xa0Adding to their lineup of perks like $1 movie rentals and discounted tickets to theme parks and sporting events, Xfinity Rewards is now offering its members a complimentary one-year subscription to Perplexity Pro, our premium knowledge experience.Perplexity Pro empowers its subscribers with:\\xa0Pro Search: Enjoy increased daily use of Pro Search, our advanced search feature that breaks down queries with multi-step reasoning and programming, ensuring deeper, more comprehensive answers\\xa0AI model flexibility: Choose between the latest advanced AI models, including Anthropic’s Claude 3.5 Sonnet, OpenAI’s GPT-4 Omni, and Perplexity’s Sonar model (built on Meta’s LlaMa 3 70B)\\xa0File analysis: Upload documents for summaries or insightsMultimodal capabilities: Capture a picture or screenshot and asking questions about itImage generation: Access Playground v3, OpenAI DALL-E 3, Flux, and Stable Diffusion XL\\xa0Search like never before\\xa0\\xa0Have you ever wondered “Why does the USA have tornadoes in the Plains states?” Or “What universities have majors for undersea exploration?” Now, you can find an answer with a simple voice prompt or typed question.\\xa0\\xa0Eligible Xfinity customers can sign up for this offer on the Perplexity website using the unique code in their Xfinity Rewards account.\\xa0\\xa0This promotion is open to active Xfinity Rewards members. Existing Perplexity Pro account holders are ineligible to redeem this offer. To redeem your complimentary year of Perplexity Pro, you must sign up prior to August 29th, 2025 using the unique promo code provided to you by Xfinity. Your unique promo code may be used up to eight (8) times. Your access and use of Perplexity Pro is subject to Perplexity’s Terms of Service and Privacy Policy. For further questions or if you run into any issues redeeming the offer, email support@perplexity.ai.\\xa0Redeem a free year of Perplexity Pro through Xfinity Rewards\\xa0\\xa0Broadband internet connects the world, putting information at our fingertips. That’s why Perplexity is thrilled to offer Perplexity Pro—our premium consumer subscription—to Xfinity customers through its customer loyalty program, Xfinity Rewards.\\xa0\\xa0The idea behind Perplexity is simple: we all deserve a faster and better way to find answers. Whether you’re looking up a quick fact or diving into an in-depth analysis of stock performance, Perplexity searches the internet in real-time to deliver answers to any question, complete with in-line source links.\\xa0\\xa0Adding to their lineup of perks like $1 movie rentals and discounted tickets to theme parks and sporting events, Xfinity Rewards is now offering its members a complimentary one-year subscription to Perplexity Pro, our premium knowledge experience.Perplexity Pro empowers its subscribers with:\\xa0Pro Search: Enjoy increased daily use of Pro Search, our advanced search feature that breaks down queries with multi-step reasoning and programming, ensuring deeper, more comprehensive answers\\xa0AI model flexibility: Choose between the latest advanced AI models, including Anthropic’s Claude 3.5 Sonnet, OpenAI’s GPT-4 Omni, and Perplexity’s Sonar model (built on Meta’s LlaMa 3 70B)\\xa0File analysis: Upload documents for summaries or insightsMultimodal capabilities: Capture a picture or screenshot and asking questions about itImage generation: Access Playground v3, OpenAI DALL-E 3, Flux, and Stable Diffusion XL\\xa0Search like never before\\xa0\\xa0Have you ever wondered “Why does the USA have tornadoes in the Plains states?” Or “What universities have majors for undersea exploration?” Now, you can find an answer with a simple voice prompt or typed question.\\xa0\\xa0Eligible Xfinity customers can sign up for this offer on the Perplexity website using the unique code in their Xfinity Rewards account.\\xa0\\xa0This promotion is open to active Xfinity Rewards members. Existing Perplexity Pro account holders are ineligible to redeem this offer. To redeem your complimentary year of Perplexity Pro, you must sign up prior to August 29th, 2025 using the unique promo code provided to you by Xfinity. Your unique promo code may be used up to eight (8) times. Your access and use of Perplexity Pro is subject to Perplexity’s Terms of Service and Privacy Policy. For further questions or if you run into any issues redeeming the offer, email support@perplexity.ai.\\xa0Redeem a free year of Perplexity Pro through Xfinity Rewards\\xa0\\xa0Broadband internet connects the world, putting information at our fingertips. That’s why Perplexity is thrilled to offer Perplexity Pro—our premium consumer subscription—to Xfinity customers through its customer loyalty program, Xfinity Rewards.\\xa0\\xa0The idea behind Perplexity is simple: we all deserve a faster and better way to find answers. Whether you’re looking up a quick fact or diving into an in-depth analysis of stock performance, Perplexity searches the internet in real-time to deliver answers to any question, complete with in-line source links.\\xa0\\xa0Adding to their lineup of perks like $1 movie rentals and discounted tickets to theme parks and sporting events, Xfinity Rewards is now offering its members a complimentary one-year subscription to Perplexity Pro, our premium knowledge experience.Perplexity Pro empowers its subscribers with:\\xa0Pro Search: Enjoy increased daily use of Pro Search, our advanced search feature that breaks down queries with multi-step reasoning and programming, ensuring deeper, more comprehensive answers\\xa0AI model flexibility: Choose between the latest advanced AI models, including Anthropic’s Claude 3.5 Sonnet, OpenAI’s GPT-4 Omni, and Perplexity’s Sonar model (built on Meta’s LlaMa 3 70B)\\xa0File analysis: Upload documents for summaries or insightsMultimodal capabilities: Capture a picture or screenshot and asking questions about itImage generation: Access Playground v3, OpenAI DALL-E 3, Flux, and Stable Diffusion XL\\xa0Search like never before\\xa0\\xa0Have you ever wondered “Why does the USA have tornadoes in the Plains states?” Or “What universities have majors for undersea exploration?” Now, you can find an answer with a simple voice prompt or typed question.\\xa0\\xa0Eligible Xfinity customers can sign up for this offer on the Perplexity website using the unique code in their Xfinity Rewards account.\\xa0\\xa0This promotion is open to active Xfinity Rewards members. Existing Perplexity Pro account holders are ineligible to redeem this offer. To redeem your complimentary year of Perplexity Pro, you must sign up prior to August 29th, 2025 using the unique promo code provided to you by Xfinity. Your unique promo code may be used up to eight (8) times. Your access and use of Perplexity Pro is subject to Perplexity’s Terms of Service and Privacy Policy. For further questions or if you run into any issues redeeming the offer, email support@perplexity.ai.\\xa0'),\n",
       " Document(metadata={'source': 'https://www.perplexity.ai/hub/blog/perplexity-makes-gift-to-northwestern-medill-to-research-ai-and-journalism'}, page_content='Perplexity makes gift to Northwestern Medill to research AI and journalismJournalism has navigated numerous technological advances, from the advent of newspapers and radio to the rise of alternative outlets like social media and podcasting. The most recent monumental shift is the introduction of artificial intelligence and its impact not only on how we work but also on how we consume information. As this technology becomes more sophisticated and pervasive, how do we ensure that news publishers thrive as our Internet ecosystem changes?\\xa0We’re excited to announce that Perplexity is making a gift of $250k in support of Northwestern University’s Medill School of Journalism, Media, Integrated Marketing Communications to bolster its research on these critical topics. Led by Medill Professor and Knight Chair in Digital Media Strategy, Jeremy Gilbert, the Knight Lab is already experimenting with ways that AI can support the work journalists do, and how this technology will morph the news industry on a broader scale.\\xa0This research will continue to explore how AI can empower journalism workflows in a way that encourages transparency and accuracy, and additional AI capabilities we can develop to support reporting. The Lab plans will explore ways that generative AI can power tools for journalism and the new ways generative AI can improve news consumers’ experiences with media. In addition to this gift, Medill will receive data, insights, and access to Perplexity employees and publisher partners like The Texas Tribune to augment the Lab’s research.\\xa0“Generative AI is reshaping the journalism landscape,” said Gilbert. “AI is changing the landscape at a rapid pace, and Medill is committed to understanding how the industry will need to evolve and how we can use this technology responsibly. The gift from Perplexity provides vital resources to research AI and media.”\\xa0Says Perplexity CEO Aravind Srinivas: “AI can be a powerful tool for researching, fact-checking, and improving the accessibility of key reporting, but we need to operate with a framework that prioritizes quality and reliability. I admire the research Medill has done thus far on these topics and appreciate its commitment to further exploring how AI companies can collaborate with news publishers to promote a thriving Internet ecosystem.”\\xa0This gift follows Perplexity’s recently launched Publishers’ Program, which will provide revenue-sharing and other technological benefits to publishers. To learn more about this research or how Perplexity works with publishers, contact publishers@perplexity.ai.Perplexity makes gift to Northwestern Medill to research AI and journalismJournalism has navigated numerous technological advances, from the advent of newspapers and radio to the rise of alternative outlets like social media and podcasting. The most recent monumental shift is the introduction of artificial intelligence and its impact not only on how we work but also on how we consume information. As this technology becomes more sophisticated and pervasive, how do we ensure that news publishers thrive as our Internet ecosystem changes?\\xa0We’re excited to announce that Perplexity is making a gift of $250k in support of Northwestern University’s Medill School of Journalism, Media, Integrated Marketing Communications to bolster its research on these critical topics. Led by Medill Professor and Knight Chair in Digital Media Strategy, Jeremy Gilbert, the Knight Lab is already experimenting with ways that AI can support the work journalists do, and how this technology will morph the news industry on a broader scale.\\xa0This research will continue to explore how AI can empower journalism workflows in a way that encourages transparency and accuracy, and additional AI capabilities we can develop to support reporting. The Lab plans will explore ways that generative AI can power tools for journalism and the new ways generative AI can improve news consumers’ experiences with media. In addition to this gift, Medill will receive data, insights, and access to Perplexity employees and publisher partners like The Texas Tribune to augment the Lab’s research.\\xa0“Generative AI is reshaping the journalism landscape,” said Gilbert. “AI is changing the landscape at a rapid pace, and Medill is committed to understanding how the industry will need to evolve and how we can use this technology responsibly. The gift from Perplexity provides vital resources to research AI and media.”\\xa0Says Perplexity CEO Aravind Srinivas: “AI can be a powerful tool for researching, fact-checking, and improving the accessibility of key reporting, but we need to operate with a framework that prioritizes quality and reliability. I admire the research Medill has done thus far on these topics and appreciate its commitment to further exploring how AI companies can collaborate with news publishers to promote a thriving Internet ecosystem.”\\xa0This gift follows Perplexity’s recently launched Publishers’ Program, which will provide revenue-sharing and other technological benefits to publishers. To learn more about this research or how Perplexity works with publishers, contact publishers@perplexity.ai.Perplexity makes gift to Northwestern Medill to research AI and journalismJournalism has navigated numerous technological advances, from the advent of newspapers and radio to the rise of alternative outlets like social media and podcasting. The most recent monumental shift is the introduction of artificial intelligence and its impact not only on how we work but also on how we consume information. As this technology becomes more sophisticated and pervasive, how do we ensure that news publishers thrive as our Internet ecosystem changes?\\xa0We’re excited to announce that Perplexity is making a gift of $250k in support of Northwestern University’s Medill School of Journalism, Media, Integrated Marketing Communications to bolster its research on these critical topics. Led by Medill Professor and Knight Chair in Digital Media Strategy, Jeremy Gilbert, the Knight Lab is already experimenting with ways that AI can support the work journalists do, and how this technology will morph the news industry on a broader scale.\\xa0This research will continue to explore how AI can empower journalism workflows in a way that encourages transparency and accuracy, and additional AI capabilities we can develop to support reporting. The Lab plans will explore ways that generative AI can power tools for journalism and the new ways generative AI can improve news consumers’ experiences with media. In addition to this gift, Medill will receive data, insights, and access to Perplexity employees and publisher partners like The Texas Tribune to augment the Lab’s research.\\xa0“Generative AI is reshaping the journalism landscape,” said Gilbert. “AI is changing the landscape at a rapid pace, and Medill is committed to understanding how the industry will need to evolve and how we can use this technology responsibly. The gift from Perplexity provides vital resources to research AI and media.”\\xa0Says Perplexity CEO Aravind Srinivas: “AI can be a powerful tool for researching, fact-checking, and improving the accessibility of key reporting, but we need to operate with a framework that prioritizes quality and reliability. I admire the research Medill has done thus far on these topics and appreciate its commitment to further exploring how AI companies can collaborate with news publishers to promote a thriving Internet ecosystem.”\\xa0This gift follows Perplexity’s recently launched Publishers’ Program, which will provide revenue-sharing and other technological benefits to publishers. To learn more about this research or how Perplexity works with publishers, contact publishers@perplexity.ai.'),\n",
       " Document(metadata={'source': 'https://www.perplexity.ai/hub/blog/eligible-uber-one-members-can-now-unlock-a-complimentary-full-year-of-perplexity-pro'}, page_content='Eligible Uber One members can now unlock a complimentary full year of Perplexity Pro\\xa0Uber One members can now save even more time with perks like Pro SearchWhen life gets busy, we can all use more efficiency. With Uber One, Uber’s membership program, millions of people get access to benefits and savings. Starting today, eligible Uber One members will have access to another perk that will help save even more time with information gathering and research.From now through October 31, eligible Uber One members in the US and Canada can redeem a complimentary year of Perplexity Pro – a $200 value. With unlimited use of Perplexity’s “answer engine,” members can get conversational responses to questions and everyday search queries.\\xa0With Perplexity, you can exercise your curiosity and learn more about any topic in an engaging format, such as “Who invented the hamburger?” “What is the largest fast food franchise in Canada?” and “Why am I suddenly seeing probiotic sodas everywhere?”\\xa0With Perplexity Pro, Uber One members can also benefit from other features, such as:\\xa0Pro Search: Perplexity’s advanced search feature, with the ability to break down queries into smaller steps, allowing for more comprehensive and accurate searches\\xa0AI model flexibility: Switch between different advanced AI models, including Anthropic’s Claude 3 Opus and Claude 3.5 Sonnet, OpenAI’s GPT-4o, and Perplexity’s Sonar model (built on Meta’s LlaMa 3.1 405B)File analysis: Upload PDFs, CSVs, and images for instant insights\\xa0Image generation: Access to AI image generation models Playground v2.5, DALL-E 3, and Stable Diffusion XL\\xa0“At Uber, we’re focused on making life a bit more effortless, which is why we built Uber One as the ultimate way to save across rides and delivery. Now, we’re thrilled that Perplexity is offering members added efficiency with free access to Perplexity Pro,” says Sarfraz Maredia, Vice President and Head of Americas at Uber Eats. “Millions of people have been trying out AI tools and apps, and we know that Perplexity’s powerful search, AI modeling, and image generation will help members get the answers quickly and easily.”\\xa0Eligible Uber One members can sign up on the Perplexity website using the unique code in their email inbox to access this limited-time offer.\\xa0Offer expires 10/31/24, while supplies last. US and Canada only. Must be (1) current Uber One member, and (2) register with a new Perplexity Pro account to be eligible. Must maintain active Uber One membership throughout the duration of the promo. Promo valid for recipient email address only, must use the unique link above to sign-up. Offer subject to change. Uber One Membership Terms apply to Uber One membership. Existing Perplexity Pro account holders are ineligible to redeem this offer. Access and use of Perplexity Pro is subject to Perplexity’s Terms of Service and Privacy Policy.\\xa0Eligible Uber One members can now unlock a complimentary full year of Perplexity Pro\\xa0Uber One members can now save even more time with perks like Pro SearchWhen life gets busy, we can all use more efficiency. With Uber One, Uber’s membership program, millions of people get access to benefits and savings. Starting today, eligible Uber One members will have access to another perk that will help save even more time with information gathering and research.From now through October 31, eligible Uber One members in the US and Canada can redeem a complimentary year of Perplexity Pro – a $200 value. With unlimited use of Perplexity’s “answer engine,” members can get conversational responses to questions and everyday search queries.\\xa0With Perplexity, you can exercise your curiosity and learn more about any topic in an engaging format, such as “Who invented the hamburger?” “What is the largest fast food franchise in Canada?” and “Why am I suddenly seeing probiotic sodas everywhere?”\\xa0With Perplexity Pro, Uber One members can also benefit from other features, such as:\\xa0Pro Search: Perplexity’s advanced search feature, with the ability to break down queries into smaller steps, allowing for more comprehensive and accurate searches\\xa0AI model flexibility: Switch between different advanced AI models, including Anthropic’s Claude 3 Opus and Claude 3.5 Sonnet, OpenAI’s GPT-4o, and Perplexity’s Sonar model (built on Meta’s LlaMa 3.1 405B)File analysis: Upload PDFs, CSVs, and images for instant insights\\xa0Image generation: Access to AI image generation models Playground v2.5, DALL-E 3, and Stable Diffusion XL\\xa0“At Uber, we’re focused on making life a bit more effortless, which is why we built Uber One as the ultimate way to save across rides and delivery. Now, we’re thrilled that Perplexity is offering members added efficiency with free access to Perplexity Pro,” says Sarfraz Maredia, Vice President and Head of Americas at Uber Eats. “Millions of people have been trying out AI tools and apps, and we know that Perplexity’s powerful search, AI modeling, and image generation will help members get the answers quickly and easily.”\\xa0Eligible Uber One members can sign up on the Perplexity website using the unique code in their email inbox to access this limited-time offer.\\xa0Offer expires 10/31/24, while supplies last. US and Canada only. Must be (1) current Uber One member, and (2) register with a new Perplexity Pro account to be eligible. Must maintain active Uber One membership throughout the duration of the promo. Promo valid for recipient email address only, must use the unique link above to sign-up. Offer subject to change. Uber One Membership Terms apply to Uber One membership. Existing Perplexity Pro account holders are ineligible to redeem this offer. Access and use of Perplexity Pro is subject to Perplexity’s Terms of Service and Privacy Policy.\\xa0Eligible Uber One members can now unlock a complimentary full year of Perplexity Pro\\xa0Uber One members can now save even more time with perks like Pro SearchWhen life gets busy, we can all use more efficiency. With Uber One, Uber’s membership program, millions of people get access to benefits and savings. Starting today, eligible Uber One members will have access to another perk that will help save even more time with information gathering and research.From now through October 31, eligible Uber One members in the US and Canada can redeem a complimentary year of Perplexity Pro – a $200 value. With unlimited use of Perplexity’s “answer engine,” members can get conversational responses to questions and everyday search queries.\\xa0With Perplexity, you can exercise your curiosity and learn more about any topic in an engaging format, such as “Who invented the hamburger?” “What is the largest fast food franchise in Canada?” and “Why am I suddenly seeing probiotic sodas everywhere?”\\xa0With Perplexity Pro, Uber One members can also benefit from other features, such as:\\xa0Pro Search: Perplexity’s advanced search feature, with the ability to break down queries into smaller steps, allowing for more comprehensive and accurate searches\\xa0AI model flexibility: Switch between different advanced AI models, including Anthropic’s Claude 3 Opus and Claude 3.5 Sonnet, OpenAI’s GPT-4o, and Perplexity’s Sonar model (built on Meta’s LlaMa 3.1 405B)File analysis: Upload PDFs, CSVs, and images for instant insights\\xa0Image generation: Access to AI image generation models Playground v2.5, DALL-E 3, and Stable Diffusion XL\\xa0“At Uber, we’re focused on making life a bit more effortless, which is why we built Uber One as the ultimate way to save across rides and delivery. Now, we’re thrilled that Perplexity is offering members added efficiency with free access to Perplexity Pro,” says Sarfraz Maredia, Vice President and Head of Americas at Uber Eats. “Millions of people have been trying out AI tools and apps, and we know that Perplexity’s powerful search, AI modeling, and image generation will help members get the answers quickly and easily.”\\xa0Eligible Uber One members can sign up on the Perplexity website using the unique code in their email inbox to access this limited-time offer.\\xa0Offer expires 10/31/24, while supplies last. US and Canada only. Must be (1) current Uber One member, and (2) register with a new Perplexity Pro account to be eligible. Must maintain active Uber One membership throughout the duration of the promo. Promo valid for recipient email address only, must use the unique link above to sign-up. Offer subject to change. Uber One Membership Terms apply to Uber One membership. Existing Perplexity Pro account holders are ineligible to redeem this offer. Access and use of Perplexity Pro is subject to Perplexity’s Terms of Service and Privacy Policy.\\xa0'),\n",
       " Document(metadata={'source': 'https://www.perplexity.ai/hub/blog/introducing-the-perplexity-publishers-program'}, page_content='Introducing the Perplexity Publishers’ Program\\xa0Every day, people turn to Perplexity with a wide array of questions. Our ability to provide high-quality answers hinges on trusted, accurate sources covering the topics people care about most. From day one, we’ve included citations in each answer, ensuring publishers receive proper credit and building user trust.\\xa0To further support the vital work of media organizations and online creators, we need to ensure publishers can thrive as Perplexity grows. That’s why we’re excited to announce the Perplexity Publishers Program and our first batch of partners: TIME, Der Spiegel, Fortune, Entrepreneur, The Texas Tribune, and WordPress.com.This program is designed to promote collective success and to equip publishers with new technology to engage their audiences. Here are the key components:\\xa0Revenue sharing: In the coming months, we’ll introduce advertising through our related questions feature. Brands can pay to ask specific related follow-up questions in our answer engine interface and on Pages. When Perplexity earns revenue from an interaction where a publisher’s content is referenced, that publisher will also earn a share. We’re also excited to work with ScalePost.ai, a platform that streamlines collaborations between content publishers and AI companies and provides AI analytics for publishers. Our collaboration with them will enable our partners to gain deeper insights into how Perplexity cites their content.Access to Perplexity’s APIs: Partners will also receive free access to our Online LLM APIs and developer support. This allows each publisher to create their own custom answer engine on their website. Visitors can ask questions and receive answers citing only that publisher’s content. We’re also offering our related questions technology for integration into their stories. We are excited to equip our partners with our technology so they can engage with their readers in new ways.Enterprise Pro for all employees: We’ve heard from creators that Perplexity is a valuable research and fact-checking tool. To support this, we’re making our Enterprise Pro offering—with enhanced data privacy and security capabilities—available to all employees of our publisher partners for free for one year.\\xa0The Internet ecosystem is evolving, and we want this program to grow and adapt. We’re open to other types of collaborations with publishers in the future, such as bundled subscriptions, where users could pay a flat fee for both Perplexity Pro and subscriptions to participating publishers.\\xa0\"TIME is committed to embracing innovative new technologies and platforms to advance our mission of providing trusted journalism for audiences around the world. We are proud to join Perplexity\\'s Publishers’ Program as a launch partner to continue to expand access to reliable information and engage audiences in new ways,\" said TIME Chief Operating Officer Mark Howard.\\xa0We are also modifying our processes and products based on feedback from our publishing partners. Recently, we updated how our systems index and cite sources. We’re also collecting feedback to guide our product roadmap and new feature releases.\\xa0\\xa0Perplexity CEO Aravind Srinivas says, “We structured this program to ensure we have a scalable and sustainable way to align incentives for all parties. We appreciate the publishers who have joined us for this program and provided us with valuable feedback about how it should operate. We have always believed that we can build a system where the whole Internet wins, and this is just the first step.”The Perplexity Publishers’ Program marks a significant move towards aligning the interests of AI technology and quality journalism. We’re excited about the potential of this program to support the future of digital publishing and to ensure that high-quality, trustworthy content remains at the heart of the AI-powered information landscape.\\xa0publishers@perplexity.aiIntroducing the Perplexity Publishers’ Program\\xa0Every day, people turn to Perplexity with a wide array of questions. Our ability to provide high-quality answers hinges on trusted, accurate sources covering the topics people care about most. From day one, we’ve included citations in each answer, ensuring publishers receive proper credit and building user trust.\\xa0To further support the vital work of media organizations and online creators, we need to ensure publishers can thrive as Perplexity grows. That’s why we’re excited to announce the Perplexity Publishers Program and our first batch of partners: TIME, Der Spiegel, Fortune, Entrepreneur, The Texas Tribune, and WordPress.com.This program is designed to promote collective success and to equip publishers with new technology to engage their audiences. Here are the key components:\\xa0Revenue sharing: In the coming months, we’ll introduce advertising through our related questions feature. Brands can pay to ask specific related follow-up questions in our answer engine interface and on Pages. When Perplexity earns revenue from an interaction where a publisher’s content is referenced, that publisher will also earn a share. We’re also excited to work with ScalePost.ai, a platform that streamlines collaborations between content publishers and AI companies and provides AI analytics for publishers. Our collaboration with them will enable our partners to gain deeper insights into how Perplexity cites their content.Access to Perplexity’s APIs: Partners will also receive free access to our Online LLM APIs and developer support. This allows each publisher to create their own custom answer engine on their website. Visitors can ask questions and receive answers citing only that publisher’s content. We’re also offering our related questions technology for integration into their stories. We are excited to equip our partners with our technology so they can engage with their readers in new ways.Enterprise Pro for all employees: We’ve heard from creators that Perplexity is a valuable research and fact-checking tool. To support this, we’re making our Enterprise Pro offering—with enhanced data privacy and security capabilities—available to all employees of our publisher partners for free for one year.\\xa0The Internet ecosystem is evolving, and we want this program to grow and adapt. We’re open to other types of collaborations with publishers in the future, such as bundled subscriptions, where users could pay a flat fee for both Perplexity Pro and subscriptions to participating publishers.\\xa0\"TIME is committed to embracing innovative new technologies and platforms to advance our mission of providing trusted journalism for audiences around the world. We are proud to join Perplexity\\'s Publishers’ Program as a launch partner to continue to expand access to reliable information and engage audiences in new ways,\" said TIME Chief Operating Officer Mark Howard.\\xa0We are also modifying our processes and products based on feedback from our publishing partners. Recently, we updated how our systems index and cite sources. We’re also collecting feedback to guide our product roadmap and new feature releases.\\xa0\\xa0Perplexity CEO Aravind Srinivas says, “We structured this program to ensure we have a scalable and sustainable way to align incentives for all parties. We appreciate the publishers who have joined us for this program and provided us with valuable feedback about how it should operate. We have always believed that we can build a system where the whole Internet wins, and this is just the first step.”The Perplexity Publishers’ Program marks a significant move towards aligning the interests of AI technology and quality journalism. We’re excited about the potential of this program to support the future of digital publishing and to ensure that high-quality, trustworthy content remains at the heart of the AI-powered information landscape.\\xa0publishers@perplexity.aiIntroducing the Perplexity Publishers’ Program\\xa0Every day, people turn to Perplexity with a wide array of questions. Our ability to provide high-quality answers hinges on trusted, accurate sources covering the topics people care about most. From day one, we’ve included citations in each answer, ensuring publishers receive proper credit and building user trust.\\xa0To further support the vital work of media organizations and online creators, we need to ensure publishers can thrive as Perplexity grows. That’s why we’re excited to announce the Perplexity Publishers Program and our first batch of partners: TIME, Der Spiegel, Fortune, Entrepreneur, The Texas Tribune, and WordPress.com.This program is designed to promote collective success and to equip publishers with new technology to engage their audiences. Here are the key components:\\xa0Revenue sharing: In the coming months, we’ll introduce advertising through our related questions feature. Brands can pay to ask specific related follow-up questions in our answer engine interface and on Pages. When Perplexity earns revenue from an interaction where a publisher’s content is referenced, that publisher will also earn a share. We’re also excited to work with ScalePost.ai, a platform that streamlines collaborations between content publishers and AI companies and provides AI analytics for publishers. Our collaboration with them will enable our partners to gain deeper insights into how Perplexity cites their content.Access to Perplexity’s APIs: Partners will also receive free access to our Online LLM APIs and developer support. This allows each publisher to create their own custom answer engine on their website. Visitors can ask questions and receive answers citing only that publisher’s content. We’re also offering our related questions technology for integration into their stories. We are excited to equip our partners with our technology so they can engage with their readers in new ways.Enterprise Pro for all employees: We’ve heard from creators that Perplexity is a valuable research and fact-checking tool. To support this, we’re making our Enterprise Pro offering—with enhanced data privacy and security capabilities—available to all employees of our publisher partners for free for one year.\\xa0The Internet ecosystem is evolving, and we want this program to grow and adapt. We’re open to other types of collaborations with publishers in the future, such as bundled subscriptions, where users could pay a flat fee for both Perplexity Pro and subscriptions to participating publishers.\\xa0\"TIME is committed to embracing innovative new technologies and platforms to advance our mission of providing trusted journalism for audiences around the world. We are proud to join Perplexity\\'s Publishers’ Program as a launch partner to continue to expand access to reliable information and engage audiences in new ways,\" said TIME Chief Operating Officer Mark Howard.\\xa0We are also modifying our processes and products based on feedback from our publishing partners. Recently, we updated how our systems index and cite sources. We’re also collecting feedback to guide our product roadmap and new feature releases.\\xa0\\xa0Perplexity CEO Aravind Srinivas says, “We structured this program to ensure we have a scalable and sustainable way to align incentives for all parties. We appreciate the publishers who have joined us for this program and provided us with valuable feedback about how it should operate. We have always believed that we can build a system where the whole Internet wins, and this is just the first step.”The Perplexity Publishers’ Program marks a significant move towards aligning the interests of AI technology and quality journalism. We’re excited about the potential of this program to support the future of digital publishing and to ensure that high-quality, trustworthy content remains at the heart of the AI-powered information landscape.\\xa0publishers@perplexity.ai'),\n",
       " Document(metadata={'source': 'https://www.perplexity.ai/hub/blog/perplexity-collaborates-with-amazon-web-services-to-launch-enterprise-pro'}, page_content='Perplexity collaborates with Amazon Web Services to launch Enterprise ProWe\\'re excited to announce a strategic collaboration with Amazon Web Services (AWS) to bring Perplexity Enterprise Pro to their broad customer base. This collaboration marks a significant milestone in our mission to empower organizations with AI-powered research tools that enhance efficiency and productivity without compromising security and control.Perplexity uses Amazon Bedrock to support its generative AI capabilities, which the company announced at the end of last year. This collaboration opens up new avenues for businesses to harness the power of AI-driven search and analytics, transforming how teams access and utilize information. Through this new agreement with AWS, Perplexity will collaborate on joint events, co-sell engagements, and co-marketing.\"Perplexity Enterprise Pro has allowed Databricks to substantially accelerate R&D, making it easier for our engineering, marketing, and sales teams to execute faster. We estimate it helps our team save 5,000 working hours monthly.\" —Ali Ghodsi, CEO of Databricks“Perplexity Enterprise Pro, powered by Amazon Bedrock, enables customers to easily leverage SOTA foundation models to improve employee productivity. We’re excited to deepen our collaboration with Perplexity to help bring this revolutionary approach to search to AWS customers.” —Atul Deo, General Manager of Amazon Bedrock.Today’s news is the first step in expanding Perplexity Enterprise Pro\\'s availability globally. For more information please visit: https://www.perplexity.ai/enterprisePerplexity collaborates with Amazon Web Services to launch Enterprise ProWe\\'re excited to announce a strategic collaboration with Amazon Web Services (AWS) to bring Perplexity Enterprise Pro to their broad customer base. This collaboration marks a significant milestone in our mission to empower organizations with AI-powered research tools that enhance efficiency and productivity without compromising security and control.Perplexity uses Amazon Bedrock to support its generative AI capabilities, which the company announced at the end of last year. This collaboration opens up new avenues for businesses to harness the power of AI-driven search and analytics, transforming how teams access and utilize information. Through this new agreement with AWS, Perplexity will collaborate on joint events, co-sell engagements, and co-marketing.\"Perplexity Enterprise Pro has allowed Databricks to substantially accelerate R&D, making it easier for our engineering, marketing, and sales teams to execute faster. We estimate it helps our team save 5,000 working hours monthly.\" —Ali Ghodsi, CEO of Databricks“Perplexity Enterprise Pro, powered by Amazon Bedrock, enables customers to easily leverage SOTA foundation models to improve employee productivity. We’re excited to deepen our collaboration with Perplexity to help bring this revolutionary approach to search to AWS customers.” —Atul Deo, General Manager of Amazon Bedrock.Today’s news is the first step in expanding Perplexity Enterprise Pro\\'s availability globally. For more information please visit: https://www.perplexity.ai/enterprisePerplexity collaborates with Amazon Web Services to launch Enterprise ProWe\\'re excited to announce a strategic collaboration with Amazon Web Services (AWS) to bring Perplexity Enterprise Pro to their broad customer base. This collaboration marks a significant milestone in our mission to empower organizations with AI-powered research tools that enhance efficiency and productivity without compromising security and control.Perplexity uses Amazon Bedrock to support its generative AI capabilities, which the company announced at the end of last year. This collaboration opens up new avenues for businesses to harness the power of AI-driven search and analytics, transforming how teams access and utilize information. Through this new agreement with AWS, Perplexity will collaborate on joint events, co-sell engagements, and co-marketing.\"Perplexity Enterprise Pro has allowed Databricks to substantially accelerate R&D, making it easier for our engineering, marketing, and sales teams to execute faster. We estimate it helps our team save 5,000 working hours monthly.\" —Ali Ghodsi, CEO of Databricks“Perplexity Enterprise Pro, powered by Amazon Bedrock, enables customers to easily leverage SOTA foundation models to improve employee productivity. We’re excited to deepen our collaboration with Perplexity to help bring this revolutionary approach to search to AWS customers.” —Atul Deo, General Manager of Amazon Bedrock.Today’s news is the first step in expanding Perplexity Enterprise Pro\\'s availability globally. For more information please visit: https://www.perplexity.ai/enterprise'),\n",
       " Document(metadata={'source': 'https://www.perplexity.ai/hub/blog/pro-search-upgraded-for-more-advanced-problem-solving'}, page_content=\"Pro Search: Upgraded for more advanced problem-solvingResearch shapes our daily lives. We use it to make informed decisions and solve problems—to innovate, learn, and grow. But it can be a time-consuming process, requiring hours of information gathering and analysis.Pro Search has been our answer to this challenge, revolutionizing knowledge discovery by making research faster and more efficient than ever before. Now, we’re taking it to the next level.Introducing the next evolution of Pro SearchWe have improved Pro search to tackle more complex queries, perform advanced math and programming computations, and deliver even more thoroughly researched answers.Multi-step reasoningPro Search now approaches intricate problems with more multi-step reasoning. It understands when a question requires planning, works through goals step-by-step, and synthesizes in-depth answers with greater efficiency. Moreover, Pro Search can analyze search results and take intelligent actions based on its findings. This includes initiating follow-up searches that build on previous results.View answerAdvanced math and programming capabilitiesWe’ve supercharged Pro Search's code execution, making it faster and more powerful for data analysis, debugging, and content generation. Plus, with the integration of the Wolfram|Alpha engine, Pro Search now solves complex mathematical questions with unprecedented accuracy and speed.View answerQuick Search vs Pro SearchOur Quick Search option is for fast and accurate answers backed by sources. It’s perfect for when you’re looking for quick, efficient information. But sometimes, you need to conduct a thorough, deep search across the internet, as if an expert research assistant scoured the web and returned a comprehensive report and analysis.That’s where the newly upgraded Pro Search comes in. It delivers even greater depth, precision, and comprehensive insights. Simply click the Pro toggle in the search bar, ask a question, and watch Perplexity explore every facet of your query with improved accuracy. Pro Search links to a broader range of trustworthy sources and shows you the step-by-step process behind its findings.View more Pro Search examplesWho can access Pro Search?Everyone can use Pro Search five times every four hours for free. For those who need more, Perplexity Pro subscribers get nearly unlimited daily access.Experience the new Pro SearchThe upgraded Pro Search can pinpoint case laws for attorneys, summarize trend analysis for marketers, and debug code for developers—and that’s just the start. Whatever your profession, Pro Search empowers you to make more informed decisions.Get Perplexity ProPro Search: Upgraded for more advanced problem-solvingResearch shapes our daily lives. We use it to make informed decisions and solve problems—to innovate, learn, and grow. But it can be a time-consuming process, requiring hours of information gathering and analysis.Pro Search has been our answer to this challenge, revolutionizing knowledge discovery by making research faster and more efficient than ever before. Now, we’re taking it to the next level.Introducing the next evolution of Pro SearchWe have improved Pro search to tackle more complex queries, perform advanced math and programming computations, and deliver even more thoroughly researched answers.Multi-step reasoningPro Search now approaches intricate problems with more multi-step reasoning. It understands when a question requires planning, works through goals step-by-step, and synthesizes in-depth answers with greater efficiency. Moreover, Pro Search can analyze search results and take intelligent actions based on its findings. This includes initiating follow-up searches that build on previous results.View answerAdvanced math and programming capabilitiesWe’ve supercharged Pro Search's code execution, making it faster and more powerful for data analysis, debugging, and content generation. Plus, with the integration of the Wolfram|Alpha engine, Pro Search now solves complex mathematical questions with unprecedented accuracy and speed.View answerQuick Search vs Pro SearchOur Quick Search option is for fast and accurate answers backed by sources. It’s perfect for when you’re looking for quick, efficient information. But sometimes, you need to conduct a thorough, deep search across the internet, as if an expert research assistant scoured the web and returned a comprehensive report and analysis.That’s where the newly upgraded Pro Search comes in. It delivers even greater depth, precision, and comprehensive insights. Simply click the Pro toggle in the search bar, ask a question, and watch Perplexity explore every facet of your query with improved accuracy. Pro Search links to a broader range of trustworthy sources and shows you the step-by-step process behind its findings.View more Pro Search examplesWho can access Pro Search?Everyone can use Pro Search five times every four hours for free. For those who need more, Perplexity Pro subscribers get nearly unlimited daily access.Experience the new Pro SearchThe upgraded Pro Search can pinpoint case laws for attorneys, summarize trend analysis for marketers, and debug code for developers—and that’s just the start. Whatever your profession, Pro Search empowers you to make more informed decisions.Get Perplexity ProPro Search: Upgraded for more advanced problem-solvingResearch shapes our daily lives. We use it to make informed decisions and solve problems—to innovate, learn, and grow. But it can be a time-consuming process, requiring hours of information gathering and analysis.Pro Search has been our answer to this challenge, revolutionizing knowledge discovery by making research faster and more efficient than ever before. Now, we’re taking it to the next level.Introducing the next evolution of Pro SearchWe have improved Pro search to tackle more complex queries, perform advanced math and programming computations, and deliver even more thoroughly researched answers.Multi-step reasoningPro Search now approaches intricate problems with more multi-step reasoning. It understands when a question requires planning, works through goals step-by-step, and synthesizes in-depth answers with greater efficiency. Moreover, Pro Search can analyze search results and take intelligent actions based on its findings. This includes initiating follow-up searches that build on previous results.View answerAdvanced math and programming capabilitiesWe’ve supercharged Pro Search's code execution, making it faster and more powerful for data analysis, debugging, and content generation. Plus, with the integration of the Wolfram|Alpha engine, Pro Search now solves complex mathematical questions with unprecedented accuracy and speed.View answerQuick Search vs Pro SearchOur Quick Search option is for fast and accurate answers backed by sources. It’s perfect for when you’re looking for quick, efficient information. But sometimes, you need to conduct a thorough, deep search across the internet, as if an expert research assistant scoured the web and returned a comprehensive report and analysis.That’s where the newly upgraded Pro Search comes in. It delivers even greater depth, precision, and comprehensive insights. Simply click the Pro toggle in the search bar, ask a question, and watch Perplexity explore every facet of your query with improved accuracy. Pro Search links to a broader range of trustworthy sources and shows you the step-by-step process behind its findings.View more Pro Search examplesWho can access Pro Search?Everyone can use Pro Search five times every four hours for free. For those who need more, Perplexity Pro subscribers get nearly unlimited daily access.Experience the new Pro SearchThe upgraded Pro Search can pinpoint case laws for attorneys, summarize trend analysis for marketers, and debug code for developers—and that’s just the start. Whatever your profession, Pro Search empowers you to make more informed decisions.Get Perplexity Pro\"),\n",
       " Document(metadata={'source': 'https://www.perplexity.ai/hub/blog/bringing-perplexity-to-education-and-not-for-profits'}, page_content='Bringing Perplexity to education and not-for-profits AI will be an integral part of our lives, influencing how we learn, consume information, and work. For AI to make the most positive impact, it needs to be accessible to all organizations, not just for-profit companies.Earlier this year, we introduced Perplexity Enterprise Pro to enable companies to leverage our AI with enhanced features like user management, increased data privacy, SOC2 compliance, and fortified security. Now, we’re excited to extend these benefits to philanthropic and public organizations by offering Perplexity Enterprise Pro at a reduced cost.For all schools, universities, nonprofits, government agencies, and other not-for-profit organizations, Enterprise Pro will be available for $20/seat per month or $200/seat annually.Perplexity for educationIncreasingly, we’re hearing from educators that they want to teach their students to use AI effectively and responsibly. As AI becomes more prevalent, people of all ages will need to understand how to utilize it with strong critical thinking and media literacy skills.As a knowledge-based platform, we want to make Perplexity Enterprise Pro available to students ages 13 and up while ensuring it is conducive to learning. Our commitment to citing sources and prioritizing accuracy can teach students valuable skills like researching and vetting information. By making Perplexity more affordable for schools, we hope to expand access to our technology and equip students with the knowledge they need to solve the problems of tomorrow.We’ve begun rolling out Perplexity Enterprise Pro to several schools, including ‘Iolani School in Hawaii. Over the next few months of summer school, teachers will learn how tools like Perplexity can support both their growth and their students’ development.\\xa0“‘Iolani is thrilled to be the first K-12 school to partner with Perplexity, providing our students and faculty with safe access to an advanced AI research and knowledge tool. This partnership is an exciting opportunity to develop case studies that demonstrate how AI can enhance learning, research, creativity, and productivity within an academic setting. We believe that integrating Perplexity into our educational framework will empower our community to explore innovative approaches to research and foster a deeper understanding of AI’s potential in various fields.” - Dr. Michael Lomuscio, Dean of Studies at ‘Iolani SchoolPerplexity isn’t just a tool for students—it’s also a valuable resource for teachers and administrators. It can help find resources, develop lesson plans, and quickly answer any question. If you’re an educator looking to learn how to use Perplexity, check out our focused seven-day course, Perplexity for Educators.Perplexity for not-for-profits and public servants\\xa0Employees at nonprofits and government agencies are already turning to Perplexity for quick answers and real-time updates on trending news, but not all organizations can afford Enterprise Pro subscriptions. That’s why we’re reducing the cost of Enterprise Pro for not-for-profits and government agencies — so more teams can benefit from our research and data analysis capabilities, freeing up time and resources and allowing staff to focus on higher-value activities.Here are some ways existing Enterprise Pro customers, like the U.S. Anti-Doping Agency and the Montana Department of Natural Resources and Conservation, benefit from Perplexity:Conduct research on real-time policy updates and current events with no knowledge cutoffFind relevant statistics for marketing materials and grant proposalsGenerate translations, transcriptions, and summaries quickly\\xa0Create internal documents, including first drafts of new policies, job vacancy postings, and meeting agendasBuild initial project plans and work breakdownsAnalyze large datasets to gain valuable insights into operations, programs, and donor behaviorsVerify server and coding issue fixes quickly\\xa0Simplify the language of complex documents\"I use Perplexity daily to speed up my workflows around research and analysis, as well as getting past the initial blank page when drafting documents. Just providing a good starting point is often enough to save hours of time weekly. Having the sources of answers cited in-text is a huge benefit to verify the generated answer, and access to multiple AI models is icing on the cake.\" - Chris Powell, Chief Information Officer at the Montana Department of Natural Resources and Conservation\\xa0We’re so appreciative of the work nonprofits and government agencies do to address critical societal issues and are excited to see the value they gain from using Perplexity.Visit perplexity.ai/enterprise or reach out to enterprise@perplexity.ai to learn more about how to get started with Perplexity as a school, university, not-for-profit organization, or government agency.Bringing Perplexity to education and not-for-profits AI will be an integral part of our lives, influencing how we learn, consume information, and work. For AI to make the most positive impact, it needs to be accessible to all organizations, not just for-profit companies.Earlier this year, we introduced Perplexity Enterprise Pro to enable companies to leverage our AI with enhanced features like user management, increased data privacy, SOC2 compliance, and fortified security. Now, we’re excited to extend these benefits to philanthropic and public organizations by offering Perplexity Enterprise Pro at a reduced cost.For all schools, universities, nonprofits, government agencies, and other not-for-profit organizations, Enterprise Pro will be available for $20/seat per month or $200/seat annually.Perplexity for educationIncreasingly, we’re hearing from educators that they want to teach their students to use AI effectively and responsibly. As AI becomes more prevalent, people of all ages will need to understand how to utilize it with strong critical thinking and media literacy skills.As a knowledge-based platform, we want to make Perplexity Enterprise Pro available to students ages 13 and up while ensuring it is conducive to learning. Our commitment to citing sources and prioritizing accuracy can teach students valuable skills like researching and vetting information. By making Perplexity more affordable for schools, we hope to expand access to our technology and equip students with the knowledge they need to solve the problems of tomorrow.We’ve begun rolling out Perplexity Enterprise Pro to several schools, including ‘Iolani School in Hawaii. Over the next few months of summer school, teachers will learn how tools like Perplexity can support both their growth and their students’ development.\\xa0“‘Iolani is thrilled to be the first K-12 school to partner with Perplexity, providing our students and faculty with safe access to an advanced AI research and knowledge tool. This partnership is an exciting opportunity to develop case studies that demonstrate how AI can enhance learning, research, creativity, and productivity within an academic setting. We believe that integrating Perplexity into our educational framework will empower our community to explore innovative approaches to research and foster a deeper understanding of AI’s potential in various fields.” - Dr. Michael Lomuscio, Dean of Studies at ‘Iolani SchoolPerplexity isn’t just a tool for students—it’s also a valuable resource for teachers and administrators. It can help find resources, develop lesson plans, and quickly answer any question. If you’re an educator looking to learn how to use Perplexity, check out our focused seven-day course, Perplexity for Educators.Perplexity for not-for-profits and public servants\\xa0Employees at nonprofits and government agencies are already turning to Perplexity for quick answers and real-time updates on trending news, but not all organizations can afford Enterprise Pro subscriptions. That’s why we’re reducing the cost of Enterprise Pro for not-for-profits and government agencies — so more teams can benefit from our research and data analysis capabilities, freeing up time and resources and allowing staff to focus on higher-value activities.Here are some ways existing Enterprise Pro customers, like the U.S. Anti-Doping Agency and the Montana Department of Natural Resources and Conservation, benefit from Perplexity:Conduct research on real-time policy updates and current events with no knowledge cutoffFind relevant statistics for marketing materials and grant proposalsGenerate translations, transcriptions, and summaries quickly\\xa0Create internal documents, including first drafts of new policies, job vacancy postings, and meeting agendasBuild initial project plans and work breakdownsAnalyze large datasets to gain valuable insights into operations, programs, and donor behaviorsVerify server and coding issue fixes quickly\\xa0Simplify the language of complex documents\"I use Perplexity daily to speed up my workflows around research and analysis, as well as getting past the initial blank page when drafting documents. Just providing a good starting point is often enough to save hours of time weekly. Having the sources of answers cited in-text is a huge benefit to verify the generated answer, and access to multiple AI models is icing on the cake.\" - Chris Powell, Chief Information Officer at the Montana Department of Natural Resources and Conservation\\xa0We’re so appreciative of the work nonprofits and government agencies do to address critical societal issues and are excited to see the value they gain from using Perplexity.Visit perplexity.ai/enterprise or reach out to enterprise@perplexity.ai to learn more about how to get started with Perplexity as a school, university, not-for-profit organization, or government agency.Bringing Perplexity to education and not-for-profits AI will be an integral part of our lives, influencing how we learn, consume information, and work. For AI to make the most positive impact, it needs to be accessible to all organizations, not just for-profit companies.Earlier this year, we introduced Perplexity Enterprise Pro to enable companies to leverage our AI with enhanced features like user management, increased data privacy, SOC2 compliance, and fortified security. Now, we’re excited to extend these benefits to philanthropic and public organizations by offering Perplexity Enterprise Pro at a reduced cost.For all schools, universities, nonprofits, government agencies, and other not-for-profit organizations, Enterprise Pro will be available for $20/seat per month or $200/seat annually.Perplexity for educationIncreasingly, we’re hearing from educators that they want to teach their students to use AI effectively and responsibly. As AI becomes more prevalent, people of all ages will need to understand how to utilize it with strong critical thinking and media literacy skills.As a knowledge-based platform, we want to make Perplexity Enterprise Pro available to students ages 13 and up while ensuring it is conducive to learning. Our commitment to citing sources and prioritizing accuracy can teach students valuable skills like researching and vetting information. By making Perplexity more affordable for schools, we hope to expand access to our technology and equip students with the knowledge they need to solve the problems of tomorrow.We’ve begun rolling out Perplexity Enterprise Pro to several schools, including ‘Iolani School in Hawaii. Over the next few months of summer school, teachers will learn how tools like Perplexity can support both their growth and their students’ development.\\xa0“‘Iolani is thrilled to be the first K-12 school to partner with Perplexity, providing our students and faculty with safe access to an advanced AI research and knowledge tool. This partnership is an exciting opportunity to develop case studies that demonstrate how AI can enhance learning, research, creativity, and productivity within an academic setting. We believe that integrating Perplexity into our educational framework will empower our community to explore innovative approaches to research and foster a deeper understanding of AI’s potential in various fields.” - Dr. Michael Lomuscio, Dean of Studies at ‘Iolani SchoolPerplexity isn’t just a tool for students—it’s also a valuable resource for teachers and administrators. It can help find resources, develop lesson plans, and quickly answer any question. If you’re an educator looking to learn how to use Perplexity, check out our focused seven-day course, Perplexity for Educators.Perplexity for not-for-profits and public servants\\xa0Employees at nonprofits and government agencies are already turning to Perplexity for quick answers and real-time updates on trending news, but not all organizations can afford Enterprise Pro subscriptions. That’s why we’re reducing the cost of Enterprise Pro for not-for-profits and government agencies — so more teams can benefit from our research and data analysis capabilities, freeing up time and resources and allowing staff to focus on higher-value activities.Here are some ways existing Enterprise Pro customers, like the U.S. Anti-Doping Agency and the Montana Department of Natural Resources and Conservation, benefit from Perplexity:Conduct research on real-time policy updates and current events with no knowledge cutoffFind relevant statistics for marketing materials and grant proposalsGenerate translations, transcriptions, and summaries quickly\\xa0Create internal documents, including first drafts of new policies, job vacancy postings, and meeting agendasBuild initial project plans and work breakdownsAnalyze large datasets to gain valuable insights into operations, programs, and donor behaviorsVerify server and coding issue fixes quickly\\xa0Simplify the language of complex documents\"I use Perplexity daily to speed up my workflows around research and analysis, as well as getting past the initial blank page when drafting documents. Just providing a good starting point is often enough to save hours of time weekly. Having the sources of answers cited in-text is a huge benefit to verify the generated answer, and access to multiple AI models is icing on the cake.\" - Chris Powell, Chief Information Officer at the Montana Department of Natural Resources and Conservation\\xa0We’re so appreciative of the work nonprofits and government agencies do to address critical societal issues and are excited to see the value they gain from using Perplexity.Visit perplexity.ai/enterprise or reach out to enterprise@perplexity.ai to learn more about how to get started with Perplexity as a school, university, not-for-profit organization, or government agency.'),\n",
       " Document(metadata={'source': 'https://www.perplexity.ai/hub/blog/perplexity-pages'}, page_content=\"Introducing Perplexity PagesYou’ve used Perplexity to search for answers, explore new topics, and expand your knowledge. Now, it’s time to share what you learned.Meet Perplexity Pages, your new tool for easily transforming research into visually stunning, comprehensive content. Whether you're crafting in-depth articles, detailed reports, or informative guides, Pages streamlines the process so you can focus on what matters most: sharing your knowledge with the world.Seamless creationPages lets you effortlessly create, organize, and share information. Search any topic, and instantly receive a well-structured, beautifully formatted article. Publish your work to our growing library of user-generated content and share it directly with your audience with a single click.What sets Perplexity Pages apart?Customizable: Tailor the tone of your Page to resonate with your target audience, whether you're writing for general readers or subject matter experts.Adaptable: Easily modify the structure of your article—add, rearrange, or remove sections to best suit your material and engage your readers.Visual: Elevate your articles with visuals generated by Pages, uploaded from your personal collection, or sourced online.A tool for everyonePages is designed to empower creators in any field to share knowledge.Educators: Develop comprehensive study guides for your students, breaking down complex topics into easily digestible content.Researchers: Create detailed reports on your findings, making your work more accessible to a wider audience.Hobbyists: Share your passions by creating engaging guides that inspire others to explore new interests.Featured Pages by the Perplexity TeamBeginner's Guide to Drumming by HenryHow to Use an AeroPress by PhiWriting Kubernetes CronJobs Guide by NikhilSteve Jobs: Visionary CEO by AbdulThe Definitive Guide to San Francisco Tennis Courts by RamanQuantum Machine Learning: The Next Leap in AI Technology by EliotTry Pages todayPages is rolling out to users now. Log in to your Perplexity account and select “Create a Page” in the library tab. Choose your topic, select your audience, and get started.With Perplexity Pages, you have the power to produce content that educates, inspires, and engages your audience.Let’s see what you create.Introducing Perplexity PagesYou’ve used Perplexity to search for answers, explore new topics, and expand your knowledge. Now, it’s time to share what you learned.Meet Perplexity Pages, your new tool for easily transforming research into visually stunning, comprehensive content. Whether you're crafting in-depth articles, detailed reports, or informative guides, Pages streamlines the process so you can focus on what matters most: sharing your knowledge with the world.Seamless creationPages lets you effortlessly create, organize, and share information. Search any topic, and instantly receive a well-structured, beautifully formatted article. Publish your work to our growing library of user-generated content and share it directly with your audience with a single click.What sets Perplexity Pages apart?Customizable: Tailor the tone of your Page to resonate with your target audience, whether you're writing for general readers or subject matter experts.Adaptable: Easily modify the structure of your article—add, rearrange, or remove sections to best suit your material and engage your readers.Visual: Elevate your articles with visuals generated by Pages, uploaded from your personal collection, or sourced online.A tool for everyonePages is designed to empower creators in any field to share knowledge.Educators: Develop comprehensive study guides for your students, breaking down complex topics into easily digestible content.Researchers: Create detailed reports on your findings, making your work more accessible to a wider audience.Hobbyists: Share your passions by creating engaging guides that inspire others to explore new interests.Featured Pages by the Perplexity TeamBeginner's Guide to Drumming by HenryHow to Use an AeroPress by PhiWriting Kubernetes CronJobs Guide by NikhilSteve Jobs: Visionary CEO by AbdulThe Definitive Guide to San Francisco Tennis Courts by RamanQuantum Machine Learning: The Next Leap in AI Technology by EliotTry Pages todayPages is rolling out to users now. Log in to your Perplexity account and select “Create a Page” in the library tab. Choose your topic, select your audience, and get started.With Perplexity Pages, you have the power to produce content that educates, inspires, and engages your audience.Let’s see what you create.Introducing Perplexity PagesYou’ve used Perplexity to search for answers, explore new topics, and expand your knowledge. Now, it’s time to share what you learned.Meet Perplexity Pages, your new tool for easily transforming research into visually stunning, comprehensive content. Whether you're crafting in-depth articles, detailed reports, or informative guides, Pages streamlines the process so you can focus on what matters most: sharing your knowledge with the world.Seamless creationPages lets you effortlessly create, organize, and share information. Search any topic, and instantly receive a well-structured, beautifully formatted article. Publish your work to our growing library of user-generated content and share it directly with your audience with a single click.What sets Perplexity Pages apart?Customizable: Tailor the tone of your Page to resonate with your target audience, whether you're writing for general readers or subject matter experts.Adaptable: Easily modify the structure of your article—add, rearrange, or remove sections to best suit your material and engage your readers.Visual: Elevate your articles with visuals generated by Pages, uploaded from your personal collection, or sourced online.A tool for everyonePages is designed to empower creators in any field to share knowledge.Educators: Develop comprehensive study guides for your students, breaking down complex topics into easily digestible content.Researchers: Create detailed reports on your findings, making your work more accessible to a wider audience.Hobbyists: Share your passions by creating engaging guides that inspire others to explore new interests.Featured Pages by the Perplexity TeamBeginner's Guide to Drumming by HenryHow to Use an AeroPress by PhiWriting Kubernetes CronJobs Guide by NikhilSteve Jobs: Visionary CEO by AbdulThe Definitive Guide to San Francisco Tennis Courts by RamanQuantum Machine Learning: The Next Leap in AI Technology by EliotTry Pages todayPages is rolling out to users now. Log in to your Perplexity account and select “Create a Page” in the library tab. Choose your topic, select your audience, and get started.With Perplexity Pages, you have the power to produce content that educates, inspires, and engages your audience.Let’s see what you create.\"),\n",
       " Document(metadata={'source': 'https://www.perplexity.ai/hub/blog/perplexity-launches-enterprise-pro'}, page_content='Perplexity launches Enterprise ProAnnounces $62.7M in funding and partnerships with SoftBank + Deutsche TelekomSince announcing our Series B funding in January 2024, Perplexity has continued to grow rapidly, now serving 169 million queries per month and cementing our position as the AI-native answer engine of choice.\\xa0Introducing Perplexity Enterprise ProTo satisfy the growing demand for our services, we\\'re introducing Perplexity Enterprise Pro, our first B2B offering that places security and control at the forefront. Millions already rely on our technology to make their research at work more accurate and efficient, and we\\'re excited to bring this power to companies to meet the demand for enterprise-scale.We’ve spent the last few months rolling out Enterprise Pro to select companies across a diverse range of industries including Stripe, Zoom, Bridgewater, Snowflake, the Cleveland Cavaliers, Universal McCann, Thrive Global, Databricks, Paytm, ElevenLabs, HP, Vercel, and Replit. It’s been incredible to witness the vast use cases across different teams and organizations – from supporting cancer researchers to giving developers quick answers to coding questions.\"Perplexity Enterprise Pro has allowed Databricks to substantially accelerate R&D, making it easier for our engineering, marketing, and sales teams to execute faster. We estimate it helps our team save 5k working hours monthly.\" - Ali Ghodsi, CEO of DatabricksPerplexity’s answer engine browses the internet in real time and provides complete, verifiable answers with citations, along with multimedia answers that include charts, videos, and images to provide more context. Instead of searching for an answer and having to browse through spammy websites full of affiliated links, Perplexity streamlines your employee’s workflow to help them save time.\\xa0Product teams at Zoom use Perplexity’s Focus functionality for targeted searchHP’s salesforce taps into Perplexity for rapid, in-depth prospect research, empowering them to craft compelling pitches and expedits the sales processInnovation Attorneys at Latham & Watkins are piloting Perplexity to conduct targeted researchHealth editorial teams at Thrive Global are creating validated behavior change microsteps based on the latest peer-reviewed science with PerplexityData teams at the Cleveland Cavaliers research ticket sales trends and do partnership prospecting\\xa0Strategy teams at Paytm draft market landscape insights to inform their roadmapsMarketing and product teams at Amplitude use Perplexity to draft market landscape insightsWith our paid offering, Perplexity Pro, we also power voice-to-text for easy prompting, unlimited file uploads to dig deeper into decks and documents, and unlimited search assisted queries to hone your questions and give you the best answer.\\xa0\\xa0With Enterprise Pro, our most robust offering yet, we add on even more functionality and features:\\xa0Increased Data Privacy: Your data stays yours, period. We never train our LLMs on our enterprise customers’ data.Enhanced Security: Get alerts about new file uploads to keep your documents in check.User Management: Add or remove team members easily.SOC2 Certification: A gold standard in security.Data Retention: Your queries stay private and are deleted after 7 days.Single Sign-On (SSO): Quick and secure access to your corporate systems.Our product changes how people work in ways that companies can’t even imagine yet – today is just the start of our expansion into transforming business.\\xa0Prices start at $40/month or $400/year per seat. Sign up todayFostering Global Expansion with Additional FundingTo support our ongoing consumer adoption and enterprise growth, we\\'ve raised an additional $62.7 million, doubling our total valuation to over $1 billion and our fundraising totals to $165 million.The new investment was led by Daniel Gross (former head of AI at Y Combinator) with participation from new investors Stanley Druckenmiller, Garry Tan (CEO of Y Combinator), Dylan Field (CEO of Figma), Brad Gerstner (Founder & CEO of Altimeter Capital), Laude Capital,\\xa0 Lip-Bu Tan (former CEO of Cadence), and Jakob Uszkoreit (co-inventor of Transformers). Many of our existing investors, including Jeff Bezos, NVIDIA, Tobi Lutke, Elad Gil, Nat Friedman, Naval Ravikant, Andrej Karpathy, IVP, and NEA, also doubled down on their support.With this funding, global expansion is a top priority. We’ve inked new partnerships with two of the world’s largest telecommunications firms — Japan’s SoftBank Corp. (TOKYO:9434) and Germany’s Deutsche Telekom — to market Perplexity\\'s capabilities to consumer and business customers. With a combined user base of more than 335M customers across mobile and broadband, these partnerships will significantly extend Perplexity’s reach.\\xa0Says Hiroyuki Terao, Executive Vice President, Head of Consumer Business Promotion Unit at SoftBank Corp.: “We are delighted to announce our alliance with Perplexity, leading innovators in the realm of AI-powered service. We have continuously transformed people\\'s lives through our philosophy: ‘Information Revolution -Happiness for everyone-’ and offering Perplexity to users in Japan aligns perfectly with this philosophy.\\xa0 This is merely the commencement of our journey. Through this collaboration, we aspire to embody our philosophy in the realm of generative AI.”\\xa0Says Jon Abrahamson, Chief Product & Digital Officer at Deutsche Telekom: “We are in the midst of a technology revolution that will reshape every aspect of our lives, and we at Deutsche Telekom want to assure that its benefits stay open and accessible to everyone. For this reason, we are thrilled at the opportunity to work with Perplexity as they seek to revolutionize search and access to information.”As global telecom leaders increasingly seek to bring AI tools to their customers, Perplexity is emerging as a partner of choice.\\xa0Everyone can benefit from Perplexity, and with these updates, we’re making our answer engine accessible to even more users: in workplaces with Enterprise Pro, and to customers in Japan and Germany with new telecommunications partnerships. A more curious society benefits us all, and we’re excited for all the partners joining our mission to make the world smarter.\\xa0Perplexity launches Enterprise ProAnnounces $62.7M in funding and partnerships with SoftBank + Deutsche TelekomSince announcing our Series B funding in January 2024, Perplexity has continued to grow rapidly, now serving 169 million queries per month and cementing our position as the AI-native answer engine of choice.\\xa0Introducing Perplexity Enterprise ProTo satisfy the growing demand for our services, we\\'re introducing Perplexity Enterprise Pro, our first B2B offering that places security and control at the forefront. Millions already rely on our technology to make their research at work more accurate and efficient, and we\\'re excited to bring this power to companies to meet the demand for enterprise-scale.We’ve spent the last few months rolling out Enterprise Pro to select companies across a diverse range of industries including Stripe, Zoom, Bridgewater, Snowflake, the Cleveland Cavaliers, Universal McCann, Thrive Global, Databricks, Paytm, ElevenLabs, HP, Vercel, and Replit. It’s been incredible to witness the vast use cases across different teams and organizations – from supporting cancer researchers to giving developers quick answers to coding questions.\"Perplexity Enterprise Pro has allowed Databricks to substantially accelerate R&D, making it easier for our engineering, marketing, and sales teams to execute faster. We estimate it helps our team save 5k working hours monthly.\" - Ali Ghodsi, CEO of DatabricksPerplexity’s answer engine browses the internet in real time and provides complete, verifiable answers with citations, along with multimedia answers that include charts, videos, and images to provide more context. Instead of searching for an answer and having to browse through spammy websites full of affiliated links, Perplexity streamlines your employee’s workflow to help them save time.\\xa0Product teams at Zoom use Perplexity’s Focus functionality for targeted searchHP’s salesforce taps into Perplexity for rapid, in-depth prospect research, empowering them to craft compelling pitches and expedits the sales processInnovation Attorneys at Latham & Watkins are piloting Perplexity to conduct targeted researchHealth editorial teams at Thrive Global are creating validated behavior change microsteps based on the latest peer-reviewed science with PerplexityData teams at the Cleveland Cavaliers research ticket sales trends and do partnership prospecting\\xa0Strategy teams at Paytm draft market landscape insights to inform their roadmapsMarketing and product teams at Amplitude use Perplexity to draft market landscape insightsWith our paid offering, Perplexity Pro, we also power voice-to-text for easy prompting, unlimited file uploads to dig deeper into decks and documents, and unlimited search assisted queries to hone your questions and give you the best answer.\\xa0\\xa0With Enterprise Pro, our most robust offering yet, we add on even more functionality and features:\\xa0Increased Data Privacy: Your data stays yours, period. We never train our LLMs on our enterprise customers’ data.Enhanced Security: Get alerts about new file uploads to keep your documents in check.User Management: Add or remove team members easily.SOC2 Certification: A gold standard in security.Data Retention: Your queries stay private and are deleted after 7 days.Single Sign-On (SSO): Quick and secure access to your corporate systems.Our product changes how people work in ways that companies can’t even imagine yet – today is just the start of our expansion into transforming business.\\xa0Prices start at $40/month or $400/year per seat. Sign up todayFostering Global Expansion with Additional FundingTo support our ongoing consumer adoption and enterprise growth, we\\'ve raised an additional $62.7 million, doubling our total valuation to over $1 billion and our fundraising totals to $165 million.The new investment was led by Daniel Gross (former head of AI at Y Combinator) with participation from new investors Stanley Druckenmiller, Garry Tan (CEO of Y Combinator), Dylan Field (CEO of Figma), Brad Gerstner (Founder & CEO of Altimeter Capital), Laude Capital,\\xa0 Lip-Bu Tan (former CEO of Cadence), and Jakob Uszkoreit (co-inventor of Transformers). Many of our existing investors, including Jeff Bezos, NVIDIA, Tobi Lutke, Elad Gil, Nat Friedman, Naval Ravikant, Andrej Karpathy, IVP, and NEA, also doubled down on their support.With this funding, global expansion is a top priority. We’ve inked new partnerships with two of the world’s largest telecommunications firms — Japan’s SoftBank Corp. (TOKYO:9434) and Germany’s Deutsche Telekom — to market Perplexity\\'s capabilities to consumer and business customers. With a combined user base of more than 335M customers across mobile and broadband, these partnerships will significantly extend Perplexity’s reach.\\xa0Says Hiroyuki Terao, Executive Vice President, Head of Consumer Business Promotion Unit at SoftBank Corp.: “We are delighted to announce our alliance with Perplexity, leading innovators in the realm of AI-powered service. We have continuously transformed people\\'s lives through our philosophy: ‘Information Revolution -Happiness for everyone-’ and offering Perplexity to users in Japan aligns perfectly with this philosophy.\\xa0 This is merely the commencement of our journey. Through this collaboration, we aspire to embody our philosophy in the realm of generative AI.”\\xa0Says Jon Abrahamson, Chief Product & Digital Officer at Deutsche Telekom: “We are in the midst of a technology revolution that will reshape every aspect of our lives, and we at Deutsche Telekom want to assure that its benefits stay open and accessible to everyone. For this reason, we are thrilled at the opportunity to work with Perplexity as they seek to revolutionize search and access to information.”As global telecom leaders increasingly seek to bring AI tools to their customers, Perplexity is emerging as a partner of choice.\\xa0Everyone can benefit from Perplexity, and with these updates, we’re making our answer engine accessible to even more users: in workplaces with Enterprise Pro, and to customers in Japan and Germany with new telecommunications partnerships. A more curious society benefits us all, and we’re excited for all the partners joining our mission to make the world smarter.\\xa0Perplexity launches Enterprise ProAnnounces $62.7M in funding and partnerships with SoftBank + Deutsche TelekomSince announcing our Series B funding in January 2024, Perplexity has continued to grow rapidly, now serving 169 million queries per month and cementing our position as the AI-native answer engine of choice.\\xa0Introducing Perplexity Enterprise ProTo satisfy the growing demand for our services, we\\'re introducing Perplexity Enterprise Pro, our first B2B offering that places security and control at the forefront. Millions already rely on our technology to make their research at work more accurate and efficient, and we\\'re excited to bring this power to companies to meet the demand for enterprise-scale.We’ve spent the last few months rolling out Enterprise Pro to select companies across a diverse range of industries including Stripe, Zoom, Bridgewater, Snowflake, the Cleveland Cavaliers, Universal McCann, Thrive Global, Databricks, Paytm, ElevenLabs, HP, Vercel, and Replit. It’s been incredible to witness the vast use cases across different teams and organizations – from supporting cancer researchers to giving developers quick answers to coding questions.\"Perplexity Enterprise Pro has allowed Databricks to substantially accelerate R&D, making it easier for our engineering, marketing, and sales teams to execute faster. We estimate it helps our team save 5k working hours monthly.\" - Ali Ghodsi, CEO of DatabricksPerplexity’s answer engine browses the internet in real time and provides complete, verifiable answers with citations, along with multimedia answers that include charts, videos, and images to provide more context. Instead of searching for an answer and having to browse through spammy websites full of affiliated links, Perplexity streamlines your employee’s workflow to help them save time.\\xa0Product teams at Zoom use Perplexity’s Focus functionality for targeted searchHP’s salesforce taps into Perplexity for rapid, in-depth prospect research, empowering them to craft compelling pitches and expedits the sales processInnovation Attorneys at Latham & Watkins are piloting Perplexity to conduct targeted researchHealth editorial teams at Thrive Global are creating validated behavior change microsteps based on the latest peer-reviewed science with PerplexityData teams at the Cleveland Cavaliers research ticket sales trends and do partnership prospecting\\xa0Strategy teams at Paytm draft market landscape insights to inform their roadmapsMarketing and product teams at Amplitude use Perplexity to draft market landscape insightsWith our paid offering, Perplexity Pro, we also power voice-to-text for easy prompting, unlimited file uploads to dig deeper into decks and documents, and unlimited search assisted queries to hone your questions and give you the best answer.\\xa0\\xa0With Enterprise Pro, our most robust offering yet, we add on even more functionality and features:\\xa0Increased Data Privacy: Your data stays yours, period. We never train our LLMs on our enterprise customers’ data.Enhanced Security: Get alerts about new file uploads to keep your documents in check.User Management: Add or remove team members easily.SOC2 Certification: A gold standard in security.Data Retention: Your queries stay private and are deleted after 7 days.Single Sign-On (SSO): Quick and secure access to your corporate systems.Our product changes how people work in ways that companies can’t even imagine yet – today is just the start of our expansion into transforming business.\\xa0Prices start at $40/month or $400/year per seat. Sign up todayFostering Global Expansion with Additional FundingTo support our ongoing consumer adoption and enterprise growth, we\\'ve raised an additional $62.7 million, doubling our total valuation to over $1 billion and our fundraising totals to $165 million.The new investment was led by Daniel Gross (former head of AI at Y Combinator) with participation from new investors Stanley Druckenmiller, Garry Tan (CEO of Y Combinator), Dylan Field (CEO of Figma), Brad Gerstner (Founder & CEO of Altimeter Capital), Laude Capital,\\xa0 Lip-Bu Tan (former CEO of Cadence), and Jakob Uszkoreit (co-inventor of Transformers). Many of our existing investors, including Jeff Bezos, NVIDIA, Tobi Lutke, Elad Gil, Nat Friedman, Naval Ravikant, Andrej Karpathy, IVP, and NEA, also doubled down on their support.With this funding, global expansion is a top priority. We’ve inked new partnerships with two of the world’s largest telecommunications firms — Japan’s SoftBank Corp. (TOKYO:9434) and Germany’s Deutsche Telekom — to market Perplexity\\'s capabilities to consumer and business customers. With a combined user base of more than 335M customers across mobile and broadband, these partnerships will significantly extend Perplexity’s reach.\\xa0Says Hiroyuki Terao, Executive Vice President, Head of Consumer Business Promotion Unit at SoftBank Corp.: “We are delighted to announce our alliance with Perplexity, leading innovators in the realm of AI-powered service. We have continuously transformed people\\'s lives through our philosophy: ‘Information Revolution -Happiness for everyone-’ and offering Perplexity to users in Japan aligns perfectly with this philosophy.\\xa0 This is merely the commencement of our journey. Through this collaboration, we aspire to embody our philosophy in the realm of generative AI.”\\xa0Says Jon Abrahamson, Chief Product & Digital Officer at Deutsche Telekom: “We are in the midst of a technology revolution that will reshape every aspect of our lives, and we at Deutsche Telekom want to assure that its benefits stay open and accessible to everyone. For this reason, we are thrilled at the opportunity to work with Perplexity as they seek to revolutionize search and access to information.”As global telecom leaders increasingly seek to bring AI tools to their customers, Perplexity is emerging as a partner of choice.\\xa0Everyone can benefit from Perplexity, and with these updates, we’re making our answer engine accessible to even more users: in workplaces with Enterprise Pro, and to customers in Japan and Germany with new telecommunications partnerships. A more curious society benefits us all, and we’re excited for all the partners joining our mission to make the world smarter.\\xa0'),\n",
       " Document(metadata={'source': 'https://www.perplexity.ai/hub/blog/perplexity-pro-is-coming-to-all-sk-telecom-users'}, page_content='Perplexity Pro is coming to all SK Telecom users We’re teaming up with the global telecommunications company and largest mobile network in Korea to expand access to 32.5 million new users. We have global ambitions to redefine search and are consistently looking for ways to bring our answer engine into the hands of more people. Today, as we kick off the first day of Mobile World Congress in Barcelona, we are excited to announce a partnership with SK Telecom (NYSE:SKM, “SKT”), one of the largest telecommunications operators and South Korea’s largest mobile carrier.\\xa0Soon, SKT’s 32M+ subscribers across South Korea will have a chance to experience Perplexity Pro at their fingertips, including unlimited guided Copilot searches and real-time information on trending events. Not only that, SKT will work with Perplexity to develop new AI tools leveraging our Online LLMs, which capture the most up-to-date information from the Internet to accurately respond to time-sensitive queries. This is all part of SKT’s expansion into new AI tools, building an AI agent powered by Perplexity’s Online LLM capabilities that the entire world can enjoy.\\xa0Says Aravind Srinivas, CEO of Perplexity: “Telecommunications carriers connect the world, and their networks make it possible for tools like Perplexity to spread knowledge on the web. Telecom partnerships are a core part of our global growth strategy, and I am excited to have SKT onboard as our first global partner in a first-of-its-kind partnership between a mobile carrier and an AI company.”\\xa0More than 90% of people in Korea use a smartphone, one of the highest adoption rates in the world. As some of the first movers with new technology, South Korea is poised to be at the forefront of a shift in how we gain access to knowledge online. Today, two companies dominate the local online search market, but SKT and Perplexity believe there’s an opportunity for a new player to provide a better user experience with more concise, direct answers.\\xa0Says Chung Suk-geun, Chief AI Global Officer at SKT: “We are united in a shared vision of using AI to redefine how we interact with the world, and we were immediately impressed with the speed, accuracy, and reliability of Perplexity’s answer engine. SKT is thrilled to be bringing Perpexity’s innovative capabilities to all users, creating an unparalleled experience for our customers. This is just the start of our collaboration, and we see lots of opportunities for this to not only impact the Korean market, but also Internet users globally.”Download Perplexity on Android and iOS today.\\xa0\\xa0Perplexity Pro is coming to all SK Telecom users We’re teaming up with the global telecommunications company and largest mobile network in Korea to expand access to 32.5 million new users. We have global ambitions to redefine search and are consistently looking for ways to bring our answer engine into the hands of more people. Today, as we kick off the first day of Mobile World Congress in Barcelona, we are excited to announce a partnership with SK Telecom (NYSE:SKM, “SKT”), one of the largest telecommunications operators and South Korea’s largest mobile carrier.\\xa0Soon, SKT’s 32M+ subscribers across South Korea will have a chance to experience Perplexity Pro at their fingertips, including unlimited guided Copilot searches and real-time information on trending events. Not only that, SKT will work with Perplexity to develop new AI tools leveraging our Online LLMs, which capture the most up-to-date information from the Internet to accurately respond to time-sensitive queries. This is all part of SKT’s expansion into new AI tools, building an AI agent powered by Perplexity’s Online LLM capabilities that the entire world can enjoy.\\xa0Says Aravind Srinivas, CEO of Perplexity: “Telecommunications carriers connect the world, and their networks make it possible for tools like Perplexity to spread knowledge on the web. Telecom partnerships are a core part of our global growth strategy, and I am excited to have SKT onboard as our first global partner in a first-of-its-kind partnership between a mobile carrier and an AI company.”\\xa0More than 90% of people in Korea use a smartphone, one of the highest adoption rates in the world. As some of the first movers with new technology, South Korea is poised to be at the forefront of a shift in how we gain access to knowledge online. Today, two companies dominate the local online search market, but SKT and Perplexity believe there’s an opportunity for a new player to provide a better user experience with more concise, direct answers.\\xa0Says Chung Suk-geun, Chief AI Global Officer at SKT: “We are united in a shared vision of using AI to redefine how we interact with the world, and we were immediately impressed with the speed, accuracy, and reliability of Perplexity’s answer engine. SKT is thrilled to be bringing Perpexity’s innovative capabilities to all users, creating an unparalleled experience for our customers. This is just the start of our collaboration, and we see lots of opportunities for this to not only impact the Korean market, but also Internet users globally.”Download Perplexity on Android and iOS today.\\xa0\\xa0Perplexity Pro is coming to all SK Telecom users We’re teaming up with the global telecommunications company and largest mobile network in Korea to expand access to 32.5 million new users. We have global ambitions to redefine search and are consistently looking for ways to bring our answer engine into the hands of more people. Today, as we kick off the first day of Mobile World Congress in Barcelona, we are excited to announce a partnership with SK Telecom (NYSE:SKM, “SKT”), one of the largest telecommunications operators and South Korea’s largest mobile carrier.\\xa0Soon, SKT’s 32M+ subscribers across South Korea will have a chance to experience Perplexity Pro at their fingertips, including unlimited guided Copilot searches and real-time information on trending events. Not only that, SKT will work with Perplexity to develop new AI tools leveraging our Online LLMs, which capture the most up-to-date information from the Internet to accurately respond to time-sensitive queries. This is all part of SKT’s expansion into new AI tools, building an AI agent powered by Perplexity’s Online LLM capabilities that the entire world can enjoy.\\xa0Says Aravind Srinivas, CEO of Perplexity: “Telecommunications carriers connect the world, and their networks make it possible for tools like Perplexity to spread knowledge on the web. Telecom partnerships are a core part of our global growth strategy, and I am excited to have SKT onboard as our first global partner in a first-of-its-kind partnership between a mobile carrier and an AI company.”\\xa0More than 90% of people in Korea use a smartphone, one of the highest adoption rates in the world. As some of the first movers with new technology, South Korea is poised to be at the forefront of a shift in how we gain access to knowledge online. Today, two companies dominate the local online search market, but SKT and Perplexity believe there’s an opportunity for a new player to provide a better user experience with more concise, direct answers.\\xa0Says Chung Suk-geun, Chief AI Global Officer at SKT: “We are united in a shared vision of using AI to redefine how we interact with the world, and we were immediately impressed with the speed, accuracy, and reliability of Perplexity’s answer engine. SKT is thrilled to be bringing Perpexity’s innovative capabilities to all users, creating an unparalleled experience for our customers. This is just the start of our collaboration, and we see lots of opportunities for this to not only impact the Korean market, but also Internet users globally.”Download Perplexity on Android and iOS today.\\xa0\\xa0'),\n",
       " Document(metadata={'source': 'https://www.perplexity.ai/hub/blog/perplexity-partners-with-elevenlabs-to-launch-discover-daily-podcast'}, page_content='Perplexity Partners with ElevenLabs to launch \\'Discover Daily\\' PodcastBringing cutting-edge knowledge to your ears.Perplexity is thrilled to announce our partnership with ElevenLabs, a pioneering voice technology company. Together, we have launched \"Discover Daily,\" a short-form podcast that brings the latest headlines in innovation, science, and culture to listeners everywhere. This collaboration marries ElevenLabs\\' state-of-the-art voice technology with Perplexity\\'s powerful search and content engine, offering a unique and accessible way to stay informed about the world\\'s most exciting developments.At Perplexity, we pride ourselves on being the fastest and most accurate way to search the web. Our platform curates relevant sources, from academic research to Reddit threads, to create comprehensive answers that go beyond the typical search engine experience. With in-line citations for every source we use, we provide trusted answers with the added benefit of easy fact-checking.\"Discover Daily\" is a testament to our commitment to making knowledge more accessible and engaging. By leveraging ElevenLabs\\' lifelike voice technology, we\\'re able to transform the way people consume information, making it possible to absorb curated knowledge in audio form—perfect for those on the go or simply looking for a more dynamic way to learn something new.Subscribe to \"Discover Daily\" and experience the blend of advanced voice technology and cutting-edge search capabilities. This partnership is just the beginning of our journey to change information discovery and sharing, and we couldn\\'t be more excited about the future possibilities this partnership brings.Subscribe on Apple Podcast, Spotify, YouTube, or on your favorite podcast platform.Perplexity Partners with ElevenLabs to launch \\'Discover Daily\\' PodcastBringing cutting-edge knowledge to your ears.Perplexity is thrilled to announce our partnership with ElevenLabs, a pioneering voice technology company. Together, we have launched \"Discover Daily,\" a short-form podcast that brings the latest headlines in innovation, science, and culture to listeners everywhere. This collaboration marries ElevenLabs\\' state-of-the-art voice technology with Perplexity\\'s powerful search and content engine, offering a unique and accessible way to stay informed about the world\\'s most exciting developments.At Perplexity, we pride ourselves on being the fastest and most accurate way to search the web. Our platform curates relevant sources, from academic research to Reddit threads, to create comprehensive answers that go beyond the typical search engine experience. With in-line citations for every source we use, we provide trusted answers with the added benefit of easy fact-checking.\"Discover Daily\" is a testament to our commitment to making knowledge more accessible and engaging. By leveraging ElevenLabs\\' lifelike voice technology, we\\'re able to transform the way people consume information, making it possible to absorb curated knowledge in audio form—perfect for those on the go or simply looking for a more dynamic way to learn something new.Subscribe to \"Discover Daily\" and experience the blend of advanced voice technology and cutting-edge search capabilities. This partnership is just the beginning of our journey to change information discovery and sharing, and we couldn\\'t be more excited about the future possibilities this partnership brings.Subscribe on Apple Podcast, Spotify, YouTube, or on your favorite podcast platform.Perplexity Partners with ElevenLabs to launch \\'Discover Daily\\' PodcastBringing cutting-edge knowledge to your ears.Perplexity is thrilled to announce our partnership with ElevenLabs, a pioneering voice technology company. Together, we have launched \"Discover Daily,\" a short-form podcast that brings the latest headlines in innovation, science, and culture to listeners everywhere. This collaboration marries ElevenLabs\\' state-of-the-art voice technology with Perplexity\\'s powerful search and content engine, offering a unique and accessible way to stay informed about the world\\'s most exciting developments.At Perplexity, we pride ourselves on being the fastest and most accurate way to search the web. Our platform curates relevant sources, from academic research to Reddit threads, to create comprehensive answers that go beyond the typical search engine experience. With in-line citations for every source we use, we provide trusted answers with the added benefit of easy fact-checking.\"Discover Daily\" is a testament to our commitment to making knowledge more accessible and engaging. By leveraging ElevenLabs\\' lifelike voice technology, we\\'re able to transform the way people consume information, making it possible to absorb curated knowledge in audio form—perfect for those on the go or simply looking for a more dynamic way to learn something new.Subscribe to \"Discover Daily\" and experience the blend of advanced voice technology and cutting-edge search capabilities. This partnership is just the beginning of our journey to change information discovery and sharing, and we couldn\\'t be more excited about the future possibilities this partnership brings.Subscribe on Apple Podcast, Spotify, YouTube, or on your favorite podcast platform.'),\n",
       " Document(metadata={'source': 'https://www.perplexity.ai/hub/blog/arc-x-perplexity'}, page_content='Perplexity now available in Arc BrowserWe are excited to announce that Perplexity is now integrated into Arc Browser as a default search engine option, allowing users to access the power of AI-powered search. With this integration, Arc users can now enjoy our answer engine, getting precise answers in real time without clutter or information overload. Perplexity leverages models like GPT-4, Claude 2.1, and Gemini Pro to provide a conversational search experience that evolves alongside AI innovations.By combining Arc\\'s minimalist interface and Perplexity\\'s intelligent search, we have created a streamlined browsing experience. No more sifting through irrelevant results or dead-end links - just fast, straightforward access to the information you need.Perplexity and Arc Browser both aim to enhance how people interact with the internet. We couldn\\'t ask for a better partner than Arc to help achieve our goal of accessible, human-centered AI search. In Arc founder Josh Miller\\'s words: \"AI Search is the next frontier & it will be distributed via the Browser too. It’s our chance to start anew. Let\\'s do it!\" We will keep refining our search technology to deliver the most intuitive, seamless user experience possible. Welcome to the new internet.Perplexity now available in Arc BrowserWe are excited to announce that Perplexity is now integrated into Arc Browser as a default search engine option, allowing users to access the power of AI-powered search. With this integration, Arc users can now enjoy our answer engine, getting precise answers in real time without clutter or information overload. Perplexity leverages models like GPT-4, Claude 2.1, and Gemini Pro to provide a conversational search experience that evolves alongside AI innovations.By combining Arc\\'s minimalist interface and Perplexity\\'s intelligent search, we have created a streamlined browsing experience. No more sifting through irrelevant results or dead-end links - just fast, straightforward access to the information you need.Perplexity and Arc Browser both aim to enhance how people interact with the internet. We couldn\\'t ask for a better partner than Arc to help achieve our goal of accessible, human-centered AI search. In Arc founder Josh Miller\\'s words: \"AI Search is the next frontier & it will be distributed via the Browser too. It’s our chance to start anew. Let\\'s do it!\" We will keep refining our search technology to deliver the most intuitive, seamless user experience possible. Welcome to the new internet.Perplexity now available in Arc BrowserWe are excited to announce that Perplexity is now integrated into Arc Browser as a default search engine option, allowing users to access the power of AI-powered search. With this integration, Arc users can now enjoy our answer engine, getting precise answers in real time without clutter or information overload. Perplexity leverages models like GPT-4, Claude 2.1, and Gemini Pro to provide a conversational search experience that evolves alongside AI innovations.By combining Arc\\'s minimalist interface and Perplexity\\'s intelligent search, we have created a streamlined browsing experience. No more sifting through irrelevant results or dead-end links - just fast, straightforward access to the information you need.Perplexity and Arc Browser both aim to enhance how people interact with the internet. We couldn\\'t ask for a better partner than Arc to help achieve our goal of accessible, human-centered AI search. In Arc founder Josh Miller\\'s words: \"AI Search is the next frontier & it will be distributed via the Browser too. It’s our chance to start anew. Let\\'s do it!\" We will keep refining our search technology to deliver the most intuitive, seamless user experience possible. Welcome to the new internet.'),\n",
       " Document(metadata={'source': 'https://www.perplexity.ai/hub/blog/perplexity-raises-series-b-funding-round'}, page_content='AI is fundamentally changing how people gain information and satisfy their quest for knowledge. Today, we’re announcing milestones reinforcing Perplexity’s increasingly dominant role in this new era of AI-native search. Since publicly launching the world’s first fully functional conversational answer engine a year ago, we’ve grown to 10 million monthly active users and have served over half a billion queries in 2023. More than a million people have installed our mobile apps, both on iOS and Android. But our ambition is to serve the entire planet’s unbounded curiosity, and we’re just getting started.To support our rapid consumer adoption and expansion plans, we’ve raised $73.6 million in Series B funding from trusted VC firms and prominent tech visionaries. IVP led the round with continued support from our Seed and Series A investors NEA, Elad Gil, Nat Friedman, and Databricks, as well as new investors NVIDIA, Jeff Bezos (through Bezos Expeditions Fund), Tobi Lutke, Bessemer Venture Partners, Naval Ravikant, Balaji Srinivasan, Guillermo Rauch, Austen Allred, Factorial Funds, and Kindred Ventures, among others. Building upon our\\xa0Series A from last year, we’ve now raised $100 million to date.\"Perplexity is intensely building a product capable of bringing the power of AI to billions. The team possesses the unique ability to uphold a grand, long-term vision while shipping product relentlessly, requirements to tackle a problem as important and fundamental as search.” — Cack Wilhelm, General Partner, IVP, who led the round, and joins Perplexity’s Board of Directors.“AI is transforming the way consumers access information”, said Jonathan Cohen, VP of Applied Research at NVIDIA. “Perplexity’s world-class team is building a trusted AI-powered search platform that will help push this transformation forward.”With Perplexity’s search tools, users get instant, reliable answers to any question with complete sources and citations included. There is no need to click on different links, compare answers, or endlessly dig for information. In an era where misinformation and AI hallucinations are causing increasing concern, we’re built on the idea that accuracy and transparency are prerequisites to making AI-powered search ubiquitous. The times of sifting through SEO spam, sponsored links, and multiple web pages will be replaced by a much more efficient way to consume and share information, propelling our society into a new era of accelerated learning and research.Since our previous funding round, we\\'ve\\xa0launched Copilot, a first-of-its-kind AI research assistant that has changed how we uncover information and learn more about new topics. Copilot tailors search queries with custom follow-up questions, introducing the concept of generative user interfaces. It removes the burden of prompt engineering and does not require users to ask perfectly phrased questions to get the answers they seek. This enables users to gain more relevant and comprehensive answers than other AI chatbots, traditional search engines, or research tools. Copilot has seen strong traction, especially among academics, students, and knowledge workers who rely on frequent research for their day-to-day work and needs.It’s not just about our technology but also how people use it. We stand at the inflection point of a massive behavioral shift in how people access information online. I am excited about the potential for Perplexity to enhance our curiosity and intelligence by providing streamlined access to reliable information. We are privileged to have the support of leading investors, technologists, and, most importantly, the millions of users who have trusted us with their time and support as we fulfill this grand mission of making the planet smarter by continuing to build the fastest and most accurate platform for answering any question.Aravind SrinivasCo-founder & CEO of PerplexityDirect media inquiries to press@perplexity.aiAI is fundamentally changing how people gain information and satisfy their quest for knowledge. Today, we’re announcing milestones reinforcing Perplexity’s increasingly dominant role in this new era of AI-native search. Since publicly launching the world’s first fully functional conversational answer engine a year ago, we’ve grown to 10 million monthly active users and have served over half a billion queries in 2023. More than a million people have installed our mobile apps, both on iOS and Android. But our ambition is to serve the entire planet’s unbounded curiosity, and we’re just getting started.To support our rapid consumer adoption and expansion plans, we’ve raised $73.6 million in Series B funding from trusted VC firms and prominent tech visionaries. IVP led the round with continued support from our Seed and Series A investors NEA, Elad Gil, Nat Friedman, and Databricks, as well as new investors NVIDIA, Jeff Bezos (through Bezos Expeditions Fund), Tobi Lutke, Bessemer Venture Partners, Naval Ravikant, Balaji Srinivasan, Guillermo Rauch, Austen Allred, Factorial Funds, and Kindred Ventures, among others. Building upon our\\xa0Series A from last year, we’ve now raised $100 million to date.\"Perplexity is intensely building a product capable of bringing the power of AI to billions. The team possesses the unique ability to uphold a grand, long-term vision while shipping product relentlessly, requirements to tackle a problem as important and fundamental as search.” — Cack Wilhelm, General Partner, IVP, who led the round, and joins Perplexity’s Board of Directors.“AI is transforming the way consumers access information”, said Jonathan Cohen, VP of Applied Research at NVIDIA. “Perplexity’s world-class team is building a trusted AI-powered search platform that will help push this transformation forward.”With Perplexity’s search tools, users get instant, reliable answers to any question with complete sources and citations included. There is no need to click on different links, compare answers, or endlessly dig for information. In an era where misinformation and AI hallucinations are causing increasing concern, we’re built on the idea that accuracy and transparency are prerequisites to making AI-powered search ubiquitous. The times of sifting through SEO spam, sponsored links, and multiple web pages will be replaced by a much more efficient way to consume and share information, propelling our society into a new era of accelerated learning and research.Since our previous funding round, we\\'ve\\xa0launched Copilot, a first-of-its-kind AI research assistant that has changed how we uncover information and learn more about new topics. Copilot tailors search queries with custom follow-up questions, introducing the concept of generative user interfaces. It removes the burden of prompt engineering and does not require users to ask perfectly phrased questions to get the answers they seek. This enables users to gain more relevant and comprehensive answers than other AI chatbots, traditional search engines, or research tools. Copilot has seen strong traction, especially among academics, students, and knowledge workers who rely on frequent research for their day-to-day work and needs.It’s not just about our technology but also how people use it. We stand at the inflection point of a massive behavioral shift in how people access information online. I am excited about the potential for Perplexity to enhance our curiosity and intelligence by providing streamlined access to reliable information. We are privileged to have the support of leading investors, technologists, and, most importantly, the millions of users who have trusted us with their time and support as we fulfill this grand mission of making the planet smarter by continuing to build the fastest and most accurate platform for answering any question.Aravind SrinivasCo-founder & CEO of PerplexityDirect media inquiries to press@perplexity.aiAI is fundamentally changing how people gain information and satisfy their quest for knowledge. Today, we’re announcing milestones reinforcing Perplexity’s increasingly dominant role in this new era of AI-native search. Since publicly launching the world’s first fully functional conversational answer engine a year ago, we’ve grown to 10 million monthly active users and have served over half a billion queries in 2023. More than a million people have installed our mobile apps, both on iOS and Android. But our ambition is to serve the entire planet’s unbounded curiosity, and we’re just getting started.To support our rapid consumer adoption and expansion plans, we’ve raised $73.6 million in Series B funding from trusted VC firms and prominent tech visionaries. IVP led the round with continued support from our Seed and Series A investors NEA, Elad Gil, Nat Friedman, and Databricks, as well as new investors NVIDIA, Jeff Bezos (through Bezos Expeditions Fund), Tobi Lutke, Bessemer Venture Partners, Naval Ravikant, Balaji Srinivasan, Guillermo Rauch, Austen Allred, Factorial Funds, and Kindred Ventures, among others. Building upon our\\xa0Series A from last year, we’ve now raised $100 million to date.\"Perplexity is intensely building a product capable of bringing the power of AI to billions. The team possesses the unique ability to uphold a grand, long-term vision while shipping product relentlessly, requirements to tackle a problem as important and fundamental as search.” — Cack Wilhelm, General Partner, IVP, who led the round, and joins Perplexity’s Board of Directors.“AI is transforming the way consumers access information”, said Jonathan Cohen, VP of Applied Research at NVIDIA. “Perplexity’s world-class team is building a trusted AI-powered search platform that will help push this transformation forward.”With Perplexity’s search tools, users get instant, reliable answers to any question with complete sources and citations included. There is no need to click on different links, compare answers, or endlessly dig for information. In an era where misinformation and AI hallucinations are causing increasing concern, we’re built on the idea that accuracy and transparency are prerequisites to making AI-powered search ubiquitous. The times of sifting through SEO spam, sponsored links, and multiple web pages will be replaced by a much more efficient way to consume and share information, propelling our society into a new era of accelerated learning and research.Since our previous funding round, we\\'ve\\xa0launched Copilot, a first-of-its-kind AI research assistant that has changed how we uncover information and learn more about new topics. Copilot tailors search queries with custom follow-up questions, introducing the concept of generative user interfaces. It removes the burden of prompt engineering and does not require users to ask perfectly phrased questions to get the answers they seek. This enables users to gain more relevant and comprehensive answers than other AI chatbots, traditional search engines, or research tools. Copilot has seen strong traction, especially among academics, students, and knowledge workers who rely on frequent research for their day-to-day work and needs.It’s not just about our technology but also how people use it. We stand at the inflection point of a massive behavioral shift in how people access information online. I am excited about the potential for Perplexity to enhance our curiosity and intelligence by providing streamlined access to reliable information. We are privileged to have the support of leading investors, technologists, and, most importantly, the millions of users who have trusted us with their time and support as we fulfill this grand mission of making the planet smarter by continuing to build the fastest and most accurate platform for answering any question.Aravind SrinivasCo-founder & CEO of PerplexityDirect media inquiries to press@perplexity.ai'),\n",
       " Document(metadata={'source': 'https://www.perplexity.ai/hub/blog/introducing-pplx-online-llms'}, page_content=\"Introducing PPLX Online LLMs The first-of-its-kind Online LLM API.We’re excited to share two new PPLX models: pplx-7b-online and pplx-70b-online! Our online models are focused on delivering helpful, up-to-date, and factual responses, and are publicly available via pplx-api, making it a first-of-its-kind API. pplx-7b-online and pplx-70b-online are also accessible via Perplexity Labs, our LLM playground.LLMs have transformed the way we find information. However, there are two limitations with most LLMs today:Freshness: LLMs often struggle to share up-to-date information.Hallucinations: LLMs can also output inaccurate statements.Our pplx-7b-online and pplx-70b-online models address current limitations by additionally providing helpful, factual, and up-to-date information in its responses.PPLX Online LLMsOur LLMs, pplx-7b-online and pplx-70b-online, are online LLMs because they can use knowledge from the internet, and thus can leverage the most up-to-date information when forming a response. By providing our LLMs with knowledge from the web, our models accurately respond to time sensitive queries, unlocking knowledge beyond its training corpus. This means Perplexity’s online LLMs can answer queries like “What was the Warriors game score last night?” that are challenging for offline models. At a high level, this is how our online LLM works:Leverage open sourced models: our PPLX models build on top of mistral-7b and llama2-70b base models.In-house search technology: our in-house search, indexing, and crawling infrastructure allows us to augment LLMs with the most relevant, up to date, and valuable information. Our search index is large, updated on a regular cadence, and uses sophisticated ranking algorithms to ensure high quality, non-SEOed sites are prioritized.\\xa0Website excerpts, which we call “snippets”, are provided to our pplx-online models to enable responses with the most up-to-date information.Fine-tuning: our PPLX models have been fine-tuned to effectively use snippets to inform their responses. Using our in-house data contractors, we carefully curate high quality, diverse, and large training sets in order to achieve high performance on various axes like helpfulness, factuality, and freshness. Our models are regularly fine-tuned to continually improve performance.Evaluating Perplexity’s Online LLMsPerplexity’s mission is to build the world’s best answer engine - one that people trust to discover and expand their knowledge. To achieve this, we are deeply focused on providing helpful, factual, and up-to-date information. To benchmark our LLMs’ performance on these axes, we curated evaluation datasets to reflect challenging yet realistic use cases for answer engines. For each query in each evaluation set, contractors were given two model responses and instructed to select the response that performed better for the following criteria:Helpfulness: which response answers the query and follow the specified instructions better?Factuality: which response provides more accurate answers without hallucinations, even for questions that require very precise or niche knowledge?Freshness (inspired by FreshLLMs): which response contains more up-to-date information? A model excels in this criterion if it is able to answer queries with “fresh” information.In addition to the three criteria above, model responses were also evaluated holistically. To evaluate responses holistically, evaluators were asked to pick the response they would rather receive from an human assistant who is helping with the query.Curating the Evaluation SetFor this evaluation, we carefully curated a diverse set of prompts with the goal of effectively evaluating helpfulness, factuality, and freshness. Each prompt was manually selected, ensuring a high level of control over the quality and relevance of the data. The dataset spans a wide range of answer engine prompts and encompasses a comprehensive overview of what we want Perplexity to excel at. This is critical for us to our model performance evaluations to have high signal.Each of the three evaluation sets contains 50 prompts. Some examples from these sets include:Helpfulness: Create a table of all the USA soccer players that played in the World Cup finals and make columns for their stats such as goals, assists etc.Factuality: Explain the toughness of AISI 1015 and AISI 1040 steels.Up-to-date-ness: Which autonomous car company got banned from San Francisco in 2023?Generating Model ResponsesWe evaluated four models:pplx-7b-online: Perplexity’s model, which includes access to information from the internet. This model was finetuned using mistral-7b.pplx-70b-online: Perplexity’s model, which includes access to information from the internet. This model was finetuned using llama2-70b. gpt-3.5-turbo-1106: OpenAI’s model, accessed via the API and with no additional augmentations. Note that we conducted this evaluation using the latest gpt-3.5 model.llama2-70b-chat: Meta AI’s model, accessed via our pplx-api and with no additional augmentations.For each prompt, the same search results and snippets were provided to the pplx-7b-online and pplx-70b-online models. All responses were generated using the same hyperparameters and system prompt.System PromptCurrent date: Monday, November 20, 2023.Ranking Model Responses With Human EvaluationFor each pairwise comparison, our in-house contractors were provided the prompt itself, the evaluation criteria (i.e., helpfulness, factuality, or freshness), and the model responses displayed side-by-side. The ordering of the responses were randomized at each turn, and the source models were not revealed to the evaluator. Evaluators were instructed to select the response they holistically prefer, as well as which performs better on the specific evaluation criterion, with ties allowed for both. Finally, evaluators were allowed to use internet search to verify the accuracy of responses. We built our own in-house preference ranking tool to conduct this evaluation.Evaluation Results We use the pairwise preference rankings collected from human evaluation to calculate per-task Elo scores for each model. Elo scores are commonly used to rank a population of players in tournament-style competitions to measure the relative performance of the players. When applied to large language models, these scores provide a prediction of how likely a human evaluator might favor the output of one model over another.While Elo scores are typically calculated for a sequence of comparisons to account for changes in players’ abilities, we aim to quantify the performance of models whose abilities cannot change. Accordingly, we adopt the Bootstrap Elo methodology described in Duan et al and also utilized by lmsys for their Chatbot Arena, which entails calculating Elo scores for many random permutations of the comparisons. For our calculations, we calculated Elo scores over 5000 permutations and used the distributions of these Elo scores to calculate 95% confidence intervals.The results, shown in Figure 1, demonstrate that our PPLX models can match and even surpass gpt-3.5 performance on Perplexity-related use cases. In particular, pplx-7b-online and pplx-70b-online model responses are preferred over gpt-3.5 and llama2-70b model responses by human evaluators for their accurate and up-to-date answers.The pplx-7b-online and pplx-70b-online perform better than gpt-3.5 and llama2-70b, on the freshness, factuality, and holistic criteria.Figure 1. Estimated Elo scores and 95% confidence intervals for pplx-7b (-online), pplx-70b (-online), llama2-70b, and gpt-3.5 across four different evaluation sets. For example, on the freshness axis, pplx-7b and pplx-70b perform better than gpt-3.5 and llama2-70b, with an estimated Elo score of 1100.6 and 1099.6 vs 879.3 and 920.3, respectively.We also calculated the pairwise win rates from the evaluation. The results, shown in Figure 2, illustrate perhaps not surprisingly, the pplx online model responses are preferred to others’ on freshness and on factuality.Figure 2. Win rates of models on the vertical axis against the models on the horizontal axis. For example, pplx-70b (-online) responses were rated as holistically superior to gpt-3.5’s on 64% of queries and gpt-3.5’s responses were rated as holistically superior to pplx-70b (-online) on 24% of queries.Here is an example from our freshness evaluation dataset.Prompt: How many people in USA have a driving license?pplx-7b-onlineIn 2023, there are a projected 243.4 million licensed drivers in the United States, which includes 89% of US adults. This number has increased from 238.6 million in 2022. The percentage of licensed drivers varies across different age groups and states. For example, in 2019, 84.1% of the US population had a driver's license. The highest percentage of licensed drivers is among 25 to 49-year-olds, with 90.1% of the population having a driver's license. In California, there were 27 million licensed drivers, making it the state with the highest number of licensed drivers.pplx-70b-onlineIn 2021, around 233 million people held valid driving licenses in the United States. By 2023, the number of licensed drivers in the US is projected to reach 243.4 million, which includes 89% of US adults. This represents an increase from 238.6 million in 2022.gpt-3.5As an AI language model, I don't have access to real-time data. However, as of 2021, it was estimated that around 227 million people in the United States held a valid driver's license. Please note that this number may have changed since then, so it's always best to refer to the latest official statistics for the most accurate information.llama2-70bI don't have access to real-time data, but I can provide some information on the approximate number of people in the United States who have a driver's license. According to the Federal Highway Administration, there were approximately 227.8 million licensed drivers in the United States as of 2020. This number includes both commercial and non-commercial drivers. It's worth noting that this number is likely an underestimate, as it doesn't account for individuals who may have a driver's license but do not drive regularly or at all. Additionally, this number does not include individuals who are not eligible for a driver's license, such as those who are under the age of 16 or have certain disabilities. It's also important to note that the number of licensed drivers can vary by state, with some states having a higher percentage of licensed drivers than others.Overall, the evaluation results demonstrate that our PPLX models can match and even outperform gpt-3.5 and llama2-70b on Perplexity-related use cases, particularly for providing accurate and up-to-date responses.Accessing Perplexity’s Online ModelsWe are thrilled to announce the pplx-api is phasing out of beta into general public release! This is the first time our pplx-7b-online and pplx-70b-online models are accessible via the pplx-api. Additionally, our pplx-7b-chat and pplx-70b-chat models are out of alpha, and are now accessible with our general release.With this, we are introducing a new usage-based pricing structure. We’re excited for our users to utilize our state-of-the-art infrastructure, which utilizes NVIDIA H100s to provide blazing fast inference. Pro users will receive a recurring $5 monthly pplx-api credit. For all other users, pricing will be determined based on usage. To sign up as a Pro user, click here. Reach out to api@perplexity.ai for commercial inquiries.Additional Resources:Get started with pplx-api here. Try out our online models for free with Perplexity Labs.Learn more about our API via the pplx-api documentation.Interested in working on a high-impact, high-velocity team building the best answer engine? Join us!Join our growing Discord Community!ContributorsLauren Yang, Kevin Hu, Aarash Heydari, Gradey Wang, Dmitry Pervukhin, Nikhil Thota, Alexandr Yarats, Max Morozov, Denis YaratsIntroducing PPLX Online LLMs The first-of-its-kind Online LLM API.We’re excited to share two new PPLX models: pplx-7b-online and pplx-70b-online! Our online models are focused on delivering helpful, up-to-date, and factual responses, and are publicly available via pplx-api, making it a first-of-its-kind API. pplx-7b-online and pplx-70b-online are also accessible via Perplexity Labs, our LLM playground.LLMs have transformed the way we find information. However, there are two limitations with most LLMs today:Freshness: LLMs often struggle to share up-to-date information.Hallucinations: LLMs can also output inaccurate statements.Our pplx-7b-online and pplx-70b-online models address current limitations by additionally providing helpful, factual, and up-to-date information in its responses.PPLX Online LLMsOur LLMs, pplx-7b-online and pplx-70b-online, are online LLMs because they can use knowledge from the internet, and thus can leverage the most up-to-date information when forming a response. By providing our LLMs with knowledge from the web, our models accurately respond to time sensitive queries, unlocking knowledge beyond its training corpus. This means Perplexity’s online LLMs can answer queries like “What was the Warriors game score last night?” that are challenging for offline models. At a high level, this is how our online LLM works:Leverage open sourced models: our PPLX models build on top of mistral-7b and llama2-70b base models.In-house search technology: our in-house search, indexing, and crawling infrastructure allows us to augment LLMs with the most relevant, up to date, and valuable information. Our search index is large, updated on a regular cadence, and uses sophisticated ranking algorithms to ensure high quality, non-SEOed sites are prioritized.\\xa0Website excerpts, which we call “snippets”, are provided to our pplx-online models to enable responses with the most up-to-date information.Fine-tuning: our PPLX models have been fine-tuned to effectively use snippets to inform their responses. Using our in-house data contractors, we carefully curate high quality, diverse, and large training sets in order to achieve high performance on various axes like helpfulness, factuality, and freshness. Our models are regularly fine-tuned to continually improve performance.Evaluating Perplexity’s Online LLMsPerplexity’s mission is to build the world’s best answer engine - one that people trust to discover and expand their knowledge. To achieve this, we are deeply focused on providing helpful, factual, and up-to-date information. To benchmark our LLMs’ performance on these axes, we curated evaluation datasets to reflect challenging yet realistic use cases for answer engines. For each query in each evaluation set, contractors were given two model responses and instructed to select the response that performed better for the following criteria:Helpfulness: which response answers the query and follow the specified instructions better?Factuality: which response provides more accurate answers without hallucinations, even for questions that require very precise or niche knowledge?Freshness (inspired by FreshLLMs): which response contains more up-to-date information? A model excels in this criterion if it is able to answer queries with “fresh” information.In addition to the three criteria above, model responses were also evaluated holistically. To evaluate responses holistically, evaluators were asked to pick the response they would rather receive from an human assistant who is helping with the query.Curating the Evaluation SetFor this evaluation, we carefully curated a diverse set of prompts with the goal of effectively evaluating helpfulness, factuality, and freshness. Each prompt was manually selected, ensuring a high level of control over the quality and relevance of the data. The dataset spans a wide range of answer engine prompts and encompasses a comprehensive overview of what we want Perplexity to excel at. This is critical for us to our model performance evaluations to have high signal.Each of the three evaluation sets contains 50 prompts. Some examples from these sets include:Helpfulness: Create a table of all the USA soccer players that played in the World Cup finals and make columns for their stats such as goals, assists etc.Factuality: Explain the toughness of AISI 1015 and AISI 1040 steels.Up-to-date-ness: Which autonomous car company got banned from San Francisco in 2023?Generating Model ResponsesWe evaluated four models:pplx-7b-online: Perplexity’s model, which includes access to information from the internet. This model was finetuned using mistral-7b.pplx-70b-online: Perplexity’s model, which includes access to information from the internet. This model was finetuned using llama2-70b. gpt-3.5-turbo-1106: OpenAI’s model, accessed via the API and with no additional augmentations. Note that we conducted this evaluation using the latest gpt-3.5 model.llama2-70b-chat: Meta AI’s model, accessed via our pplx-api and with no additional augmentations.For each prompt, the same search results and snippets were provided to the pplx-7b-online and pplx-70b-online models. All responses were generated using the same hyperparameters and system prompt.System PromptCurrent date: Monday, November 20, 2023.Ranking Model Responses With Human EvaluationFor each pairwise comparison, our in-house contractors were provided the prompt itself, the evaluation criteria (i.e., helpfulness, factuality, or freshness), and the model responses displayed side-by-side. The ordering of the responses were randomized at each turn, and the source models were not revealed to the evaluator. Evaluators were instructed to select the response they holistically prefer, as well as which performs better on the specific evaluation criterion, with ties allowed for both. Finally, evaluators were allowed to use internet search to verify the accuracy of responses. We built our own in-house preference ranking tool to conduct this evaluation.Evaluation Results We use the pairwise preference rankings collected from human evaluation to calculate per-task Elo scores for each model. Elo scores are commonly used to rank a population of players in tournament-style competitions to measure the relative performance of the players. When applied to large language models, these scores provide a prediction of how likely a human evaluator might favor the output of one model over another.While Elo scores are typically calculated for a sequence of comparisons to account for changes in players’ abilities, we aim to quantify the performance of models whose abilities cannot change. Accordingly, we adopt the Bootstrap Elo methodology described in Duan et al and also utilized by lmsys for their Chatbot Arena, which entails calculating Elo scores for many random permutations of the comparisons. For our calculations, we calculated Elo scores over 5000 permutations and used the distributions of these Elo scores to calculate 95% confidence intervals.The results, shown in Figure 1, demonstrate that our PPLX models can match and even surpass gpt-3.5 performance on Perplexity-related use cases. In particular, pplx-7b-online and pplx-70b-online model responses are preferred over gpt-3.5 and llama2-70b model responses by human evaluators for their accurate and up-to-date answers.The pplx-7b-online and pplx-70b-online perform better than gpt-3.5 and llama2-70b, on the freshness, factuality, and holistic criteria.Figure 1. Estimated Elo scores and 95% confidence intervals for pplx-7b (-online), pplx-70b (-online), llama2-70b, and gpt-3.5 across four different evaluation sets. For example, on the freshness axis, pplx-7b and pplx-70b perform better than gpt-3.5 and llama2-70b, with an estimated Elo score of 1100.6 and 1099.6 vs 879.3 and 920.3, respectively.We also calculated the pairwise win rates from the evaluation. The results, shown in Figure 2, illustrate perhaps not surprisingly, the pplx online model responses are preferred to others’ on freshness and on factuality.Figure 2. Win rates of models on the vertical axis against the models on the horizontal axis. For example, pplx-70b (-online) responses were rated as holistically superior to gpt-3.5’s on 64% of queries and gpt-3.5’s responses were rated as holistically superior to pplx-70b (-online) on 24% of queries.Here is an example from our freshness evaluation dataset.Prompt: How many people in USA have a driving license?pplx-7b-onlineIn 2023, there are a projected 243.4 million licensed drivers in the United States, which includes 89% of US adults. This number has increased from 238.6 million in 2022. The percentage of licensed drivers varies across different age groups and states. For example, in 2019, 84.1% of the US population had a driver's license. The highest percentage of licensed drivers is among 25 to 49-year-olds, with 90.1% of the population having a driver's license. In California, there were 27 million licensed drivers, making it the state with the highest number of licensed drivers.pplx-70b-onlineIn 2021, around 233 million people held valid driving licenses in the United States. By 2023, the number of licensed drivers in the US is projected to reach 243.4 million, which includes 89% of US adults. This represents an increase from 238.6 million in 2022.gpt-3.5As an AI language model, I don't have access to real-time data. However, as of 2021, it was estimated that around 227 million people in the United States held a valid driver's license. Please note that this number may have changed since then, so it's always best to refer to the latest official statistics for the most accurate information.llama2-70bI don't have access to real-time data, but I can provide some information on the approximate number of people in the United States who have a driver's license. According to the Federal Highway Administration, there were approximately 227.8 million licensed drivers in the United States as of 2020. This number includes both commercial and non-commercial drivers. It's worth noting that this number is likely an underestimate, as it doesn't account for individuals who may have a driver's license but do not drive regularly or at all. Additionally, this number does not include individuals who are not eligible for a driver's license, such as those who are under the age of 16 or have certain disabilities. It's also important to note that the number of licensed drivers can vary by state, with some states having a higher percentage of licensed drivers than others.Overall, the evaluation results demonstrate that our PPLX models can match and even outperform gpt-3.5 and llama2-70b on Perplexity-related use cases, particularly for providing accurate and up-to-date responses.Accessing Perplexity’s Online ModelsWe are thrilled to announce the pplx-api is phasing out of beta into general public release! This is the first time our pplx-7b-online and pplx-70b-online models are accessible via the pplx-api. Additionally, our pplx-7b-chat and pplx-70b-chat models are out of alpha, and are now accessible with our general release.With this, we are introducing a new usage-based pricing structure. We’re excited for our users to utilize our state-of-the-art infrastructure, which utilizes NVIDIA H100s to provide blazing fast inference. Pro users will receive a recurring $5 monthly pplx-api credit. For all other users, pricing will be determined based on usage. To sign up as a Pro user, click here. Reach out to api@perplexity.ai for commercial inquiries.Additional Resources:Get started with pplx-api here. Try out our online models for free with Perplexity Labs.Learn more about our API via the pplx-api documentation.Interested in working on a high-impact, high-velocity team building the best answer engine? Join us!Join our growing Discord Community!ContributorsLauren Yang, Kevin Hu, Aarash Heydari, Gradey Wang, Dmitry Pervukhin, Nikhil Thota, Alexandr Yarats, Max Morozov, Denis YaratsIntroducing PPLX Online LLMs The first-of-its-kind Online LLM API.We’re excited to share two new PPLX models: pplx-7b-online and pplx-70b-online! Our online models are focused on delivering helpful, up-to-date, and factual responses, and are publicly available via pplx-api, making it a first-of-its-kind API. pplx-7b-online and pplx-70b-online are also accessible via Perplexity Labs, our LLM playground.LLMs have transformed the way we find information. However, there are two limitations with most LLMs today:Freshness: LLMs often struggle to share up-to-date information.Hallucinations: LLMs can also output inaccurate statements.Our pplx-7b-online and pplx-70b-online models address current limitations by additionally providing helpful, factual, and up-to-date information in its responses.PPLX Online LLMsOur LLMs, pplx-7b-online and pplx-70b-online, are online LLMs because they can use knowledge from the internet, and thus can leverage the most up-to-date information when forming a response. By providing our LLMs with knowledge from the web, our models accurately respond to time sensitive queries, unlocking knowledge beyond its training corpus. This means Perplexity’s online LLMs can answer queries like “What was the Warriors game score last night?” that are challenging for offline models. At a high level, this is how our online LLM works:Leverage open sourced models: our PPLX models build on top of mistral-7b and llama2-70b base models.In-house search technology: our in-house search, indexing, and crawling infrastructure allows us to augment LLMs with the most relevant, up to date, and valuable information. Our search index is large, updated on a regular cadence, and uses sophisticated ranking algorithms to ensure high quality, non-SEOed sites are prioritized.\\xa0Website excerpts, which we call “snippets”, are provided to our pplx-online models to enable responses with the most up-to-date information.Fine-tuning: our PPLX models have been fine-tuned to effectively use snippets to inform their responses. Using our in-house data contractors, we carefully curate high quality, diverse, and large training sets in order to achieve high performance on various axes like helpfulness, factuality, and freshness. Our models are regularly fine-tuned to continually improve performance.Evaluating Perplexity’s Online LLMsPerplexity’s mission is to build the world’s best answer engine - one that people trust to discover and expand their knowledge. To achieve this, we are deeply focused on providing helpful, factual, and up-to-date information. To benchmark our LLMs’ performance on these axes, we curated evaluation datasets to reflect challenging yet realistic use cases for answer engines. For each query in each evaluation set, contractors were given two model responses and instructed to select the response that performed better for the following criteria:Helpfulness: which response answers the query and follow the specified instructions better?Factuality: which response provides more accurate answers without hallucinations, even for questions that require very precise or niche knowledge?Freshness (inspired by FreshLLMs): which response contains more up-to-date information? A model excels in this criterion if it is able to answer queries with “fresh” information.In addition to the three criteria above, model responses were also evaluated holistically. To evaluate responses holistically, evaluators were asked to pick the response they would rather receive from an human assistant who is helping with the query.Curating the Evaluation SetFor this evaluation, we carefully curated a diverse set of prompts with the goal of effectively evaluating helpfulness, factuality, and freshness. Each prompt was manually selected, ensuring a high level of control over the quality and relevance of the data. The dataset spans a wide range of answer engine prompts and encompasses a comprehensive overview of what we want Perplexity to excel at. This is critical for us to our model performance evaluations to have high signal.Each of the three evaluation sets contains 50 prompts. Some examples from these sets include:Helpfulness: Create a table of all the USA soccer players that played in the World Cup finals and make columns for their stats such as goals, assists etc.Factuality: Explain the toughness of AISI 1015 and AISI 1040 steels.Up-to-date-ness: Which autonomous car company got banned from San Francisco in 2023?Generating Model ResponsesWe evaluated four models:pplx-7b-online: Perplexity’s model, which includes access to information from the internet. This model was finetuned using mistral-7b.pplx-70b-online: Perplexity’s model, which includes access to information from the internet. This model was finetuned using llama2-70b. gpt-3.5-turbo-1106: OpenAI’s model, accessed via the API and with no additional augmentations. Note that we conducted this evaluation using the latest gpt-3.5 model.llama2-70b-chat: Meta AI’s model, accessed via our pplx-api and with no additional augmentations.For each prompt, the same search results and snippets were provided to the pplx-7b-online and pplx-70b-online models. All responses were generated using the same hyperparameters and system prompt.System PromptCurrent date: Monday, November 20, 2023.Ranking Model Responses With Human EvaluationFor each pairwise comparison, our in-house contractors were provided the prompt itself, the evaluation criteria (i.e., helpfulness, factuality, or freshness), and the model responses displayed side-by-side. The ordering of the responses were randomized at each turn, and the source models were not revealed to the evaluator. Evaluators were instructed to select the response they holistically prefer, as well as which performs better on the specific evaluation criterion, with ties allowed for both. Finally, evaluators were allowed to use internet search to verify the accuracy of responses. We built our own in-house preference ranking tool to conduct this evaluation.Evaluation Results We use the pairwise preference rankings collected from human evaluation to calculate per-task Elo scores for each model. Elo scores are commonly used to rank a population of players in tournament-style competitions to measure the relative performance of the players. When applied to large language models, these scores provide a prediction of how likely a human evaluator might favor the output of one model over another.While Elo scores are typically calculated for a sequence of comparisons to account for changes in players’ abilities, we aim to quantify the performance of models whose abilities cannot change. Accordingly, we adopt the Bootstrap Elo methodology described in Duan et al and also utilized by lmsys for their Chatbot Arena, which entails calculating Elo scores for many random permutations of the comparisons. For our calculations, we calculated Elo scores over 5000 permutations and used the distributions of these Elo scores to calculate 95% confidence intervals.The results, shown in Figure 1, demonstrate that our PPLX models can match and even surpass gpt-3.5 performance on Perplexity-related use cases. In particular, pplx-7b-online and pplx-70b-online model responses are preferred over gpt-3.5 and llama2-70b model responses by human evaluators for their accurate and up-to-date answers.The pplx-7b-online and pplx-70b-online perform better than gpt-3.5 and llama2-70b, on the freshness, factuality, and holistic criteria.Figure 1. Estimated Elo scores and 95% confidence intervals for pplx-7b (-online), pplx-70b (-online), llama2-70b, and gpt-3.5 across four different evaluation sets. For example, on the freshness axis, pplx-7b and pplx-70b perform better than gpt-3.5 and llama2-70b, with an estimated Elo score of 1100.6 and 1099.6 vs 879.3 and 920.3, respectively.We also calculated the pairwise win rates from the evaluation. The results, shown in Figure 2, illustrate perhaps not surprisingly, the pplx online model responses are preferred to others’ on freshness and on factuality.Figure 2. Win rates of models on the vertical axis against the models on the horizontal axis. For example, pplx-70b (-online) responses were rated as holistically superior to gpt-3.5’s on 64% of queries and gpt-3.5’s responses were rated as holistically superior to pplx-70b (-online) on 24% of queries.Here is an example from our freshness evaluation dataset.Prompt: How many people in USA have a driving license?pplx-7b-onlineIn 2023, there are a projected 243.4 million licensed drivers in the United States, which includes 89% of US adults. This number has increased from 238.6 million in 2022. The percentage of licensed drivers varies across different age groups and states. For example, in 2019, 84.1% of the US population had a driver's license. The highest percentage of licensed drivers is among 25 to 49-year-olds, with 90.1% of the population having a driver's license. In California, there were 27 million licensed drivers, making it the state with the highest number of licensed drivers.pplx-70b-onlineIn 2021, around 233 million people held valid driving licenses in the United States. By 2023, the number of licensed drivers in the US is projected to reach 243.4 million, which includes 89% of US adults. This represents an increase from 238.6 million in 2022.gpt-3.5As an AI language model, I don't have access to real-time data. However, as of 2021, it was estimated that around 227 million people in the United States held a valid driver's license. Please note that this number may have changed since then, so it's always best to refer to the latest official statistics for the most accurate information.llama2-70bI don't have access to real-time data, but I can provide some information on the approximate number of people in the United States who have a driver's license. According to the Federal Highway Administration, there were approximately 227.8 million licensed drivers in the United States as of 2020. This number includes both commercial and non-commercial drivers. It's worth noting that this number is likely an underestimate, as it doesn't account for individuals who may have a driver's license but do not drive regularly or at all. Additionally, this number does not include individuals who are not eligible for a driver's license, such as those who are under the age of 16 or have certain disabilities. It's also important to note that the number of licensed drivers can vary by state, with some states having a higher percentage of licensed drivers than others.Overall, the evaluation results demonstrate that our PPLX models can match and even outperform gpt-3.5 and llama2-70b on Perplexity-related use cases, particularly for providing accurate and up-to-date responses.Accessing Perplexity’s Online ModelsWe are thrilled to announce the pplx-api is phasing out of beta into general public release! This is the first time our pplx-7b-online and pplx-70b-online models are accessible via the pplx-api. Additionally, our pplx-7b-chat and pplx-70b-chat models are out of alpha, and are now accessible with our general release.With this, we are introducing a new usage-based pricing structure. We’re excited for our users to utilize our state-of-the-art infrastructure, which utilizes NVIDIA H100s to provide blazing fast inference. Pro users will receive a recurring $5 monthly pplx-api credit. For all other users, pricing will be determined based on usage. To sign up as a Pro user, click here. Reach out to api@perplexity.ai for commercial inquiries.Additional Resources:Get started with pplx-api here. Try out our online models for free with Perplexity Labs.Learn more about our API via the pplx-api documentation.Interested in working on a high-impact, high-velocity team building the best answer engine? Join us!Join our growing Discord Community!ContributorsLauren Yang, Kevin Hu, Aarash Heydari, Gradey Wang, Dmitry Pervukhin, Nikhil Thota, Alexandr Yarats, Max Morozov, Denis Yarats\"),\n",
       " Document(metadata={'source': 'https://www.perplexity.ai/hub/blog/turbocharging-llama-2-70b-with-nvidia-h100'}, page_content='Turbocharging Llama 2 70B with NVIDIA H100 The pursuit of performance in Perplexity’s answer engine drives us to adopt the latest technology that NVIDIA and AWS have to offer. In this blog, we are excited to share the results of our latest experiments: a comparison of Llama 2 70B inference across various hardware and software settings.Our LLM inference platform, pplx-api, is built on a cutting-edge stack powered by open-source libraries. In the time since pplx-api’s public beta began in October, we’ve been tackling scaling challenges and learning how best to tune our configuration to achieve massive scale. This led us to run experiments with the following guiding questions:What is the raw performance gain from switching our GPUs from NVIDIA A100 to NVIDIA H100, all other settings remaining the same?What is the efficiency gain of 8-bit floating point (fp8) quantization, which H100 adds native support for? What is the accuracy cost of this quantization?How do tensor parallelism and batch size affect latency and token throughput?Considering the above, which configuration results in the most scalable balance of performance and cost-efficiency?Experimental setupWe ran the following experiment as a series of local benchmarks to avoid network latency.Key MetricsLatency: The total time it takes for the inference server to generate its full response.Throughput: The number of output tokens, per second, per GPU, that the inference server can generate across all users and requests.ConstantsThe following factors would influence the key metrics, so we kept them consistent across different trials of the experiment.AI ModelPerformance scales with the size of the LLM. More parameters require more computations resulting in slower inference. For example, Llama 2 13B is faster than Llama 2 70B when other settings are equal. We stick to Llama 2 70B in this experiment because we want to optimize for serving the most capable open source models.Input/Output Token DatasetThe amount of input and output tokens in each sample request/response pair can influence performance measurements. In general, output token generation dominates overall response time. When sampling data only induces “yes/no” responses from the LLM, then the response is faster compared to samples that ask the LLM to write essays. Our dataset is composed of synthetic requests with 1024 input tokens inducing 512 output tokens. This distribution was chosen to match the observed distribution of traffic on our public deployment of Llama2 70B.Software VersionNVIDIA TensorRT-LLM (release v0.5.0) is an open-source library for optimizing LLM inference. Released in late 2023, it synthesizes NVIDIA’s many inference optimizations and provides a flexible layer of customization for the key parameters of this experiment: batch size, quantization, and tensor parallelism.VariablesWe experimented across 4 axes of configuration: tensor parallelism, GPU architecture, quantization, and max batch size. These axes are interconnected because they each represent a tradeoff with respect to the critical bottleneck resource of GPU memory.GPU architectureThe ninth-generation Hopper (H100-HBM3-80GB / p5.48xlarge) GPU architecture packs a huge list of features over its predecessor, Ampere (A100-SXM4-80GB / p4de.24xlarge), including 2x-6x computation rates and nearly 2x GPU memory bandwidth. GPU memory bandwidth is a critical metric for inference because a primary latency bottleneck of inference’s matrix multiplications comes from loading gigabytes of model data from GPU memory into compute registers. Based on these stats, we hypothesized that an apples-to-apples comparison of NVIDIA H100 and A100 will exhibit 2x improvement in both latency and throughput.Another key difference between the NVIDIA H100 and A100 is that the H100 tensor core natively adds support for 8-bit floating point (fp8) instructions, which opens the door to further optimizations detailed below. This is why we use fp8 and fp16 specifically for the H100.To keep memory-per-GPU consistent in this experiment, we stuck to nodes with 8x80GB GPU memory for both our NVIDIA A100s and H100s. In addition to enabling higher batch sizes, GPU memory is important because the model’s parameters are loaded into GPU memory during server startup for fast access. For example, if each of the 70 billion parameters in our model is a 16-bit floating point number, then the model is around 140GB in size, which does not fit on a single GPU. Hence the need for tensor parallelism, which we explain below.Tensor ParallelismTensor parallelism refers to the number of GPU devices consumed to run the inference server. When we allocate a number of GPUs, TensorRT-LLM pools their resources together to help us reach the minimum required memory budget for running Llama2 70B. Our hypothesis is that lower tensor parallelism will result in higher latency (due to fewer resources consumed to satisfy each batch) but higher throughput per GPU (due to better utilization) when compared to higher tensor parallelism.QuantizationQuantization is the reduction of precision in the weights and activations used by neural networks. We use this technique to halve the GPU memory consumption when we switch from fp16 to fp8. This makes it possible to run the same model with lower total GPU memory usage, enabling lower tensor parallelism, which drives up throughput.Implementations of quantization have the potential to degrade accuracy. Thus, we evaluated accuracy for different precisions by comparing their perplexity statistic, a measure of how well the LLM predicts each next token in a sentence, on the WikiText corpus. For 8-bit floating point and 8-bit weight with 8-bit activation and SmoothQuant (w8a8 SQ), there was no significant change in perplexity (< 1%) compared to fp16 on WikiText, so we felt confident to proceed. However, w4a16 exhibited a substantial 7% change in perplexity, potentially attributable to the even lower precision and necessary dynamic conversions between int4 and fp16.Batch SizeParallelism via batching is a classic strategy to squeeze performance out of a resource constrained system. By processing multiple requests in each forward pass through the neural network, batching is known to increase throughput at the cost of some latency. Batching also incurs higher GPU memory consumption because the size of the KV cache which manages the attention mechanism grows linearly with the batch size.In the case of Llama 2 70B (which has 80 layers), fp16 with batch size 32 for 4096 context size, the size of the KV cache comes out to a substantial 40 GB. This ends up preventing Llama 2 70B fp16, whose weights alone take up 140GB, from comfortably fitting into the 160GB GPU memory available at tensor parallelism 2 (TP-2).ResultsWe swept through compatible combinations of the 4 variables of the experiment and present the most insightful trends below.Figure 1 - The latency of requests with varying batch size across five different configurations, all with tensor parallelism 8, which yields the best latency with 8 available GPUs. Within each configuration, latency generally doubles when increasing batch size from 1 → 32, and doubles again from 32 → 128. On sufficiently large batch sizes, H100 approximately halves the latency compared to A100. A100 uses mixed precision because the architecture lacks native support for fp8. w8a8 with SmoothQuant (SQ) is meant to resemble fp8.The latency improvement of quantization is in the neighborhood of 10% when comparing H100 fp16 → fp8 and A100 fp16 → w8a8 with SmoothQuant. However, the mixed precision w4a16 actually performs better at low batch sizes and worse at higher batch sizes compared to fp16. This may be due to a number of factors, including less optimized compute kernels, casting time between int4 and fp16, and the fact that w4a16 still uses 16-bit floating points for activations, resulting in no savings in the dimensions of the KV cache. Because w4a16 also demonstrated lower accuracy, we conclude we should stick to w8a8 SQ for A100s and fp8 for H100s.Figure 2 - The throughput across TP-8 configurations with different architecture, quantization, and batch size. For each architecture and quantization, the batch size was chosen as the largest which honored a latency requirement of 25600ms (20 tokens per second for 512 tokens), so that we compare configurations having similar latency. Under this requirement, H100 with BS-128 reaches 228% throughput compared to A100 BS-64 using the same quantization (fp16) and even has lower response latency despite the doubled batch size. Quantization with fp8 improves this factor to 251%.In our first two figures, we only present configurations of TP-8. H100 achieves 54% latency and 184% throughput compared to A100 when both use fp16 / BS-128 / TP-8, which improves to 49% latency and 202% throughput when using fp8 on H100. This improvement in performance can be attributed to the increases in computation power and memory bandwidth between H100 and A100. Notably, the difference is less pronounced under lower batch sizes where utilization may be lower.As we build our platform, we want to honor certain latency requirements for our users while maximizing throughput. Thus, rather than compare A100 vs. H100 at the same batch size, it actually makes more sense for us to compare their throughput only under configurations where they satisfy a latency requirement. We set a cutoff at 25600ms latency for completion of the 512 output tokens and found that H100 / TP-8 / fp8 / BS-128 yields 251% throughput compared to A100 / TP-8 / fp16 / BS-64, since it’s able to process double the batch size at a lower latency. Given that quantization provides GPU memory savings, we now need to evaluate how tensor parallelism can add a next layer of optimization.Figure 3 - The latency across varying batch sizes and tensor parallelism for H100 fp8. Latency generally doubles when increasing batch size from 1 → 32, and doubles again from 32 → 128. TP-2 is consistently around twice as slow as TP-8 when batch sizes are equal. Doubling the tensor parallelism while doubling the batch size keeps latency relatively stable as the number of batches processed per GPUs remains even.When it comes to quantization and architecture, there are clear winners: H100 dominates A100 and lowered-precision quantization improves memory utilization, latency, and throughput. However, batch size and tensor parallelism present a tradeoff in our key metrics. A larger batch size optimizes for throughput at the cost of increased latency and memory consumption. On the other hand, higher tensor parallelism increases the overall pool of available memory and optimizes for latency, but trades off with throughput due to synchronization costs of distributed matrix multiplication and by virtue of locking up more GPU resources.Figure 4 - The throughput across varying batch sizes and tensor parallelism for H100 fp8. The highest throughput comes from TP-2 BS-128, at 460% compared to the baseline of A100/TP-8/fp16/BS-64. However, TP-2 BS-128 is also the slowest result in Figure 3.The throughput-maximizing configuration of our experiment is H100 / fp8 / TP-2 / BS-128, at 767 output tokens per second per GPU. This is a 460% improvement over A100 / fp16 / TP-8 / BS-64. However, it comes at the cost of doubled latency - closer to 42000ms for 512 output tokens - so it may be unsuitable as a production configuration. The results of TP-4 BS-128 (626 tok/sec/gpu at 26188ms response time) and TP-2 BS-32 (435 tok/sec/gpu at 18821ms response time) may represent better tradeoffs on our key metrics.ConclusionOur results demonstrate that:We reach 54% latency and 184% throughput using H100 compared to A100 given the same configuration, which improves to 49% and 202% respectively when H100 takes advantage of its native support for fp8.When maximizing throughput subject to a latency constraint, H100 / fp8 / TP-8 / BS-128 yields 251% throughput compared to A100 / fp16 / TP-8 / BS-64, as it can process double the batch at a faster speed.Taking advantage of H100 with TP-2 with fp8, we can achieve 373% the throughput of A100 / fp16 / TP-8 / BS-128, with less than a 10% increase in latency.Batch size and tensor parallelism present a tradeoff between throughput and latency to the operator of an LLM inference system.These results make us feel confident about a full transition to H100 GPUs in our previously A100 powered hardware stack. We are excited to be able to confirm the performance gains advertised by NVIDIA and look forward to their next breakthroughs in accelerated hardware.What’s NextOur next frontier for optimization is to examine the accuracy and performance impact of structured sparsity and int4 precision, which could significantly reduce Llama 2 70B’s GPU memory footprint and yield up to 2x improvements in latency. We are excited to continue the journey of blazing fast inference and hope to empower our users to build powerful applications on top of our platform.In the near term, pplx-api will be lifting rate limits and offering more custom Perplexity LLMs, including an internet-powered LLM with grounding for facts.Sign up for Perplexity Pro at perplexity.ai/pro. Get access to our cutting-edge pplx-api, and leverage these advanced capabilities in your projects. Discover more about pplx-api on our blog.Interested in shaping the future of AI? We’re hiring! Be part of a team driving massive-scale, generative LLM infrastructure. Explore opportunities at Perplexity Careers.AuthorsAarash Heydari, Grigorii Alekseev, Kevin Hu, Denis YaratsTurbocharging Llama 2 70B with NVIDIA H100 The pursuit of performance in Perplexity’s answer engine drives us to adopt the latest technology that NVIDIA and AWS have to offer. In this blog, we are excited to share the results of our latest experiments: a comparison of Llama 2 70B inference across various hardware and software settings.Our LLM inference platform, pplx-api, is built on a cutting-edge stack powered by open-source libraries. In the time since pplx-api’s public beta began in October, we’ve been tackling scaling challenges and learning how best to tune our configuration to achieve massive scale. This led us to run experiments with the following guiding questions:What is the raw performance gain from switching our GPUs from NVIDIA A100 to NVIDIA H100, all other settings remaining the same?What is the efficiency gain of 8-bit floating point (fp8) quantization, which H100 adds native support for? What is the accuracy cost of this quantization?How do tensor parallelism and batch size affect latency and token throughput?Considering the above, which configuration results in the most scalable balance of performance and cost-efficiency?Experimental setupWe ran the following experiment as a series of local benchmarks to avoid network latency.Key MetricsLatency: The total time it takes for the inference server to generate its full response.Throughput: The number of output tokens, per second, per GPU, that the inference server can generate across all users and requests.ConstantsThe following factors would influence the key metrics, so we kept them consistent across different trials of the experiment.AI ModelPerformance scales with the size of the LLM. More parameters require more computations resulting in slower inference. For example, Llama 2 13B is faster than Llama 2 70B when other settings are equal. We stick to Llama 2 70B in this experiment because we want to optimize for serving the most capable open source models.Input/Output Token DatasetThe amount of input and output tokens in each sample request/response pair can influence performance measurements. In general, output token generation dominates overall response time. When sampling data only induces “yes/no” responses from the LLM, then the response is faster compared to samples that ask the LLM to write essays. Our dataset is composed of synthetic requests with 1024 input tokens inducing 512 output tokens. This distribution was chosen to match the observed distribution of traffic on our public deployment of Llama2 70B.Software VersionNVIDIA TensorRT-LLM (release v0.5.0) is an open-source library for optimizing LLM inference. Released in late 2023, it synthesizes NVIDIA’s many inference optimizations and provides a flexible layer of customization for the key parameters of this experiment: batch size, quantization, and tensor parallelism.VariablesWe experimented across 4 axes of configuration: tensor parallelism, GPU architecture, quantization, and max batch size. These axes are interconnected because they each represent a tradeoff with respect to the critical bottleneck resource of GPU memory.GPU architectureThe ninth-generation Hopper (H100-HBM3-80GB / p5.48xlarge) GPU architecture packs a huge list of features over its predecessor, Ampere (A100-SXM4-80GB / p4de.24xlarge), including 2x-6x computation rates and nearly 2x GPU memory bandwidth. GPU memory bandwidth is a critical metric for inference because a primary latency bottleneck of inference’s matrix multiplications comes from loading gigabytes of model data from GPU memory into compute registers. Based on these stats, we hypothesized that an apples-to-apples comparison of NVIDIA H100 and A100 will exhibit 2x improvement in both latency and throughput.Another key difference between the NVIDIA H100 and A100 is that the H100 tensor core natively adds support for 8-bit floating point (fp8) instructions, which opens the door to further optimizations detailed below. This is why we use fp8 and fp16 specifically for the H100.To keep memory-per-GPU consistent in this experiment, we stuck to nodes with 8x80GB GPU memory for both our NVIDIA A100s and H100s. In addition to enabling higher batch sizes, GPU memory is important because the model’s parameters are loaded into GPU memory during server startup for fast access. For example, if each of the 70 billion parameters in our model is a 16-bit floating point number, then the model is around 140GB in size, which does not fit on a single GPU. Hence the need for tensor parallelism, which we explain below.Tensor ParallelismTensor parallelism refers to the number of GPU devices consumed to run the inference server. When we allocate a number of GPUs, TensorRT-LLM pools their resources together to help us reach the minimum required memory budget for running Llama2 70B. Our hypothesis is that lower tensor parallelism will result in higher latency (due to fewer resources consumed to satisfy each batch) but higher throughput per GPU (due to better utilization) when compared to higher tensor parallelism.QuantizationQuantization is the reduction of precision in the weights and activations used by neural networks. We use this technique to halve the GPU memory consumption when we switch from fp16 to fp8. This makes it possible to run the same model with lower total GPU memory usage, enabling lower tensor parallelism, which drives up throughput.Implementations of quantization have the potential to degrade accuracy. Thus, we evaluated accuracy for different precisions by comparing their perplexity statistic, a measure of how well the LLM predicts each next token in a sentence, on the WikiText corpus. For 8-bit floating point and 8-bit weight with 8-bit activation and SmoothQuant (w8a8 SQ), there was no significant change in perplexity (< 1%) compared to fp16 on WikiText, so we felt confident to proceed. However, w4a16 exhibited a substantial 7% change in perplexity, potentially attributable to the even lower precision and necessary dynamic conversions between int4 and fp16.Batch SizeParallelism via batching is a classic strategy to squeeze performance out of a resource constrained system. By processing multiple requests in each forward pass through the neural network, batching is known to increase throughput at the cost of some latency. Batching also incurs higher GPU memory consumption because the size of the KV cache which manages the attention mechanism grows linearly with the batch size.In the case of Llama 2 70B (which has 80 layers), fp16 with batch size 32 for 4096 context size, the size of the KV cache comes out to a substantial 40 GB. This ends up preventing Llama 2 70B fp16, whose weights alone take up 140GB, from comfortably fitting into the 160GB GPU memory available at tensor parallelism 2 (TP-2).ResultsWe swept through compatible combinations of the 4 variables of the experiment and present the most insightful trends below.Figure 1 - The latency of requests with varying batch size across five different configurations, all with tensor parallelism 8, which yields the best latency with 8 available GPUs. Within each configuration, latency generally doubles when increasing batch size from 1 → 32, and doubles again from 32 → 128. On sufficiently large batch sizes, H100 approximately halves the latency compared to A100. A100 uses mixed precision because the architecture lacks native support for fp8. w8a8 with SmoothQuant (SQ) is meant to resemble fp8.The latency improvement of quantization is in the neighborhood of 10% when comparing H100 fp16 → fp8 and A100 fp16 → w8a8 with SmoothQuant. However, the mixed precision w4a16 actually performs better at low batch sizes and worse at higher batch sizes compared to fp16. This may be due to a number of factors, including less optimized compute kernels, casting time between int4 and fp16, and the fact that w4a16 still uses 16-bit floating points for activations, resulting in no savings in the dimensions of the KV cache. Because w4a16 also demonstrated lower accuracy, we conclude we should stick to w8a8 SQ for A100s and fp8 for H100s.Figure 2 - The throughput across TP-8 configurations with different architecture, quantization, and batch size. For each architecture and quantization, the batch size was chosen as the largest which honored a latency requirement of 25600ms (20 tokens per second for 512 tokens), so that we compare configurations having similar latency. Under this requirement, H100 with BS-128 reaches 228% throughput compared to A100 BS-64 using the same quantization (fp16) and even has lower response latency despite the doubled batch size. Quantization with fp8 improves this factor to 251%.In our first two figures, we only present configurations of TP-8. H100 achieves 54% latency and 184% throughput compared to A100 when both use fp16 / BS-128 / TP-8, which improves to 49% latency and 202% throughput when using fp8 on H100. This improvement in performance can be attributed to the increases in computation power and memory bandwidth between H100 and A100. Notably, the difference is less pronounced under lower batch sizes where utilization may be lower.As we build our platform, we want to honor certain latency requirements for our users while maximizing throughput. Thus, rather than compare A100 vs. H100 at the same batch size, it actually makes more sense for us to compare their throughput only under configurations where they satisfy a latency requirement. We set a cutoff at 25600ms latency for completion of the 512 output tokens and found that H100 / TP-8 / fp8 / BS-128 yields 251% throughput compared to A100 / TP-8 / fp16 / BS-64, since it’s able to process double the batch size at a lower latency. Given that quantization provides GPU memory savings, we now need to evaluate how tensor parallelism can add a next layer of optimization.Figure 3 - The latency across varying batch sizes and tensor parallelism for H100 fp8. Latency generally doubles when increasing batch size from 1 → 32, and doubles again from 32 → 128. TP-2 is consistently around twice as slow as TP-8 when batch sizes are equal. Doubling the tensor parallelism while doubling the batch size keeps latency relatively stable as the number of batches processed per GPUs remains even.When it comes to quantization and architecture, there are clear winners: H100 dominates A100 and lowered-precision quantization improves memory utilization, latency, and throughput. However, batch size and tensor parallelism present a tradeoff in our key metrics. A larger batch size optimizes for throughput at the cost of increased latency and memory consumption. On the other hand, higher tensor parallelism increases the overall pool of available memory and optimizes for latency, but trades off with throughput due to synchronization costs of distributed matrix multiplication and by virtue of locking up more GPU resources.Figure 4 - The throughput across varying batch sizes and tensor parallelism for H100 fp8. The highest throughput comes from TP-2 BS-128, at 460% compared to the baseline of A100/TP-8/fp16/BS-64. However, TP-2 BS-128 is also the slowest result in Figure 3.The throughput-maximizing configuration of our experiment is H100 / fp8 / TP-2 / BS-128, at 767 output tokens per second per GPU. This is a 460% improvement over A100 / fp16 / TP-8 / BS-64. However, it comes at the cost of doubled latency - closer to 42000ms for 512 output tokens - so it may be unsuitable as a production configuration. The results of TP-4 BS-128 (626 tok/sec/gpu at 26188ms response time) and TP-2 BS-32 (435 tok/sec/gpu at 18821ms response time) may represent better tradeoffs on our key metrics.ConclusionOur results demonstrate that:We reach 54% latency and 184% throughput using H100 compared to A100 given the same configuration, which improves to 49% and 202% respectively when H100 takes advantage of its native support for fp8.When maximizing throughput subject to a latency constraint, H100 / fp8 / TP-8 / BS-128 yields 251% throughput compared to A100 / fp16 / TP-8 / BS-64, as it can process double the batch at a faster speed.Taking advantage of H100 with TP-2 with fp8, we can achieve 373% the throughput of A100 / fp16 / TP-8 / BS-128, with less than a 10% increase in latency.Batch size and tensor parallelism present a tradeoff between throughput and latency to the operator of an LLM inference system.These results make us feel confident about a full transition to H100 GPUs in our previously A100 powered hardware stack. We are excited to be able to confirm the performance gains advertised by NVIDIA and look forward to their next breakthroughs in accelerated hardware.What’s NextOur next frontier for optimization is to examine the accuracy and performance impact of structured sparsity and int4 precision, which could significantly reduce Llama 2 70B’s GPU memory footprint and yield up to 2x improvements in latency. We are excited to continue the journey of blazing fast inference and hope to empower our users to build powerful applications on top of our platform.In the near term, pplx-api will be lifting rate limits and offering more custom Perplexity LLMs, including an internet-powered LLM with grounding for facts.Sign up for Perplexity Pro at perplexity.ai/pro. Get access to our cutting-edge pplx-api, and leverage these advanced capabilities in your projects. Discover more about pplx-api on our blog.Interested in shaping the future of AI? We’re hiring! Be part of a team driving massive-scale, generative LLM infrastructure. Explore opportunities at Perplexity Careers.AuthorsAarash Heydari, Grigorii Alekseev, Kevin Hu, Denis YaratsTurbocharging Llama 2 70B with NVIDIA H100 The pursuit of performance in Perplexity’s answer engine drives us to adopt the latest technology that NVIDIA and AWS have to offer. In this blog, we are excited to share the results of our latest experiments: a comparison of Llama 2 70B inference across various hardware and software settings.Our LLM inference platform, pplx-api, is built on a cutting-edge stack powered by open-source libraries. In the time since pplx-api’s public beta began in October, we’ve been tackling scaling challenges and learning how best to tune our configuration to achieve massive scale. This led us to run experiments with the following guiding questions:What is the raw performance gain from switching our GPUs from NVIDIA A100 to NVIDIA H100, all other settings remaining the same?What is the efficiency gain of 8-bit floating point (fp8) quantization, which H100 adds native support for? What is the accuracy cost of this quantization?How do tensor parallelism and batch size affect latency and token throughput?Considering the above, which configuration results in the most scalable balance of performance and cost-efficiency?Experimental setupWe ran the following experiment as a series of local benchmarks to avoid network latency.Key MetricsLatency: The total time it takes for the inference server to generate its full response.Throughput: The number of output tokens, per second, per GPU, that the inference server can generate across all users and requests.ConstantsThe following factors would influence the key metrics, so we kept them consistent across different trials of the experiment.AI ModelPerformance scales with the size of the LLM. More parameters require more computations resulting in slower inference. For example, Llama 2 13B is faster than Llama 2 70B when other settings are equal. We stick to Llama 2 70B in this experiment because we want to optimize for serving the most capable open source models.Input/Output Token DatasetThe amount of input and output tokens in each sample request/response pair can influence performance measurements. In general, output token generation dominates overall response time. When sampling data only induces “yes/no” responses from the LLM, then the response is faster compared to samples that ask the LLM to write essays. Our dataset is composed of synthetic requests with 1024 input tokens inducing 512 output tokens. This distribution was chosen to match the observed distribution of traffic on our public deployment of Llama2 70B.Software VersionNVIDIA TensorRT-LLM (release v0.5.0) is an open-source library for optimizing LLM inference. Released in late 2023, it synthesizes NVIDIA’s many inference optimizations and provides a flexible layer of customization for the key parameters of this experiment: batch size, quantization, and tensor parallelism.VariablesWe experimented across 4 axes of configuration: tensor parallelism, GPU architecture, quantization, and max batch size. These axes are interconnected because they each represent a tradeoff with respect to the critical bottleneck resource of GPU memory.GPU architectureThe ninth-generation Hopper (H100-HBM3-80GB / p5.48xlarge) GPU architecture packs a huge list of features over its predecessor, Ampere (A100-SXM4-80GB / p4de.24xlarge), including 2x-6x computation rates and nearly 2x GPU memory bandwidth. GPU memory bandwidth is a critical metric for inference because a primary latency bottleneck of inference’s matrix multiplications comes from loading gigabytes of model data from GPU memory into compute registers. Based on these stats, we hypothesized that an apples-to-apples comparison of NVIDIA H100 and A100 will exhibit 2x improvement in both latency and throughput.Another key difference between the NVIDIA H100 and A100 is that the H100 tensor core natively adds support for 8-bit floating point (fp8) instructions, which opens the door to further optimizations detailed below. This is why we use fp8 and fp16 specifically for the H100.To keep memory-per-GPU consistent in this experiment, we stuck to nodes with 8x80GB GPU memory for both our NVIDIA A100s and H100s. In addition to enabling higher batch sizes, GPU memory is important because the model’s parameters are loaded into GPU memory during server startup for fast access. For example, if each of the 70 billion parameters in our model is a 16-bit floating point number, then the model is around 140GB in size, which does not fit on a single GPU. Hence the need for tensor parallelism, which we explain below.Tensor ParallelismTensor parallelism refers to the number of GPU devices consumed to run the inference server. When we allocate a number of GPUs, TensorRT-LLM pools their resources together to help us reach the minimum required memory budget for running Llama2 70B. Our hypothesis is that lower tensor parallelism will result in higher latency (due to fewer resources consumed to satisfy each batch) but higher throughput per GPU (due to better utilization) when compared to higher tensor parallelism.QuantizationQuantization is the reduction of precision in the weights and activations used by neural networks. We use this technique to halve the GPU memory consumption when we switch from fp16 to fp8. This makes it possible to run the same model with lower total GPU memory usage, enabling lower tensor parallelism, which drives up throughput.Implementations of quantization have the potential to degrade accuracy. Thus, we evaluated accuracy for different precisions by comparing their perplexity statistic, a measure of how well the LLM predicts each next token in a sentence, on the WikiText corpus. For 8-bit floating point and 8-bit weight with 8-bit activation and SmoothQuant (w8a8 SQ), there was no significant change in perplexity (< 1%) compared to fp16 on WikiText, so we felt confident to proceed. However, w4a16 exhibited a substantial 7% change in perplexity, potentially attributable to the even lower precision and necessary dynamic conversions between int4 and fp16.Batch SizeParallelism via batching is a classic strategy to squeeze performance out of a resource constrained system. By processing multiple requests in each forward pass through the neural network, batching is known to increase throughput at the cost of some latency. Batching also incurs higher GPU memory consumption because the size of the KV cache which manages the attention mechanism grows linearly with the batch size.In the case of Llama 2 70B (which has 80 layers), fp16 with batch size 32 for 4096 context size, the size of the KV cache comes out to a substantial 40 GB. This ends up preventing Llama 2 70B fp16, whose weights alone take up 140GB, from comfortably fitting into the 160GB GPU memory available at tensor parallelism 2 (TP-2).ResultsWe swept through compatible combinations of the 4 variables of the experiment and present the most insightful trends below.Figure 1 - The latency of requests with varying batch size across five different configurations, all with tensor parallelism 8, which yields the best latency with 8 available GPUs. Within each configuration, latency generally doubles when increasing batch size from 1 → 32, and doubles again from 32 → 128. On sufficiently large batch sizes, H100 approximately halves the latency compared to A100. A100 uses mixed precision because the architecture lacks native support for fp8. w8a8 with SmoothQuant (SQ) is meant to resemble fp8.The latency improvement of quantization is in the neighborhood of 10% when comparing H100 fp16 → fp8 and A100 fp16 → w8a8 with SmoothQuant. However, the mixed precision w4a16 actually performs better at low batch sizes and worse at higher batch sizes compared to fp16. This may be due to a number of factors, including less optimized compute kernels, casting time between int4 and fp16, and the fact that w4a16 still uses 16-bit floating points for activations, resulting in no savings in the dimensions of the KV cache. Because w4a16 also demonstrated lower accuracy, we conclude we should stick to w8a8 SQ for A100s and fp8 for H100s.Figure 2 - The throughput across TP-8 configurations with different architecture, quantization, and batch size. For each architecture and quantization, the batch size was chosen as the largest which honored a latency requirement of 25600ms (20 tokens per second for 512 tokens), so that we compare configurations having similar latency. Under this requirement, H100 with BS-128 reaches 228% throughput compared to A100 BS-64 using the same quantization (fp16) and even has lower response latency despite the doubled batch size. Quantization with fp8 improves this factor to 251%.In our first two figures, we only present configurations of TP-8. H100 achieves 54% latency and 184% throughput compared to A100 when both use fp16 / BS-128 / TP-8, which improves to 49% latency and 202% throughput when using fp8 on H100. This improvement in performance can be attributed to the increases in computation power and memory bandwidth between H100 and A100. Notably, the difference is less pronounced under lower batch sizes where utilization may be lower.As we build our platform, we want to honor certain latency requirements for our users while maximizing throughput. Thus, rather than compare A100 vs. H100 at the same batch size, it actually makes more sense for us to compare their throughput only under configurations where they satisfy a latency requirement. We set a cutoff at 25600ms latency for completion of the 512 output tokens and found that H100 / TP-8 / fp8 / BS-128 yields 251% throughput compared to A100 / TP-8 / fp16 / BS-64, since it’s able to process double the batch size at a lower latency. Given that quantization provides GPU memory savings, we now need to evaluate how tensor parallelism can add a next layer of optimization.Figure 3 - The latency across varying batch sizes and tensor parallelism for H100 fp8. Latency generally doubles when increasing batch size from 1 → 32, and doubles again from 32 → 128. TP-2 is consistently around twice as slow as TP-8 when batch sizes are equal. Doubling the tensor parallelism while doubling the batch size keeps latency relatively stable as the number of batches processed per GPUs remains even.When it comes to quantization and architecture, there are clear winners: H100 dominates A100 and lowered-precision quantization improves memory utilization, latency, and throughput. However, batch size and tensor parallelism present a tradeoff in our key metrics. A larger batch size optimizes for throughput at the cost of increased latency and memory consumption. On the other hand, higher tensor parallelism increases the overall pool of available memory and optimizes for latency, but trades off with throughput due to synchronization costs of distributed matrix multiplication and by virtue of locking up more GPU resources.Figure 4 - The throughput across varying batch sizes and tensor parallelism for H100 fp8. The highest throughput comes from TP-2 BS-128, at 460% compared to the baseline of A100/TP-8/fp16/BS-64. However, TP-2 BS-128 is also the slowest result in Figure 3.The throughput-maximizing configuration of our experiment is H100 / fp8 / TP-2 / BS-128, at 767 output tokens per second per GPU. This is a 460% improvement over A100 / fp16 / TP-8 / BS-64. However, it comes at the cost of doubled latency - closer to 42000ms for 512 output tokens - so it may be unsuitable as a production configuration. The results of TP-4 BS-128 (626 tok/sec/gpu at 26188ms response time) and TP-2 BS-32 (435 tok/sec/gpu at 18821ms response time) may represent better tradeoffs on our key metrics.ConclusionOur results demonstrate that:We reach 54% latency and 184% throughput using H100 compared to A100 given the same configuration, which improves to 49% and 202% respectively when H100 takes advantage of its native support for fp8.When maximizing throughput subject to a latency constraint, H100 / fp8 / TP-8 / BS-128 yields 251% throughput compared to A100 / fp16 / TP-8 / BS-64, as it can process double the batch at a faster speed.Taking advantage of H100 with TP-2 with fp8, we can achieve 373% the throughput of A100 / fp16 / TP-8 / BS-128, with less than a 10% increase in latency.Batch size and tensor parallelism present a tradeoff between throughput and latency to the operator of an LLM inference system.These results make us feel confident about a full transition to H100 GPUs in our previously A100 powered hardware stack. We are excited to be able to confirm the performance gains advertised by NVIDIA and look forward to their next breakthroughs in accelerated hardware.What’s NextOur next frontier for optimization is to examine the accuracy and performance impact of structured sparsity and int4 precision, which could significantly reduce Llama 2 70B’s GPU memory footprint and yield up to 2x improvements in latency. We are excited to continue the journey of blazing fast inference and hope to empower our users to build powerful applications on top of our platform.In the near term, pplx-api will be lifting rate limits and offering more custom Perplexity LLMs, including an internet-powered LLM with grounding for facts.Sign up for Perplexity Pro at perplexity.ai/pro. Get access to our cutting-edge pplx-api, and leverage these advanced capabilities in your projects. Discover more about pplx-api on our blog.Interested in shaping the future of AI? We’re hiring! Be part of a team driving massive-scale, generative LLM infrastructure. Explore opportunities at Perplexity Careers.AuthorsAarash Heydari, Grigorii Alekseev, Kevin Hu, Denis Yarats'),\n",
       " Document(metadata={'source': 'https://www.perplexity.ai/hub/blog/introducing-pplx-api'}, page_content='We’re excited to announce pplx-api, designed to be one of the fastest ways to access Mistral 7B, Llama2 13B, Code Llama 34B, Llama2 70B, replit-code-v1.5-3b models. pplx-api makes it easy for developers to integrate cutting-edge open-source LLMs into their projects.Our pplx-api provides:Ease of use: developers can use state-of-the-art open-source models off-the-shelf and get started within minutes with a familiar REST API.Blazing fast inference: our thoughtfully designed inference system is efficient and achieves up to 2.9x lower latency than Replicate and 3.1x lower latency than Anyscale.Battle tested infrastructure: pplx-api is proven to be reliable, serving production-level traffic in both our Perplexity answer engine and our Labs playground.One-stop shop for open-source LLMs: our team is dedicated to adding new open-source models as they arrive. For example, we added Llama and Mistral models within a few hours of launch without pre-release access.pplx-api is in public beta and is free for users with a Perplexity Pro subscription. Use pplx-api for a casual weekend hackathon or as a commercial solution to build new and innovative products. We hope to learn how people can build cool and innovative products with our API through this release. Please reach out to api@perplexity.ai if you have a business use case for pplx-api. We would love to hear from you!Benefits of pplx-apiEase of UseLLM deployment and inference require significant infrastructure undertaking to make model serving performant and cost-efficient. Developers can use our API out-of-the-box, without deep knowledge of C++/CUDA or access to GPUs, while still enjoying the state-of-the-art performance. Our LLM inference also abstracts the complexity and necessity of managing your own hardware, further adding to your ease of use.Blazing Fast InferencePerplexity’s LLM API is carefully designed and optimized for fast inference. To achieve this, we built a proprietary LLM inference infrastructure around NVIDIA’s TensortRT-LLM that is served on A100 GPUs provided by AWS. Learn more in the Overview of pplx-api Infrastructure section. As a result, pplx-api is one of the fastest Llama and Mistral APIs commercially available.To benchmark against existing solutions, we compared the latency of pplx-api with other LLM inference libraries. In our experiments, pplx-api achieves up to 2.92x faster overall latency compared to Text Generation Inference (TGI), and up to 4.35x faster initial response latency. For this experiment, we compared TGI and Perplexity’s inference for single-stream and server scenarios on 2 A100 GPUs using a Llama-2-13B-chat model sharded across both GPUs. For the single-stream scenario, the server processes one request after another. In the server scenario, the client sends requests according to a Poisson distribution with varying request rates to emulate a varying load. For the request rate, we perform a small sweep up to a maximum of 1 request / second, the maximum throughput sustained by TGI. We used real-world data with a variety of input and output token lengths to simulate production behavior. The requests average ~700 input tokens and ~550 output tokens.Using the same inputs and sending a single stream of requests, we also measured the mean latencies of Replicate’s and Anyscale’s APIs for this same model to gather a performance baseline against other existing APIs.Using the same experimental setup, we compared the maximum throughput of pplx-api against TGI, with decoding speed as a latency constraint. In our experiments, pplx-api processes tokens 1.90x-6.75x faster than TGI, and TGI entirely fails to satisfy our stricter latency constraints at 60 and 80\\xa0tokens/second. We evaluate TGI under the same hardware and load conditions that we used to evaluate pplx-api. Comparing this metric with Replicate and Anyscale is not possible since we cannot control their hardware and load factors.For reference, the average human reading speed is 5 tokens/seconds, meaning pplx-api is able to serve at a rate faster than one can read.Overview of pplx-api infrastructureAchieving these latency numbers requires a combination of state-of-the-art software and hardware.AWS p4d instances powered by NVIDIA A100 GPUs set the stage as the most cost-effective and reliable option for scaling out GPUs with best-in-class clock speeds.For software to take advantage of this hardware, we run NVIDIA’s TensorRT-LLM, an open-source library that accelerates and optimizes LLM inference. TensorRT-LLM wraps TensorRT’s deep learning compiler and includes the latest optimized kernels made for cutting-edge implementations of FlashAttention and masked multi-head attention (MHA) for the context and generation phases of LLM model execution.From here, the backbone of AWS and its robust integration with Kubernetes empower us to scale elastically beyond hundreds of GPUs and minimize downtime and network overhead.Use Case: Our API in Productionpplx-api In Perplexity: Cost Reduction and ReliabilityOur API is already powering one of Perplexity’s core product features. Just switching a single feature from an external API to pplx-api resulted in cost savings of $0.62M/year, approximately a 4x reduction in costs. We ran A/B tests and monitored infrastructure metrics to ensure no quality degradation. Over the course of 2 weeks, we observed no statistically significant difference in the A/B test. Additionally, pplx-api could sustain a daily load of over one million requests, totaling almost one billion processed tokens daily.The results of this initial exploration are very encouraging, and we anticipate pplx-api to power more of our product features over time.pplx-api in Perplexity Labs: Open Source Inference EcosystemWe also use pplx-api to power Perplexity Labs, our model playground serving various open-source models.Our team is committed to providing access to the latest state-of-the-art open-sourced LLMs. We integrated Mistral 7B, Code Llama 34b, and all Llama 2 models in a matter of hours after their release, and plan to do so as more capable and open-source LLMs become available.Get Started with Perplexity’s AI APIYou can access the pplx-api REST API using HTTPS requests. Authenticating into pplx-api involves the following steps:Generate an API key through the Perplexity Account Settings Page. The API key is a long-lived access token that can be used until it is manually refreshed or deleted.Send the API key as a bearer token in the Authorization header with each pplx-api request.In the following example, PERPLEXITY_API_KEY is an environment variable bound to a key generated using the above instructions. CURL is used to submit a chat completion request.curl -X POST \\\\\\n     --url https://api.perplexity.ai/chat/completions \\\\\\n     --header \\'accept: application/json\\' \\\\\\n     --header \\'content-type: application/json\\' \\\\\\n     --header \"Authorization: Bearer ${PERPLEXITY_API_KEY}\" \\\\\\n     --data \\'{\\n\\t\"model\": \"mistral-7b-instruct\",\\n  \"stream\": false,\\n  \"max_tokens\": 1024,\\n  \"frequency_penalty\": 1,\\n  \"temperature\": 0.0,\\n  \"messages\": [\\n\\t\\t{\\n\\t\\t\\t\"role\": \"system\",\\n\\t\\t\\t\"content\": \"Be precise and concise in your responses.\"\\n\\t\\t},\\n\\t\\t{\\n\\t\\t\\t\"role\": \"user\",\\n\\t\\t\\t\"content\": \"How many stars are there in our galaxy?\"\\n\\t\\t}\\n\\t]\\n}\\'Which yields the following response, having content-type: application/json{\\n  \"id\": \"3fbf9a47-ac23-446d-8c6b-d911e190a898\",\\n  \"model\": \"mistral-7b-instruct\",\\n  \"object\": \"chat.completion\",\\n  \"created\": 1765322,\\n  \"choices\": [\\n    {\\n      \"index\": 0,\\n      \"finish_reason\": \"stop\",\\n      \"message\": {\\n        \"role\": \"assistant\",\\n        \"content\": \" The Milky Way galaxy contains an estimated 200-400 billion stars..\"\\n      },\\n      \"delta\": {\\n        \"role\": \"assistant\",\\n        \"content\": \" The Milky Way galaxy contains an estimated 200-400 billion stars..\"\\n      }\\n    }\\n  ],\\n  \"usage\": {\\n    \"prompt_tokens\": 40,\\n    \"completion_tokens\": 22,\\n    \"total_tokens\": 62\\n  }\\n}Here is an example Python call:from openai import OpenAI\\n\\nYOUR_API_KEY = \"INSERT API KEY HERE\"\\n\\nmessages = [\\n    {\\n        \"role\": \"system\",\\n        \"content\": (\\n            \"You are an artificial intelligence assistant and you need to \"\\n            \"engage in a helpful, detailed, polite conversation with a user.\"\\n        ),\\n    },\\n    {\\n        \"role\": \"user\",\\n        \"content\": (\\n            \"Count to 100, with a comma between each number and no newlines. \"\\n            \"E.g., 1, 2, 3, ...\"\\n        ),\\n    },\\n]\\n\\nclient = OpenAI(api_key=YOUR_API_KEY, base_url=\"https://api.perplexity.ai\")\\n\\n# demo chat completion without streaming\\nresponse = client.chat.completions.create(\\n    model=\"mistral-7b-instruct\",\\n    messages=messages,\\n)\\nprint(response)\\n\\n# demo chat completion with streaming\\nresponse_stream = client.chat.completions.create(\\n    model=\"mistral-7b-instruct\",\\n    messages=messages,\\n    stream=True,\\n)\\nfor response in response_stream:\\n    print(response)We currently support Mistral 7B, Llama 13B, Code Llama 34B, Llama 70B, and the API is conveniently OpenAI client-compatible for easy integration with existing applications.For more information, please visit our API documentation and Quickstart Guide.What’s NextIn the near future, pplx-api will support:Custom Perplexity LLMs and other open-source LLMs.Custom Perplexity embeddings and open-source embeddings.Dedicated API pricing structure with general access after public beta is phased out.Perplexity RAG-LLM API with grounding for facts and citations.Reach out to api@perplexity.ai if you are interested if any of these use cases.This is also the start of our Perplexity Blog post series. In our next post, we will share a deep dive on A100 vs H100 performance comparison for LLM inference. Stay tuned!We’re hiring! If you want to work on a product deployed at a massive scale and build thoughtfully designed, carefully optimized generative and large language model infrastructure with us, please join us.Follow us on Twitter, LinkedIn and join our Discord for more discussion.AuthorsLauren Yang, Kevin Hu, Aarash Heydari, William Zhang, Dmitry Pervukhin, Grigorii Alekseev, Alexandr YaratsData PrivacyBy choosing\\xa0pplx-api, you can harness the full potential of LLMs while safeguarding the privacy and trust of your users and customers. We understand the importance of protecting your personal information and are committed to maintaining the security and privacy of our users. API data is automatically deleted after 30 days, and we never train on any data transmitted via the pplx-api. Users have the option to opt out of data retention in their account settings. Find our API privacy policy here.We’re excited to announce pplx-api, designed to be one of the fastest ways to access Mistral 7B, Llama2 13B, Code Llama 34B, Llama2 70B, replit-code-v1.5-3b models. pplx-api makes it easy for developers to integrate cutting-edge open-source LLMs into their projects.Our pplx-api provides:Ease of use: developers can use state-of-the-art open-source models off-the-shelf and get started within minutes with a familiar REST API.Blazing fast inference: our thoughtfully designed inference system is efficient and achieves up to 2.9x lower latency than Replicate and 3.1x lower latency than Anyscale.Battle tested infrastructure: pplx-api is proven to be reliable, serving production-level traffic in both our Perplexity answer engine and our Labs playground.One-stop shop for open-source LLMs: our team is dedicated to adding new open-source models as they arrive. For example, we added Llama and Mistral models within a few hours of launch without pre-release access.pplx-api is in public beta and is free for users with a Perplexity Pro subscription. Use pplx-api for a casual weekend hackathon or as a commercial solution to build new and innovative products. We hope to learn how people can build cool and innovative products with our API through this release. Please reach out to api@perplexity.ai if you have a business use case for pplx-api. We would love to hear from you!Benefits of pplx-apiEase of UseLLM deployment and inference require significant infrastructure undertaking to make model serving performant and cost-efficient. Developers can use our API out-of-the-box, without deep knowledge of C++/CUDA or access to GPUs, while still enjoying the state-of-the-art performance. Our LLM inference also abstracts the complexity and necessity of managing your own hardware, further adding to your ease of use.Blazing Fast InferencePerplexity’s LLM API is carefully designed and optimized for fast inference. To achieve this, we built a proprietary LLM inference infrastructure around NVIDIA’s TensortRT-LLM that is served on A100 GPUs provided by AWS. Learn more in the Overview of pplx-api Infrastructure section. As a result, pplx-api is one of the fastest Llama and Mistral APIs commercially available.To benchmark against existing solutions, we compared the latency of pplx-api with other LLM inference libraries. In our experiments, pplx-api achieves up to 2.92x faster overall latency compared to Text Generation Inference (TGI), and up to 4.35x faster initial response latency. For this experiment, we compared TGI and Perplexity’s inference for single-stream and server scenarios on 2 A100 GPUs using a Llama-2-13B-chat model sharded across both GPUs. For the single-stream scenario, the server processes one request after another. In the server scenario, the client sends requests according to a Poisson distribution with varying request rates to emulate a varying load. For the request rate, we perform a small sweep up to a maximum of 1 request / second, the maximum throughput sustained by TGI. We used real-world data with a variety of input and output token lengths to simulate production behavior. The requests average ~700 input tokens and ~550 output tokens.Using the same inputs and sending a single stream of requests, we also measured the mean latencies of Replicate’s and Anyscale’s APIs for this same model to gather a performance baseline against other existing APIs.Using the same experimental setup, we compared the maximum throughput of pplx-api against TGI, with decoding speed as a latency constraint. In our experiments, pplx-api processes tokens 1.90x-6.75x faster than TGI, and TGI entirely fails to satisfy our stricter latency constraints at 60 and 80\\xa0tokens/second. We evaluate TGI under the same hardware and load conditions that we used to evaluate pplx-api. Comparing this metric with Replicate and Anyscale is not possible since we cannot control their hardware and load factors.For reference, the average human reading speed is 5 tokens/seconds, meaning pplx-api is able to serve at a rate faster than one can read.Overview of pplx-api infrastructureAchieving these latency numbers requires a combination of state-of-the-art software and hardware.AWS p4d instances powered by NVIDIA A100 GPUs set the stage as the most cost-effective and reliable option for scaling out GPUs with best-in-class clock speeds.For software to take advantage of this hardware, we run NVIDIA’s TensorRT-LLM, an open-source library that accelerates and optimizes LLM inference. TensorRT-LLM wraps TensorRT’s deep learning compiler and includes the latest optimized kernels made for cutting-edge implementations of FlashAttention and masked multi-head attention (MHA) for the context and generation phases of LLM model execution.From here, the backbone of AWS and its robust integration with Kubernetes empower us to scale elastically beyond hundreds of GPUs and minimize downtime and network overhead.Use Case: Our API in Productionpplx-api In Perplexity: Cost Reduction and ReliabilityOur API is already powering one of Perplexity’s core product features. Just switching a single feature from an external API to pplx-api resulted in cost savings of $0.62M/year, approximately a 4x reduction in costs. We ran A/B tests and monitored infrastructure metrics to ensure no quality degradation. Over the course of 2 weeks, we observed no statistically significant difference in the A/B test. Additionally, pplx-api could sustain a daily load of over one million requests, totaling almost one billion processed tokens daily.The results of this initial exploration are very encouraging, and we anticipate pplx-api to power more of our product features over time.pplx-api in Perplexity Labs: Open Source Inference EcosystemWe also use pplx-api to power Perplexity Labs, our model playground serving various open-source models.Our team is committed to providing access to the latest state-of-the-art open-sourced LLMs. We integrated Mistral 7B, Code Llama 34b, and all Llama 2 models in a matter of hours after their release, and plan to do so as more capable and open-source LLMs become available.Get Started with Perplexity’s AI APIYou can access the pplx-api REST API using HTTPS requests. Authenticating into pplx-api involves the following steps:Generate an API key through the Perplexity Account Settings Page. The API key is a long-lived access token that can be used until it is manually refreshed or deleted.Send the API key as a bearer token in the Authorization header with each pplx-api request.In the following example, PERPLEXITY_API_KEY is an environment variable bound to a key generated using the above instructions. CURL is used to submit a chat completion request.curl -X POST \\\\\\n     --url https://api.perplexity.ai/chat/completions \\\\\\n     --header \\'accept: application/json\\' \\\\\\n     --header \\'content-type: application/json\\' \\\\\\n     --header \"Authorization: Bearer ${PERPLEXITY_API_KEY}\" \\\\\\n     --data \\'{\\n\\t\"model\": \"mistral-7b-instruct\",\\n  \"stream\": false,\\n  \"max_tokens\": 1024,\\n  \"frequency_penalty\": 1,\\n  \"temperature\": 0.0,\\n  \"messages\": [\\n\\t\\t{\\n\\t\\t\\t\"role\": \"system\",\\n\\t\\t\\t\"content\": \"Be precise and concise in your responses.\"\\n\\t\\t},\\n\\t\\t{\\n\\t\\t\\t\"role\": \"user\",\\n\\t\\t\\t\"content\": \"How many stars are there in our galaxy?\"\\n\\t\\t}\\n\\t]\\n}\\'Which yields the following response, having content-type: application/json{\\n  \"id\": \"3fbf9a47-ac23-446d-8c6b-d911e190a898\",\\n  \"model\": \"mistral-7b-instruct\",\\n  \"object\": \"chat.completion\",\\n  \"created\": 1765322,\\n  \"choices\": [\\n    {\\n      \"index\": 0,\\n      \"finish_reason\": \"stop\",\\n      \"message\": {\\n        \"role\": \"assistant\",\\n        \"content\": \" The Milky Way galaxy contains an estimated 200-400 billion stars..\"\\n      },\\n      \"delta\": {\\n        \"role\": \"assistant\",\\n        \"content\": \" The Milky Way galaxy contains an estimated 200-400 billion stars..\"\\n      }\\n    }\\n  ],\\n  \"usage\": {\\n    \"prompt_tokens\": 40,\\n    \"completion_tokens\": 22,\\n    \"total_tokens\": 62\\n  }\\n}Here is an example Python call:from openai import OpenAI\\n\\nYOUR_API_KEY = \"INSERT API KEY HERE\"\\n\\nmessages = [\\n    {\\n        \"role\": \"system\",\\n        \"content\": (\\n            \"You are an artificial intelligence assistant and you need to \"\\n            \"engage in a helpful, detailed, polite conversation with a user.\"\\n        ),\\n    },\\n    {\\n        \"role\": \"user\",\\n        \"content\": (\\n            \"Count to 100, with a comma between each number and no newlines. \"\\n            \"E.g., 1, 2, 3, ...\"\\n        ),\\n    },\\n]\\n\\nclient = OpenAI(api_key=YOUR_API_KEY, base_url=\"https://api.perplexity.ai\")\\n\\n# demo chat completion without streaming\\nresponse = client.chat.completions.create(\\n    model=\"mistral-7b-instruct\",\\n    messages=messages,\\n)\\nprint(response)\\n\\n# demo chat completion with streaming\\nresponse_stream = client.chat.completions.create(\\n    model=\"mistral-7b-instruct\",\\n    messages=messages,\\n    stream=True,\\n)\\nfor response in response_stream:\\n    print(response)We currently support Mistral 7B, Llama 13B, Code Llama 34B, Llama 70B, and the API is conveniently OpenAI client-compatible for easy integration with existing applications.For more information, please visit our API documentation and Quickstart Guide.What’s NextIn the near future, pplx-api will support:Custom Perplexity LLMs and other open-source LLMs.Custom Perplexity embeddings and open-source embeddings.Dedicated API pricing structure with general access after public beta is phased out.Perplexity RAG-LLM API with grounding for facts and citations.Reach out to api@perplexity.ai if you are interested if any of these use cases.This is also the start of our Perplexity Blog post series. In our next post, we will share a deep dive on A100 vs H100 performance comparison for LLM inference. Stay tuned!We’re hiring! If you want to work on a product deployed at a massive scale and build thoughtfully designed, carefully optimized generative and large language model infrastructure with us, please join us.Follow us on Twitter, LinkedIn and join our Discord for more discussion.AuthorsLauren Yang, Kevin Hu, Aarash Heydari, William Zhang, Dmitry Pervukhin, Grigorii Alekseev, Alexandr YaratsData PrivacyBy choosing\\xa0pplx-api, you can harness the full potential of LLMs while safeguarding the privacy and trust of your users and customers. We understand the importance of protecting your personal information and are committed to maintaining the security and privacy of our users. API data is automatically deleted after 30 days, and we never train on any data transmitted via the pplx-api. Users have the option to opt out of data retention in their account settings. Find our API privacy policy here.We’re excited to announce pplx-api, designed to be one of the fastest ways to access Mistral 7B, Llama2 13B, Code Llama 34B, Llama2 70B, replit-code-v1.5-3b models. pplx-api makes it easy for developers to integrate cutting-edge open-source LLMs into their projects.Our pplx-api provides:Ease of use: developers can use state-of-the-art open-source models off-the-shelf and get started within minutes with a familiar REST API.Blazing fast inference: our thoughtfully designed inference system is efficient and achieves up to 2.9x lower latency than Replicate and 3.1x lower latency than Anyscale.Battle tested infrastructure: pplx-api is proven to be reliable, serving production-level traffic in both our Perplexity answer engine and our Labs playground.One-stop shop for open-source LLMs: our team is dedicated to adding new open-source models as they arrive. For example, we added Llama and Mistral models within a few hours of launch without pre-release access.pplx-api is in public beta and is free for users with a Perplexity Pro subscription. Use pplx-api for a casual weekend hackathon or as a commercial solution to build new and innovative products. We hope to learn how people can build cool and innovative products with our API through this release. Please reach out to api@perplexity.ai if you have a business use case for pplx-api. We would love to hear from you!Benefits of pplx-apiEase of UseLLM deployment and inference require significant infrastructure undertaking to make model serving performant and cost-efficient. Developers can use our API out-of-the-box, without deep knowledge of C++/CUDA or access to GPUs, while still enjoying the state-of-the-art performance. Our LLM inference also abstracts the complexity and necessity of managing your own hardware, further adding to your ease of use.Blazing Fast InferencePerplexity’s LLM API is carefully designed and optimized for fast inference. To achieve this, we built a proprietary LLM inference infrastructure around NVIDIA’s TensortRT-LLM that is served on A100 GPUs provided by AWS. Learn more in the Overview of pplx-api Infrastructure section. As a result, pplx-api is one of the fastest Llama and Mistral APIs commercially available.To benchmark against existing solutions, we compared the latency of pplx-api with other LLM inference libraries. In our experiments, pplx-api achieves up to 2.92x faster overall latency compared to Text Generation Inference (TGI), and up to 4.35x faster initial response latency. For this experiment, we compared TGI and Perplexity’s inference for single-stream and server scenarios on 2 A100 GPUs using a Llama-2-13B-chat model sharded across both GPUs. For the single-stream scenario, the server processes one request after another. In the server scenario, the client sends requests according to a Poisson distribution with varying request rates to emulate a varying load. For the request rate, we perform a small sweep up to a maximum of 1 request / second, the maximum throughput sustained by TGI. We used real-world data with a variety of input and output token lengths to simulate production behavior. The requests average ~700 input tokens and ~550 output tokens.Using the same inputs and sending a single stream of requests, we also measured the mean latencies of Replicate’s and Anyscale’s APIs for this same model to gather a performance baseline against other existing APIs.Using the same experimental setup, we compared the maximum throughput of pplx-api against TGI, with decoding speed as a latency constraint. In our experiments, pplx-api processes tokens 1.90x-6.75x faster than TGI, and TGI entirely fails to satisfy our stricter latency constraints at 60 and 80\\xa0tokens/second. We evaluate TGI under the same hardware and load conditions that we used to evaluate pplx-api. Comparing this metric with Replicate and Anyscale is not possible since we cannot control their hardware and load factors.For reference, the average human reading speed is 5 tokens/seconds, meaning pplx-api is able to serve at a rate faster than one can read.Overview of pplx-api infrastructureAchieving these latency numbers requires a combination of state-of-the-art software and hardware.AWS p4d instances powered by NVIDIA A100 GPUs set the stage as the most cost-effective and reliable option for scaling out GPUs with best-in-class clock speeds.For software to take advantage of this hardware, we run NVIDIA’s TensorRT-LLM, an open-source library that accelerates and optimizes LLM inference. TensorRT-LLM wraps TensorRT’s deep learning compiler and includes the latest optimized kernels made for cutting-edge implementations of FlashAttention and masked multi-head attention (MHA) for the context and generation phases of LLM model execution.From here, the backbone of AWS and its robust integration with Kubernetes empower us to scale elastically beyond hundreds of GPUs and minimize downtime and network overhead.Use Case: Our API in Productionpplx-api In Perplexity: Cost Reduction and ReliabilityOur API is already powering one of Perplexity’s core product features. Just switching a single feature from an external API to pplx-api resulted in cost savings of $0.62M/year, approximately a 4x reduction in costs. We ran A/B tests and monitored infrastructure metrics to ensure no quality degradation. Over the course of 2 weeks, we observed no statistically significant difference in the A/B test. Additionally, pplx-api could sustain a daily load of over one million requests, totaling almost one billion processed tokens daily.The results of this initial exploration are very encouraging, and we anticipate pplx-api to power more of our product features over time.pplx-api in Perplexity Labs: Open Source Inference EcosystemWe also use pplx-api to power Perplexity Labs, our model playground serving various open-source models.Our team is committed to providing access to the latest state-of-the-art open-sourced LLMs. We integrated Mistral 7B, Code Llama 34b, and all Llama 2 models in a matter of hours after their release, and plan to do so as more capable and open-source LLMs become available.Get Started with Perplexity’s AI APIYou can access the pplx-api REST API using HTTPS requests. Authenticating into pplx-api involves the following steps:Generate an API key through the Perplexity Account Settings Page. The API key is a long-lived access token that can be used until it is manually refreshed or deleted.Send the API key as a bearer token in the Authorization header with each pplx-api request.In the following example, PERPLEXITY_API_KEY is an environment variable bound to a key generated using the above instructions. CURL is used to submit a chat completion request.curl -X POST \\\\\\n     --url https://api.perplexity.ai/chat/completions \\\\\\n     --header \\'accept: application/json\\' \\\\\\n     --header \\'content-type: application/json\\' \\\\\\n     --header \"Authorization: Bearer ${PERPLEXITY_API_KEY}\" \\\\\\n     --data \\'{\\n\\t\"model\": \"mistral-7b-instruct\",\\n  \"stream\": false,\\n  \"max_tokens\": 1024,\\n  \"frequency_penalty\": 1,\\n  \"temperature\": 0.0,\\n  \"messages\": [\\n\\t\\t{\\n\\t\\t\\t\"role\": \"system\",\\n\\t\\t\\t\"content\": \"Be precise and concise in your responses.\"\\n\\t\\t},\\n\\t\\t{\\n\\t\\t\\t\"role\": \"user\",\\n\\t\\t\\t\"content\": \"How many stars are there in our galaxy?\"\\n\\t\\t}\\n\\t]\\n}\\'Which yields the following response, having content-type: application/json{\\n  \"id\": \"3fbf9a47-ac23-446d-8c6b-d911e190a898\",\\n  \"model\": \"mistral-7b-instruct\",\\n  \"object\": \"chat.completion\",\\n  \"created\": 1765322,\\n  \"choices\": [\\n    {\\n      \"index\": 0,\\n      \"finish_reason\": \"stop\",\\n      \"message\": {\\n        \"role\": \"assistant\",\\n        \"content\": \" The Milky Way galaxy contains an estimated 200-400 billion stars..\"\\n      },\\n      \"delta\": {\\n        \"role\": \"assistant\",\\n        \"content\": \" The Milky Way galaxy contains an estimated 200-400 billion stars..\"\\n      }\\n    }\\n  ],\\n  \"usage\": {\\n    \"prompt_tokens\": 40,\\n    \"completion_tokens\": 22,\\n    \"total_tokens\": 62\\n  }\\n}Here is an example Python call:from openai import OpenAI\\n\\nYOUR_API_KEY = \"INSERT API KEY HERE\"\\n\\nmessages = [\\n    {\\n        \"role\": \"system\",\\n        \"content\": (\\n            \"You are an artificial intelligence assistant and you need to \"\\n            \"engage in a helpful, detailed, polite conversation with a user.\"\\n        ),\\n    },\\n    {\\n        \"role\": \"user\",\\n        \"content\": (\\n            \"Count to 100, with a comma between each number and no newlines. \"\\n            \"E.g., 1, 2, 3, ...\"\\n        ),\\n    },\\n]\\n\\nclient = OpenAI(api_key=YOUR_API_KEY, base_url=\"https://api.perplexity.ai\")\\n\\n# demo chat completion without streaming\\nresponse = client.chat.completions.create(\\n    model=\"mistral-7b-instruct\",\\n    messages=messages,\\n)\\nprint(response)\\n\\n# demo chat completion with streaming\\nresponse_stream = client.chat.completions.create(\\n    model=\"mistral-7b-instruct\",\\n    messages=messages,\\n    stream=True,\\n)\\nfor response in response_stream:\\n    print(response)We currently support Mistral 7B, Llama 13B, Code Llama 34B, Llama 70B, and the API is conveniently OpenAI client-compatible for easy integration with existing applications.For more information, please visit our API documentation and Quickstart Guide.What’s NextIn the near future, pplx-api will support:Custom Perplexity LLMs and other open-source LLMs.Custom Perplexity embeddings and open-source embeddings.Dedicated API pricing structure with general access after public beta is phased out.Perplexity RAG-LLM API with grounding for facts and citations.Reach out to api@perplexity.ai if you are interested if any of these use cases.This is also the start of our Perplexity Blog post series. In our next post, we will share a deep dive on A100 vs H100 performance comparison for LLM inference. Stay tuned!We’re hiring! If you want to work on a product deployed at a massive scale and build thoughtfully designed, carefully optimized generative and large language model infrastructure with us, please join us.Follow us on Twitter, LinkedIn and join our Discord for more discussion.AuthorsLauren Yang, Kevin Hu, Aarash Heydari, William Zhang, Dmitry Pervukhin, Grigorii Alekseev, Alexandr YaratsData PrivacyBy choosing\\xa0pplx-api, you can harness the full potential of LLMs while safeguarding the privacy and trust of your users and customers. We understand the importance of protecting your personal information and are committed to maintaining the security and privacy of our users. API data is automatically deleted after 30 days, and we never train on any data transmitted via the pplx-api. Users have the option to opt out of data retention in their account settings. Find our API privacy policy here.'),\n",
       " Document(metadata={'source': 'https://www.perplexity.ai/hub/blog/announcing-our-series-a-funding-round-and-mobile-app-launch'}, page_content=\"Perplexity raises Series A funding round At Perplexity.ai, we strive to bring you the best possible knowledge discovery experience. Founded in August 2022, our journey began with the release of Ask, our answer engine, in December 2022. Since then, we've experienced rapid growth, reaching 2 million monthly active users in just four months. As our user base grows, so does our commitment to innovation.We are delighted to announce that we have recently raised a $25.6 million series A funding round led by Peter Sonsini of New Enterprise Associates (Board member, Databricks) with participation from our seed round investors Elad Gil (Founder, Color Health), Nat Friedman (Former CEO of GitHub) and Bob Muglia (Former President of Microsoft), as well as new investors Susan Wojcicki (Former CEO of Youtube), Paul Buchheit (Creator of Gmail), Soleio (Designer of Messenger, Dropbox), and Databricks Ventures.We are also grateful to our angel investors who participated in our $3.1 million seed round in September 2022, led by Elad Gil and Nat Friedman, with participation from Pieter Abbeel (UC Berkeley, AIX Ventures), Yann LeCun (Chief Scientist, Meta), Andrej Karpathy (Founding Member, OpenAI), Ashish Vaswani (Lead Inventor of Transformer), Amjad Masad (CEO, Replit), Clem Delangue (CEO, HuggingFace) and others.Our long-term mission is to become the best platform for answers and information, serving as the go-to source for people seeking quick, accurate answers tailored to their asks.We envision Perplexity AI as a platform beyond a traditional search engine, evolving into a comprehensive knowledge hub where anyone can explore and learn effortlessly. In pursuit of this vision, we are committed to providing citations with every answer, providing proper attribution for sources of information and allowing for verification.As part of our mission, we are excited to expand our platform from web to mobile, with almost half of our users already accessing us via mobile web. To improve our mobile experience, today we are launching the Perplexity AI iPhone app. With instant answers, cited sources, voice search, follow-up questions, and thread history, our app delivers a comprehensive interface for information.As a team, we are honored to have the support of our users and investors as we pursue our mission of redefining the way people search for and access information. Thank you for joining us on this journey, and we look forward to continuing to push the limits of language models and search. The frontier of interactive possibilities is just beginning to be explored.Download our iPhone app today at perplexity.ai/download-iphone.Our team is small but growing: see perplexity.ai/careers for the latest job opportunities at Perplexity.Follow our Twitter at twitter.com/perplexity_ai for new releases.Join our Discord community at discord.gg/perplexity-ai.Perplexity raises Series A funding round At Perplexity.ai, we strive to bring you the best possible knowledge discovery experience. Founded in August 2022, our journey began with the release of Ask, our answer engine, in December 2022. Since then, we've experienced rapid growth, reaching 2 million monthly active users in just four months. As our user base grows, so does our commitment to innovation.We are delighted to announce that we have recently raised a $25.6 million series A funding round led by Peter Sonsini of New Enterprise Associates (Board member, Databricks) with participation from our seed round investors Elad Gil (Founder, Color Health), Nat Friedman (Former CEO of GitHub) and Bob Muglia (Former President of Microsoft), as well as new investors Susan Wojcicki (Former CEO of Youtube), Paul Buchheit (Creator of Gmail), Soleio (Designer of Messenger, Dropbox), and Databricks Ventures.We are also grateful to our angel investors who participated in our $3.1 million seed round in September 2022, led by Elad Gil and Nat Friedman, with participation from Pieter Abbeel (UC Berkeley, AIX Ventures), Yann LeCun (Chief Scientist, Meta), Andrej Karpathy (Founding Member, OpenAI), Ashish Vaswani (Lead Inventor of Transformer), Amjad Masad (CEO, Replit), Clem Delangue (CEO, HuggingFace) and others.Our long-term mission is to become the best platform for answers and information, serving as the go-to source for people seeking quick, accurate answers tailored to their asks.We envision Perplexity AI as a platform beyond a traditional search engine, evolving into a comprehensive knowledge hub where anyone can explore and learn effortlessly. In pursuit of this vision, we are committed to providing citations with every answer, providing proper attribution for sources of information and allowing for verification.As part of our mission, we are excited to expand our platform from web to mobile, with almost half of our users already accessing us via mobile web. To improve our mobile experience, today we are launching the Perplexity AI iPhone app. With instant answers, cited sources, voice search, follow-up questions, and thread history, our app delivers a comprehensive interface for information.As a team, we are honored to have the support of our users and investors as we pursue our mission of redefining the way people search for and access information. Thank you for joining us on this journey, and we look forward to continuing to push the limits of language models and search. The frontier of interactive possibilities is just beginning to be explored.Download our iPhone app today at perplexity.ai/download-iphone.Our team is small but growing: see perplexity.ai/careers for the latest job opportunities at Perplexity.Follow our Twitter at twitter.com/perplexity_ai for new releases.Join our Discord community at discord.gg/perplexity-ai.Perplexity raises Series A funding round At Perplexity.ai, we strive to bring you the best possible knowledge discovery experience. Founded in August 2022, our journey began with the release of Ask, our answer engine, in December 2022. Since then, we've experienced rapid growth, reaching 2 million monthly active users in just four months. As our user base grows, so does our commitment to innovation.We are delighted to announce that we have recently raised a $25.6 million series A funding round led by Peter Sonsini of New Enterprise Associates (Board member, Databricks) with participation from our seed round investors Elad Gil (Founder, Color Health), Nat Friedman (Former CEO of GitHub) and Bob Muglia (Former President of Microsoft), as well as new investors Susan Wojcicki (Former CEO of Youtube), Paul Buchheit (Creator of Gmail), Soleio (Designer of Messenger, Dropbox), and Databricks Ventures.We are also grateful to our angel investors who participated in our $3.1 million seed round in September 2022, led by Elad Gil and Nat Friedman, with participation from Pieter Abbeel (UC Berkeley, AIX Ventures), Yann LeCun (Chief Scientist, Meta), Andrej Karpathy (Founding Member, OpenAI), Ashish Vaswani (Lead Inventor of Transformer), Amjad Masad (CEO, Replit), Clem Delangue (CEO, HuggingFace) and others.Our long-term mission is to become the best platform for answers and information, serving as the go-to source for people seeking quick, accurate answers tailored to their asks.We envision Perplexity AI as a platform beyond a traditional search engine, evolving into a comprehensive knowledge hub where anyone can explore and learn effortlessly. In pursuit of this vision, we are committed to providing citations with every answer, providing proper attribution for sources of information and allowing for verification.As part of our mission, we are excited to expand our platform from web to mobile, with almost half of our users already accessing us via mobile web. To improve our mobile experience, today we are launching the Perplexity AI iPhone app. With instant answers, cited sources, voice search, follow-up questions, and thread history, our app delivers a comprehensive interface for information.As a team, we are honored to have the support of our users and investors as we pursue our mission of redefining the way people search for and access information. Thank you for joining us on this journey, and we look forward to continuing to push the limits of language models and search. The frontier of interactive possibilities is just beginning to be explored.Download our iPhone app today at perplexity.ai/download-iphone.Our team is small but growing: see perplexity.ai/careers for the latest job opportunities at Perplexity.Follow our Twitter at twitter.com/perplexity_ai for new releases.Join our Discord community at discord.gg/perplexity-ai.\")]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04adb2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Example docs list\n",
    "# docs = [{\"metadata\": {\"source\": \"https://example.com/page/heading\"}, \"page_content\": \"Some content\"}]\n",
    "#This removes any trailing slash / at the end of the URL. Because if you don’t remove the slash, the next step (split(\"/\")[-1]) might return an empty string by mistake\n",
    "#[-1] This selects the last element of the list.\n",
    "\n",
    "df = pd.DataFrame([{\n",
    "    \"url\": doc.metadata[\"source\"],\n",
    "    \"heading\": doc.metadata[\"source\"].rstrip(\"/\").split(\"/\")[-1],  # Extract last part after '/'\n",
    "    \"content\": doc.page_content\n",
    "} for doc in docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e7057b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"perplexity_articles.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2f90a16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>heading</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://research.perplexity.ai/articles/weight...</td>\n",
       "      <td>weight-transfer-for-rl-post-training-in-under-...</td>\n",
       "      <td>We recently achieved 1.3-second cross-machine ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://research.perplexity.ai/articles/gpt-os...</td>\n",
       "      <td>gpt-oss-on-day-0</td>\n",
       "      <td>GPT-OSS on Day 0Perplexity is committed to mak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://research.perplexity.ai/articles/disagg...</td>\n",
       "      <td>disaggregated-prefill-and-decode</td>\n",
       "      <td>In order to generate output tokens from an inp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://research.perplexity.ai/articles/high-p...</td>\n",
       "      <td>high-performance-gpu-memory-transfer-on-aws-sa...</td>\n",
       "      <td>Modern deep learning infrastructure often requ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://research.perplexity.ai/articles/accele...</td>\n",
       "      <td>accelerating-sonar-through-speculation</td>\n",
       "      <td>Speculative decoding speeds up the generation ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>https://research.perplexity.ai/articles/rl-tra...</td>\n",
       "      <td>rl-training-for-math-reasoning</td>\n",
       "      <td>Reinforcement Learning (RL) algorithms, especi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>https://research.perplexity.ai/articles/lower-...</td>\n",
       "      <td>lower-latency-and-higher-throughput-with-multi...</td>\n",
       "      <td>In most systems, latency and throughput are of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>https://research.perplexity.ai/articles/effici...</td>\n",
       "      <td>efficient-and-portable-mixture-of-experts-comm...</td>\n",
       "      <td>We present a high-performance, portable, open-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>https://www.perplexity.ai/hub/blog/announcing-...</td>\n",
       "      <td>announcing-comet-plus-launch-partners</td>\n",
       "      <td>Announcing Comet Plus Launch Partners Today, w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>https://www.perplexity.ai/hub/blog/introducing...</td>\n",
       "      <td>introducing-the-perplexity-search-api</td>\n",
       "      <td>Introducing the Perplexity Search APIProviding...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  \\\n",
       "0  https://research.perplexity.ai/articles/weight...   \n",
       "1  https://research.perplexity.ai/articles/gpt-os...   \n",
       "2  https://research.perplexity.ai/articles/disagg...   \n",
       "3  https://research.perplexity.ai/articles/high-p...   \n",
       "4  https://research.perplexity.ai/articles/accele...   \n",
       "5  https://research.perplexity.ai/articles/rl-tra...   \n",
       "6  https://research.perplexity.ai/articles/lower-...   \n",
       "7  https://research.perplexity.ai/articles/effici...   \n",
       "8  https://www.perplexity.ai/hub/blog/announcing-...   \n",
       "9  https://www.perplexity.ai/hub/blog/introducing...   \n",
       "\n",
       "                                             heading  \\\n",
       "0  weight-transfer-for-rl-post-training-in-under-...   \n",
       "1                                   gpt-oss-on-day-0   \n",
       "2                   disaggregated-prefill-and-decode   \n",
       "3  high-performance-gpu-memory-transfer-on-aws-sa...   \n",
       "4             accelerating-sonar-through-speculation   \n",
       "5                     rl-training-for-math-reasoning   \n",
       "6  lower-latency-and-higher-throughput-with-multi...   \n",
       "7  efficient-and-portable-mixture-of-experts-comm...   \n",
       "8              announcing-comet-plus-launch-partners   \n",
       "9              introducing-the-perplexity-search-api   \n",
       "\n",
       "                                             content  \n",
       "0  We recently achieved 1.3-second cross-machine ...  \n",
       "1  GPT-OSS on Day 0Perplexity is committed to mak...  \n",
       "2  In order to generate output tokens from an inp...  \n",
       "3  Modern deep learning infrastructure often requ...  \n",
       "4  Speculative decoding speeds up the generation ...  \n",
       "5  Reinforcement Learning (RL) algorithms, especi...  \n",
       "6  In most systems, latency and throughput are of...  \n",
       "7  We present a high-performance, portable, open-...  \n",
       "8  Announcing Comet Plus Launch Partners Today, w...  \n",
       "9  Introducing the Perplexity Search APIProviding...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b606c04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=[\"url\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cbb31e81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>heading</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>weight-transfer-for-rl-post-training-in-under-...</td>\n",
       "      <td>We recently achieved 1.3-second cross-machine ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt-oss-on-day-0</td>\n",
       "      <td>GPT-OSS on Day 0Perplexity is committed to mak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>disaggregated-prefill-and-decode</td>\n",
       "      <td>In order to generate output tokens from an inp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>high-performance-gpu-memory-transfer-on-aws-sa...</td>\n",
       "      <td>Modern deep learning infrastructure often requ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>accelerating-sonar-through-speculation</td>\n",
       "      <td>Speculative decoding speeds up the generation ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             heading  \\\n",
       "0  weight-transfer-for-rl-post-training-in-under-...   \n",
       "1                                   gpt-oss-on-day-0   \n",
       "2                   disaggregated-prefill-and-decode   \n",
       "3  high-performance-gpu-memory-transfer-on-aws-sa...   \n",
       "4             accelerating-sonar-through-speculation   \n",
       "\n",
       "                                             content  \n",
       "0  We recently achieved 1.3-second cross-machine ...  \n",
       "1  GPT-OSS on Day 0Perplexity is committed to mak...  \n",
       "2  In order to generate output tokens from an inp...  \n",
       "3  Modern deep learning infrastructure often requ...  \n",
       "4  Speculative decoding speeds up the generation ...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60aff1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"content\"] = df[\"heading\"] + \" \" + df[\"content\"]\n",
    "# we have to merge the heading column with content column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "502dc1b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>heading</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>weight-transfer-for-rl-post-training-in-under-...</td>\n",
       "      <td>weight-transfer-for-rl-post-training-in-under-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt-oss-on-day-0</td>\n",
       "      <td>gpt-oss-on-day-0 GPT-OSS on Day 0Perplexity is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>disaggregated-prefill-and-decode</td>\n",
       "      <td>disaggregated-prefill-and-decode In order to g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>high-performance-gpu-memory-transfer-on-aws-sa...</td>\n",
       "      <td>high-performance-gpu-memory-transfer-on-aws-sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>accelerating-sonar-through-speculation</td>\n",
       "      <td>accelerating-sonar-through-speculation Specula...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             heading  \\\n",
       "0  weight-transfer-for-rl-post-training-in-under-...   \n",
       "1                                   gpt-oss-on-day-0   \n",
       "2                   disaggregated-prefill-and-decode   \n",
       "3  high-performance-gpu-memory-transfer-on-aws-sa...   \n",
       "4             accelerating-sonar-through-speculation   \n",
       "\n",
       "                                             content  \n",
       "0  weight-transfer-for-rl-post-training-in-under-...  \n",
       "1  gpt-oss-on-day-0 GPT-OSS on Day 0Perplexity is...  \n",
       "2  disaggregated-prefill-and-decode In order to g...  \n",
       "3  high-performance-gpu-memory-transfer-on-aws-sa...  \n",
       "4  accelerating-sonar-through-speculation Specula...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6597939e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=[\"heading\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f23ff375",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>weight-transfer-for-rl-post-training-in-under-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt-oss-on-day-0 GPT-OSS on Day 0Perplexity is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>disaggregated-prefill-and-decode In order to g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>high-performance-gpu-memory-transfer-on-aws-sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>accelerating-sonar-through-speculation Specula...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content\n",
       "0  weight-transfer-for-rl-post-training-in-under-...\n",
       "1  gpt-oss-on-day-0 GPT-OSS on Day 0Perplexity is...\n",
       "2  disaggregated-prefill-and-decode In order to g...\n",
       "3  high-performance-gpu-memory-transfer-on-aws-sa...\n",
       "4  accelerating-sonar-through-speculation Specula..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "033becc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>weight-transfer-for-rl-post-training-in-under-2-seconds We recently achieved 1.3-second cross-machine parameter updates for Kimi-K2 (1T parameters), transferring weights from 256 training GPUs (BF16) to 128 inference GPUs (FP8).In asynchronous reinforcement learning fine-tuning, training and inference run on separate GPUs. After each training step, new weights must be pushed to inference nodes. Many existing frameworks take several seconds—or even minutes—for trillion-parameter models.By leveraging RDMA point-to-point communication, we are able to make the weight transfer blazing fast, without changing inference engine, and make the code easier to write and maintain.RDMA WRITE: one-sided transfersOur solution is built on RDMA WRITE, a one-sided primitive where the source directly writes into the destination’s GPU memory.def rdma_write(src_ptr, dst_ptr, size, src_mr, dst_mr):\\n    # Write from local [src_ptr, src_ptr+size) to remote [dst_ptr, dst_ptr+size).\\n    # src_mr and dst_mr contains Memory Region metadata of both sides.\\n    ...The destination side won’t even get notified for the transfer. This gives us low-latency, high-throughput, zero-copy transfers driven by the training nodes without any control logic on the inference nodes.High-level workflowMetadata collection – Controller gathers parameter metadata from all training and inference GPUs.Schedule computation – Controller computes a static weight transfer schedule, mapping which training GPU sends which parameter to which inference GPU, and in what order.Schedule distribution – Controller sends the schedule to all training GPUs.Execution – After each training step, the controller signals training GPUs to start transfers.Weight transfer executionWith the high-level workflow defined, the key challenge is how to execute weight transfers efficiently at trillion-parameter scale. Here we describe the details of the execution path.DeviceMesh and Mesh GroupsParameters in training are distributed according to FSDP placements. Using full_tensor(), all GPUs in a DeviceMesh can reconstruct the full parameter, hence all can serve as a source for weight transfer.Multiple disjoint DeviceMeshes form a mesh group. Because DeviceMeshes in the same group are disjoint, their transfers don’t interfere and can run fully in parallel. Between mesh groups, we insert a global barrier to enforce ordering.Task pipelineWe treat the transfer of each parameter tensor as a task. The weight transfer process utilizes multiple types of hardware sources, hence we split a weight transfer task into different pipeline stages which overlap in time:Host to device memcpy — If FSDP offloads weight to CPUParameter preparation — Reconstruct full weight with full_tensor(), apply projection fusion, quantize if needed.RDMA transfer — Zero-copy write to remote inference GPU memoryGlobal barrier — After all full_tensor() calls are done, synchronize across mesh groups using GLOO via Ethernet.In implementation, we maintain a FIFO queue of tasks for each pipeline stage. Whenever the head of queue task completes the stage, it is moved to the tail of the next stage queue.GPU memory usage controlfull_tensor() and other GPU operations introduces extra GPU memory usage. To avoid out of memory error, we start the execution of a task only if the current on-the-fly tasks occupies less temporary GPU memory than a configurable watermark.Why it’s fast and simpleSeveral design choices make our system significantly faster to run and easier to maintain than common open-source solutions.Point-to-point communicationA common pattern is to funnel all parameters through rank-0 GPUs: gather on training rank-0, send to inference rank-0, then scatter again. This quickly becomes a choke point, limited by a single GPU’s PCIe bandwidth and NIC (e.g., 400 Gbps ≈ 50 GB/s).In contrast, our point-to-point setup allows every training GPU to send directly to every inference GPU, saturating the full network fabric rather than a single link.One-sided data transferSome systems rely on calling into the inference engine’s update_weight() method for each tensor. That means intrusive changes to the inference code, plus overhead from RPCs, serialization, and control-plane coordination.With RDMA WRITE primitive, we update weights silently on inference GPU memory, without extra copies. No control plane message and no CPU control logic is involved. No modification to inference engine is required.PipeliningThe weight transfer process can leverage four types of hardware resources: (1) Host-device data movement (2) GPU computation for projection fusion and quantization (3) RDMA network for data plane (4) Ethernet for control plane.Our design split weight transfer tasks into pipeline stages, allowing easy overlapping across different hardware resources.Static ScheduleSome implementations recompute a transfer schedule at every training step, repeatedly collecting metadata and distributing instructions. This adds unnecessary control-plane latency.Our schedule is computed once at initialization. Each training iteration simply replays the plan: the controller issues a “go” signal, and GPUs follow their pre-assigned routes. Execution is predictable and lightweight.Clean separationIt’s tempting to entangle the whole weight update process in one monolithic function: collect metadata, name matching, intra-node gathering, projection fusion, quantization, subslicing communication world, inter-node network transfer. It’s hard to program correctly, and even harder to optimize.In our engineering, we separate these steps as individual components. Each components can be unit tested, reasoned about, and optimized in isolation.ConclusionFast, reliable weight transfer is a critical building block for large-scale RL fine-tuning. By combining the RDMA WRITE primitive, a static transfer schedule, and pipelined execution, we reduced trillion-parameter updates to just 1.3 seconds on Kimi-K2. The approach is simple to reason about, easy to maintain, and avoids the bottlenecks of traditional designs.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 content\n",
       "0  weight-transfer-for-rl-post-training-in-under-2-seconds We recently achieved 1.3-second cross-machine parameter updates for Kimi-K2 (1T parameters), transferring weights from 256 training GPUs (BF16) to 128 inference GPUs (FP8).In asynchronous reinforcement learning fine-tuning, training and inference run on separate GPUs. After each training step, new weights must be pushed to inference nodes. Many existing frameworks take several seconds—or even minutes—for trillion-parameter models.By leveraging RDMA point-to-point communication, we are able to make the weight transfer blazing fast, without changing inference engine, and make the code easier to write and maintain.RDMA WRITE: one-sided transfersOur solution is built on RDMA WRITE, a one-sided primitive where the source directly writes into the destination’s GPU memory.def rdma_write(src_ptr, dst_ptr, size, src_mr, dst_mr):\\n    # Write from local [src_ptr, src_ptr+size) to remote [dst_ptr, dst_ptr+size).\\n    # src_mr and dst_mr contains Memory Region metadata of both sides.\\n    ...The destination side won’t even get notified for the transfer. This gives us low-latency, high-throughput, zero-copy transfers driven by the training nodes without any control logic on the inference nodes.High-level workflowMetadata collection – Controller gathers parameter metadata from all training and inference GPUs.Schedule computation – Controller computes a static weight transfer schedule, mapping which training GPU sends which parameter to which inference GPU, and in what order.Schedule distribution – Controller sends the schedule to all training GPUs.Execution – After each training step, the controller signals training GPUs to start transfers.Weight transfer executionWith the high-level workflow defined, the key challenge is how to execute weight transfers efficiently at trillion-parameter scale. Here we describe the details of the execution path.DeviceMesh and Mesh GroupsParameters in training are distributed according to FSDP placements. Using full_tensor(), all GPUs in a DeviceMesh can reconstruct the full parameter, hence all can serve as a source for weight transfer.Multiple disjoint DeviceMeshes form a mesh group. Because DeviceMeshes in the same group are disjoint, their transfers don’t interfere and can run fully in parallel. Between mesh groups, we insert a global barrier to enforce ordering.Task pipelineWe treat the transfer of each parameter tensor as a task. The weight transfer process utilizes multiple types of hardware sources, hence we split a weight transfer task into different pipeline stages which overlap in time:Host to device memcpy — If FSDP offloads weight to CPUParameter preparation — Reconstruct full weight with full_tensor(), apply projection fusion, quantize if needed.RDMA transfer — Zero-copy write to remote inference GPU memoryGlobal barrier — After all full_tensor() calls are done, synchronize across mesh groups using GLOO via Ethernet.In implementation, we maintain a FIFO queue of tasks for each pipeline stage. Whenever the head of queue task completes the stage, it is moved to the tail of the next stage queue.GPU memory usage controlfull_tensor() and other GPU operations introduces extra GPU memory usage. To avoid out of memory error, we start the execution of a task only if the current on-the-fly tasks occupies less temporary GPU memory than a configurable watermark.Why it’s fast and simpleSeveral design choices make our system significantly faster to run and easier to maintain than common open-source solutions.Point-to-point communicationA common pattern is to funnel all parameters through rank-0 GPUs: gather on training rank-0, send to inference rank-0, then scatter again. This quickly becomes a choke point, limited by a single GPU’s PCIe bandwidth and NIC (e.g., 400 Gbps ≈ 50 GB/s).In contrast, our point-to-point setup allows every training GPU to send directly to every inference GPU, saturating the full network fabric rather than a single link.One-sided data transferSome systems rely on calling into the inference engine’s update_weight() method for each tensor. That means intrusive changes to the inference code, plus overhead from RPCs, serialization, and control-plane coordination.With RDMA WRITE primitive, we update weights silently on inference GPU memory, without extra copies. No control plane message and no CPU control logic is involved. No modification to inference engine is required.PipeliningThe weight transfer process can leverage four types of hardware resources: (1) Host-device data movement (2) GPU computation for projection fusion and quantization (3) RDMA network for data plane (4) Ethernet for control plane.Our design split weight transfer tasks into pipeline stages, allowing easy overlapping across different hardware resources.Static ScheduleSome implementations recompute a transfer schedule at every training step, repeatedly collecting metadata and distributing instructions. This adds unnecessary control-plane latency.Our schedule is computed once at initialization. Each training iteration simply replays the plan: the controller issues a “go” signal, and GPUs follow their pre-assigned routes. Execution is predictable and lightweight.Clean separationIt’s tempting to entangle the whole weight update process in one monolithic function: collect metadata, name matching, intra-node gathering, projection fusion, quantization, subslicing communication world, inter-node network transfer. It’s hard to program correctly, and even harder to optimize.In our engineering, we separate these steps as individual components. Each components can be unit tested, reasoned about, and optimized in isolation.ConclusionFast, reliable weight transfer is a critical building block for large-scale RL fine-tuning. By combining the RDMA WRITE primitive, a static transfer schedule, and pipelined execution, we reduced trillion-parameter updates to just 1.3 seconds on Kimi-K2. The approach is simple to reason about, easy to maintain, and avoids the bottlenecks of traditional designs."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#to an embedding model, it doesn’t care if it’s a number or a word it converts the entire string into a numeric vector\n",
    "# that represents the meaning semantics of the text.\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02dfec9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noisy symbols found: {'~', '&', '|', '*', '#', '^', '%', '\\\\', '$', '@'}\n"
     ]
    }
   ],
   "source": [
    "# Numbers, punctuation, and meaningful symbols like:\n",
    "# Hyphens - (in weight-transfer) Periods . and commas, Parentheses (),Em-dashes — are kept, so our text still preserves meaning.\n",
    "# Embeddings and RAG models will capture the semantics properly without these noisy symbols.\n",
    "\n",
    "import re\n",
    "\n",
    "all_text = \" \".join(df['content'].astype(str))\n",
    "noisy_chars = set(re.findall(r\"[~`@#$%^&*|\\\\]\", all_text))\n",
    "\n",
    "print(\"Noisy symbols found:\", noisy_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461837ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "# Define the noisy symbols pattern\n",
    "# Remove them from the content column\n",
    "pattern = r\"[~`@#$%^&*|\\\\]\"\n",
    "df['content'] = df['content'].str.replace(pattern, '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "03d232f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noisy symbols found: set()\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "all_text = \" \".join(df['content'].astype(str))\n",
    "noisy_chars = set(re.findall(r\"[~`@#$%^&*|\\\\]\", all_text))\n",
    "\n",
    "print(\"Noisy symbols found:\", noisy_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6c04ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "slm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
